[{"title":"修改Flume源码使taildir source支持递归（可配置）","date":"2020-03-06T14:12:36.000Z","path":"大数据技术/修改Flume源码使taildir source支持递归（可配置）/","text":"修改Flume源码使taildir source支持递归（可配置） taildir Flume的source选哪个？taildir source首选！ 1.断点还原 positionFile可以记录偏移量 2.可配置文件组，里面使用正则表达式配置多个要监控的文件 这么好的taildir source有一点不完美，只支持监控一级目录下的文件，不能支持递归监控文件夹。 例如： /log/login/20200306/1.1og /log/login/20200307/1.1og 这两个文件无法用taildir采集 为实现目录递归, 即可以同时监控子目录，让上述两个文件可以通过/log/login/*.log采集，就只能修改源代码了。 改源码，先读源码 Flume的taildir source启动会调用start()方法作初始化，里面创建一个ReliableTaildirEventReader,这里用到了建造者模式。 12345678910111213141516171819202122232425262728293031@Override public synchronized void start() &#123; logger.info(\"&#123;&#125; TaildirSource source starting with directory: &#123;&#125;\", getName(), filePaths); try &#123; reader = new ReliableTaildirEventReader.Builder() .filePaths(filePaths) .headerTable(headerTable) .positionFilePath(positionFilePath) .skipToEnd(skipToEnd) .addByteOffset(byteOffsetHeader) .cachePatternMatching(cachePatternMatching) .annotateFileName(fileHeader) .fileNameHeader(fileHeaderKey) .build(); &#125; catch (IOException e) &#123; throw new FlumeException(\"Error instantiating ReliableTaildirEventReader\", e); &#125; idleFileChecker = Executors.newSingleThreadScheduledExecutor( new ThreadFactoryBuilder().setNameFormat(\"idleFileChecker\").build()); idleFileChecker.scheduleWithFixedDelay(new idleFileCheckerRunnable(), idleTimeout, checkIdleInterval, TimeUnit.MILLISECONDS); positionWriter = Executors.newSingleThreadScheduledExecutor( new ThreadFactoryBuilder().setNameFormat(\"positionWriter\").build()); positionWriter.scheduleWithFixedDelay(new PositionWriterRunnable(), writePosInitDelay, writePosInterval, TimeUnit.MILLISECONDS); super.start(); logger.debug(\"TaildirSource started\"); sourceCounter.start(); &#125; taildir source属于PollableSource，下面是PollableSource的文档注释： 123456789/** * A &#123;@link Source&#125; that requires an external driver to poll to determine * whether there are &#123;@linkplain Event events&#125; that are available to ingest * from the source. * * @see org.apache.flume.source.EventDrivenSourceRunner */public interface PollableSource extends Source &#123;... 这段注释的意思是PollableSource是需要一个外部驱动去查看有没有需要消费的事件，从而拉取事件，讲白了就是定时拉取。所以flume也不一定是真正实时的，只是隔一会儿不停地来查看事件而已(与之相应的是另一种EventDrivenSourceRunner)。 那么taildir source在定时拉取事件的时候是调用的process方法。 123456789101112131415161718192021222324@Override public Status process() &#123; Status status = Status.READY; try &#123; existingInodes.clear(); existingInodes.addAll(reader.updateTailFiles()); for (long inode : existingInodes) &#123; TailFile tf = reader.getTailFiles().get(inode); if (tf.needTail()) &#123; tailFileProcess(tf, true); &#125; &#125; closeTailFiles(); try &#123; TimeUnit.MILLISECONDS.sleep(retryInterval); &#125; catch (InterruptedException e) &#123; logger.info(\"Interrupted while sleeping\"); &#125; &#125; catch (Throwable t) &#123; logger.error(\"Unable to tail files\", t); status = Status.BACKOFF; &#125; return status; &#125; 重点就是下面这几行: 1234567existingInodes.addAll(reader.updateTailFiles()); for (long inode : existingInodes) &#123; TailFile tf = reader.getTailFiles().get(inode); if (tf.needTail()) &#123; tailFileProcess(tf, true); &#125; &#125; 从reader.updateTailFiles()获取需要监控的文件，然后对每一个进行处理，查看最后修改时间，判定是否需要tail，需要tail就tail。 那么进入reader.updateTailFiles() 1234567891011121314for (File f : taildir.getMatchingFiles()) &#123; long inode; try &#123; inode = getInode(f); &#125; catch (NoSuchFileException e) &#123; logger.info(\"File has been deleted in the meantime: \" + e.getMessage()); continue; &#125; TailFile tf = tailFiles.get(inode); if (tf == null || !tf.getPath().equals(f.getAbsolutePath())) &#123; long startPos = skipToEnd ? f.length() : 0; tf = openFile(f, headers, inode, startPos); &#125; else &#123; ... 遍历每一个正则表达式匹配对应的匹配器，每个匹配器去获取匹配的文件！taildir.getMatchingFiles() 123456789101112131415161718192021List&lt;File&gt; getMatchingFiles() &#123; long now = TimeUnit.SECONDS.toMillis( TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis())); long currentParentDirMTime = parentDir.lastModified(); List&lt;File&gt; result; // calculate matched files if // - we don't want to use cache (recalculate every time) OR // - directory was clearly updated after the last check OR // - last mtime change wasn't already checked for sure // (system clock hasn't passed that second yet) if (!cachePatternMatching || lastSeenParentDirMTime &lt; currentParentDirMTime || !(currentParentDirMTime &lt; lastCheckedTime)) &#123; lastMatchedFiles = sortByLastModifiedTime(getMatchingFilesNoCache(isRecursive)); lastSeenParentDirMTime = currentParentDirMTime; lastCheckedTime = now; &#125; return lastMatchedFiles; &#125; 可以看到getMatchingFilesNoCache(isRecursive)就是获取匹配的文件的方法，也就是需要修改的方法了！ ps：这里的isRecursive是我加的~ 点进去： 123456789101112private List&lt;File&gt; getMatchingFilesNoCache() &#123; List&lt;File&gt; result = Lists.newArrayList(); try (DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(parentDir.toPath(), fileFilter)) &#123; for (Path entry : stream) &#123; result.add(entry.toFile()); &#125; &#125; catch (IOException e) &#123; logger.error(\"I/O exception occurred while listing parent directory. \" + \"Files already matched will be returned. \" + parentDir.toPath(), e); &#125; return result;&#125; 源码是用了Files.newDirectoryStream(parentDir.toPath(), fileFilter))，将父目录下符合正则表达式的文件都添加到一个迭代器里。（这里还用了try (…)的语法糖） 找到地方了，开始改 在getMatchingFilesNoCache()方法下面加一个重载的方法, 可增加扩展性： 123456789101112131415161718192021222324252627private List&lt;File&gt; getMatchingFilesNoCache(boolean recursion) &#123; if (!recursion) &#123; return getMatchingFilesNoCache(); &#125; List&lt;File&gt; result = Lists.newArrayList(); // 使用非递归的方式遍历文件夹 Queue&lt;File&gt; dirs = new ArrayBlockingQueue&lt;&gt;(10); dirs.offer(parentDir); while (dirs.size() &gt; 0) &#123; File dir = dirs.poll(); try &#123; DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(dir.toPath(), fileFilter); stream.forEach(path -&gt; result.add(path.toFile())); &#125; catch (IOException e) &#123; logger.error(\"I/O exception occurred while listing parent directory. \" + \"Files already matched will be returned. (recursion)\" + parentDir.toPath(), e); &#125; File[] dirList = dir.listFiles(); assert dirList != null; for (File f : dirList) &#123; if (f.isDirectory()) &#123; dirs.add(f); &#125; &#125; &#125; return result;&#125; 使用了非递归的方式遍历文件夹，就是树到队列的转换。 到这里，核心部分就改完了，接下来要处理这个recursion的参数。 华丽的分割线后，顺腾摸瓜 一路改构造方法，添加这个参数，最终参数从哪来呢？ flume的source启动时会调用configure方法，将Context中的内容配置进reader等对象中。 isRecursive = context.getBoolean(RECURSIVE, DEFAULT_RECURSIVE); context从TaildirSourceConfigurationConstants中获取配置名和默认值。 1234/** * Whether to support recursion. */public static final String RECURSIVE = \"recursive\";public static final boolean DEFAULT_RECURSIVE = false; 这里的recursive也就是flume配置文件里配置项了 12# Whether to support recusiona1.sources.r1.recursive = true 大功告成，打包试试 执行package将其放在flume的lib下，替换原来的flume-taildir-source***.jar 启动，测试，成功！","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://zzuuriel.github.io/tags/Flume/"}]},{"title":"解决Spark on YARN时jar包乱飞的问题","date":"2020-03-06T12:09:39.000Z","path":"大数据技术/解决Spark on YARN时jar包乱飞的问题/","text":"解决Spark on YARN时jar包乱飞的问题 存在问题 使用Spark on YARN运行自带的SparkPi作业 123456./spark-submit \\--class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode client \\/home/hadoop/app/spark-2.4.5-bin-2.6.0-cdh5.16.2/examples/jars/spark-examples_2.12-2.4.5.jar \\2 查看日志 123WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.INFO yarn.Client: Uploading resource file:/tmp/spark-47b51eb1-58db-4683-8172-7c6db44c94d8/__spark_libs__5430804291093212829.zip -&gt; hdfs://hadoop000:8020/user/hadoop/.sparkStaging/application_1589545550777_0002/__spark_libs__5430804291093212829.zipINFO yarn.Client: Uploading resource file:/tmp/spark-47b51eb1-58db-4683-8172-7c6db44c94d8/__spark_conf__3780031617713858810.zip -&gt; hdfs://hadoop000:8020/user/hadoop/.sparkStaging/application_1589545550777_0002/__spark_conf__.zip 提示我们既没有设置spark.yarn.archive也没有设置spark.yarn.jars，所以spark自己把所有jar包打成zip包上传到hdfs上(用完后又删掉了) 解决方法 根据日志提示，将spark jar包上传到hdfs上。 1、把spark/jars下的jar包打成zip包，防止小文件过多，影响hdfs 12cd jarszip sparkjars.zip *.jar 2、上传到hdfs 1hadoop fs -put sparkjars.zip /lib 3、在spark-default配置中指定jar包的地址 12vi spark-defaults.confspark.yarn.archive hdfs://hadoop000:8020/lib/sparkjars.zip 结果验证 重新运行作业，查看日志 1INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://hadoop000:8020/lib/sparkjars.zip 可以发现spark已经从我们配置的路径读取了jar包","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Flink之Watermark理解","date":"2020-03-02T14:12:36.000Z","path":"大数据技术/Flink之Watermark理解/","text":"Flink之Watermark理解 概述 我们知道实时计算中，数据时间比较敏感，有eventTime和processTime之分，一般来说eventTime是从原始的消息中提取过来的，processTime是Flink自己提供的，Flink中一个亮点就是可以基于eventTime计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用processTime显然是不合理的。 为了处理事件时间，首先，流程序需要相应地设置时间特性，代码如下： 12val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) 其次，需要提取eventTime和设置WaterMark。下面我们具体分析一下，eventTime结合Watermark的工作方式。 案例分析 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556object WMApp &#123; def main(args: Array[String]): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val output = new OutputTag[Info](\"late\") val MAX_ALLOWED_UNBOUNDED_TIME = 10000L val stream = env.socketTextStream(\"hadoop000\", 9999) .map(x =&gt; &#123; val splits = x.split(\",\") Info(splits(0).trim, splits(1).trim.toDouble, splits(2).trim, splits(3).trim.toLong) &#125;).assignTimestampsAndWatermarks(new RuozedataAssignerWithPeriodicWatermarks(MAX_ALLOWED_UNBOUNDED_TIME)) .keyBy(0) .window(TumblingEventTimeWindows.of(Time.seconds(3))) //.allowedLateness(Time.seconds(2)) // 允许等待两秒 .sideOutputLateData(output) .apply(new RichWindowFunction[Info, String, Tuple, TimeWindow] &#123; override def apply(key: Tuple, window: TimeWindow, input: Iterable[Info], out: Collector[String]): Unit = &#123; val totals = input.size var totalTmp = 0.0 input.foreach(x =&gt; totalTmp = totalTmp + x.temperature) val avg = totalTmp / totals out.collect(s\"$avg, $&#123;new Timestamp(window.getStart)&#125;, $&#123;new Timestamp(window.getEnd)&#125;\") &#125; &#125;) stream.print() stream.getSideOutput(output).print(\"-----\") env.execute(this.getClass.getSimpleName) &#125;&#125;class RuozedataAssignerWithPeriodicWatermarks(maxAllowedUnorderedTime: Long) extends AssignerWithPeriodicWatermarks[Info] &#123; var maxTimestamp: Long = 0 override def extractTimestamp(element: Info, previousElementTimestamp: Long): Long = &#123; val nowTime = element.time * 1000 maxTimestamp = maxTimestamp.max(nowTime) println(new Timestamp(nowTime) + \",\" + new Timestamp(maxTimestamp) + \",\" + new Timestamp(getCurrentWatermark.getTimestamp)) nowTime &#125; override def getCurrentWatermark: Watermark = &#123; new Watermark(maxTimestamp - maxAllowedUnorderedTime) &#125;&#125;case class Info(id: String, temperature: Double, name: String, time: Long) 测试 [hadoop@hadoop000 ~]$ nc -lk 9999 输入： 1231,36.8,a,15831462671,37.1,a,1583146268 1,36.6,a,1583146278 返回： 1232020-03-02 18:51:07.0,2020-03-02 18:51:07.0,2020-03-02 18:50:57.02020-03-02 18:51:08.0,2020-03-02 18:51:08.0,2020-03-02 18:50:58.02020-03-02 18:51:18.0,2020-03-02 18:51:18.0,2020-03-02 18:51:08.0 此时窗口未被触发，窗口触发需要要满足两个条件： 1.Watermark&gt;=window_end_time, 2.此窗口内有数据。 而2020-03-02 18:51:07.0，2020-03-02 18:51:08.0在 [06,09) 区间内，故当Watermark≥09时，窗口就会被触发，输入1,36.6,a,1583146279 ，返回： 122020-03-02 18:51:19.0,2020-03-02 18:51:19.0,2020-03-02 18:51:09.036.95, 2020-03-02 18:51:06.0, 2020-03-02 18:51:09.0 追加一条延时数据1,36.8,a,1583146267，返回： 122020-03-02 18:51:07.0,2020-03-02 18:51:19.0,2020-03-02 18:51:09.0-----&gt; Info(1,36.8,a,1583146267) 分析 1.提取WaterMark的方式两类： 一类是定时提取watermark，对应AssignerWithPeriodicWatermarks，这种方式会定时提取更新wartermark。通常情况下采用定时提取就足够了。 另一类伴随event的到来就提取watermark，就是每一个event到来的时候，就会提取一次watermark，对应AssignerWithPunctuatedWatermarks，这样的方式当然设置watermark更为精准，但是当数据量大的时候，频繁的更新wartermark会比较影响性能。 2.在上面例子中extractTimestamp方法，在每一个event到来之后就会被调用。 这里其实就是为了设置watermark的值，关键代码在于Math.max(timestamp,currentMaxTimestamp)，意思是在当前的水位和当前事件时间中选取一个较大值，来让watermark流动。 为什么要选取最大值，因为理想状态下，消息的事件时间肯定是递增的，实际处理中，消息乱序是大概率事件，所以为了保证watermark递增，要取最大值。 3.getCurrentWatermarker会被定时调用，可以看到方法中watermark减了一个常量（本例设置为10s），为了理解这么做的原因，需要了解Watermark的工作方式，上文提到在基于eventTime的计算中，需要watermark的协助来触发window的计算，触发规则是watermark≥window的结束时间，并且这个窗口中有数据。 上例中，如果watermark没有减去10s，设想理想情况下消息都没有延迟，watermark ≥18:51:09.0的时候，就会触发窗口[06,09)的计算，这个时候因为消息都没有延迟，watermark之前的消息都已经落入到window中，所以会计算window中全量的数据。那么假如有一条消息data1，eventTime是18:51:07，应该属于[06,09)，但是在18:51:10才到达，因为窗口已经关闭，那么这条消息就会被丢弃，没有加入计算，这样就会出现问题。 每次提取eventTime的时候，如果watermark减去10s，那么当data1在18:51:10到达的时候，watermark才18:51:00，这个时候，窗口还没有触发计算，那么data1会被加入窗口[06,09)，这个时候计算完全没有问题，所以减去一个常量是为了对延时的消息进行容错的。 4.watermark是全局性的参数，watermark超过window的endtime之后，就会触发窗口计算。一般情况下，触发窗口计算之后，窗口就销毁掉了，后面再来的数据也不会再计算，但这个一般情况对应的是allowedLateness=0的场景。 allowedLateness是窗口函数的属性，默认情况下，如果不指定allowedLateness，其值是0，即对于watermark超过end-of-window之后，还有此window的数据到达时，这些数据被删除掉了 如果allowedLateness &gt; 0， allowedLateness会再次触发窗口的计算，而之前触发的数据，会buffer起来，直到watermark ≥ end-of-window + allowedLateness（），窗口的数据及元数据信息才会被删除。 5.如果指定窗口已经彻底关闭了，可以利用sideOutputLateData把延迟数据放到侧输出流。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://zzuuriel.github.io/tags/Flink/"}]},{"title":" Spark-2.4.5源码编译","date":"2020-02-21T06:23:09.000Z","path":"大数据技术/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/","text":"Spark-2.4.5源码编译 背景 从Spark官网https://spark.apache.org/ 下载的预编译版本的Spark，受到诸多限制： 比如我们生产环境使用的Hadoop是CDH版本，那么从官网下载的预编译版本就不能使用； 线上生产环境和实际业务需求的复杂性，不可避免地需要修改spark源码； 因此，我们必须学会如何编译Spark源码，以便应用于线上生产环境。 前置条件 官网下载Spark源码 查看官方文档，确定前置条件 Spark2.4.5版本源码编译文档地址：https://spark.apache.org/docs/latest/building-spark.html 根据需要编译的Spark版本，查看对应到官网文档，里面有必备的条件，比如本文的： 条件1：Maven 3.5.4 版本及以上 条件2：Java 8 版本及以上 条件3：需要使用兼容的 scala 版本(2.12.x)。 Maven配置文件增加阿里镜像 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;&lt;/mirror&gt; 解压源码并修改pom.xml文件 由于本文需要编译的Hadoop版本是2.6.0-CDH5.15.1版本，编译过程中需要下载CDH相关的包，因此在pom文件需要添加cloudera仓库。 1234&lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;&lt;/repository&gt; 编译步骤 官网里介绍了两种编译方式，一是mvn，另外是 make-distribution.sh；下面分别介绍具体实现。 Apache Maven 1.设置Maven的工作使用内存 1export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" 如果不设置会报以下错误： 12[INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.12/classes...[ERROR] Java heap space -&gt; [Help 1] 2.build/mvn 进到解压后源码文件夹下，执行如下命令： 1./build/mvn -Pyarn -Phive -Phive-thriftserver -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.15.1 -DskipTests clean package Building a Runnable Distribution 编译成可部署在分布式系统上的可执行版本，方便后续安装，这种方法是本文推荐使用的。 1.为了加快编译速度，需要修改启动编译脚本-Phadoop-2.6 vim spark-2.4.5/dev/make-distribution.sh 1234567891011121314151617181920212223242526\"\"\"添加以下部分\"\"\"VERSION=2.4.5SCALA_VERSION=2.12SPARK_HADOOP_VERSION=2.6.0-cdh5.15.1SPARK_HIVE=1\"\"\"注释掉以下部分，该部分为了获取版本号将会占用大量时间\"\"\"#VERSION=$(\"$MVN\" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\\# | grep -v \"INFO\"\\# | grep -v \"WARNING\"\\# | tail -n 1)#SCALA_VERSION=$(\"$MVN\" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\\# | grep -v \"INFO\"\\# | grep -v \"WARNING\"\\# | tail -n 1)#SPARK_HADOOP_VERSION=$(\"$MVN\" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\\# | grep -v \"INFO\"\\# | grep -v \"WARNING\"\\# | tail -n 1)#SPARK_HIVE=$(\"$MVN\" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\\# | grep -v \"INFO\"\\# | grep -v \"WARNING\"\\# | fgrep --count \"&lt;id&gt;hive&lt;/id&gt;\";\\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\\# # because we use \"set -o pipefail\"# echo -n) 2.执行编译脚本 1./dev/make-distribution.sh --name 2.6.0-cdh5.15.1 --tgz -Pyarn -Phive -Phive-thriftserver -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.15.1 12345678注：-Phadoop hadoop的大版本号-Dhadoop.version=2.6.0-cdh5.15.1 hadoop的详细版本号--name 编译后spark安装包的名字--tgz 以tgz方式压缩-Phive 编译出来的spark支持hive-Phive-thriftserver 编译出来的spark支持hive-thriftserver-Pyarn 编译出来的spark支持在yarn上运行 3.编译成功 最后编译成功后，还会执行一段脚本将中间过程产生的文件删除，并将编译成功的文件打包。 将编译后成功的安装包spark-2.4.5-bin-2.6.0-cdh5.15.1解压，以local的方式启动spark ， 进入bin目录下执行下面命令： 12345678910111213141516171819[hadoop@hadoop000 bin]$ ./spark-shell master local[2]20/02/21 07:52:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Spark context Web UI available at http://hadoop000:4040Spark context available as 'sc' (master = local[*], app id = local-1583538833348).Spark session available as 'spark'.Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91)Type in expressions to have them evaluated.Type :help for more information.scala&gt;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Flink自定义触发器","date":"2020-02-20T14:12:36.000Z","path":"大数据技术/Flink自定义触发器/","text":"Flink自定义触发器 Flink触发器 触发器决定窗口(由窗口分配者形成)什么时候可以被窗口函数处理。 每个窗户都有一个默认触发器。 如果默认触发器不符合我们的需要，可以使用trigger(…)自定义触发器。 主要方法 触发器接口有五种方法，允许触发器对不同事件做出反应: onElement() 添加到每个窗口的元素都会调用此方法。 onEventTime() 当注册的事件时间计时器触发时，将调用此方法。 onProcessingTime() 当注册的处理时间计时器触发时，将调用此方法。 onMerge() 与有状态触发器相关，并在两个触发器对应的窗口合并时合并它们的状态，例如在使用会话窗口时。 clear() 执行删除相应窗口时所需的任何操作。(一般是删除定义的状态、定时器等) 以上方法有两点需要注意: 1)前三个函数通过返回一个TriggerResult来决定如何对它们的调用事件进行操作。 行动可以是以下任何一种: CONTINUE: 什么也不做 FIRE: 表示触发计算，同时保留窗口中的数据 PURGE: 简单地删除窗口的内容，并保留关于窗口和任何触发器状态的任何潜在元信息。 FIRE_AND_PURGE: 触发计算，然后清除窗口中的元素。（默认情况下，预先实现的触发器只触发而不清除窗口状态） 2)这些方法中的任何一个都可以用来注册处理时间计时器或事件时间计时器，以便将来执行操作。 案例 需求 当窗口中的数据量达到一定数量的时候触发计算 根据执行时间每隔一定时间且窗口中有数据触发计算，如果没有数据不触发计算 窗口关闭的时候清除数据 实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566object TriggerWindowApp &#123; def main(args: Array[String]): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val MAX_WAIT_TIME = 10 // time window val MAX_PERSON_CNTS = 4 // count window env.socketTextStream(\"hadoop000\", 9999) .filter(x =&gt; x.trim.nonEmpty) .map(x =&gt; &#123; (x.split(\",\")(0), 1) &#125;).keyBy(0) .timeWindow(Time.seconds(MAX_WAIT_TIME)) // time window .trigger(new RuozedataTrigger(MAX_PERSON_CNTS)) .sum(1) .print() env.execute(this.getClass.getSimpleName) &#125;&#125;class RuozedataTrigger(max: Int) extends Trigger[(String, Int), TimeWindow] &#123; //定义state描述符 val desc: ReducingStateDescriptor[Int] = new ReducingStateDescriptor(\"count\", new ReduceFunction[Int] &#123; override def reduce(value1: Int, value2: Int): Int = value1 + value2 &#125;, classOf[Int]) override def onElement(element: (String, Int), timestamp: Long, window: TimeWindow, ctx: Trigger.TriggerContext): TriggerResult = &#123; ctx.registerProcessingTimeTimer(window.maxTimestamp) val state = ctx.getPartitionedState(desc) state.add(1) val value = state.get() if (value &gt;= max) &#123; println(\"---count触发---\") state.clear() TriggerResult.FIRE_AND_PURGE &#125; else &#123; TriggerResult.CONTINUE &#125; &#125; override def onProcessingTime(time: Long, window: TimeWindow, ctx: Trigger.TriggerContext): TriggerResult = &#123; println(\"~~~time触发~~~\") TriggerResult.FIRE_AND_PURGE &#125; override def onEventTime(time: Long, window: TimeWindow, ctx: Trigger.TriggerContext): TriggerResult = &#123; TriggerResult.CONTINUE &#125; override def clear(window: TimeWindow, ctx: Trigger.TriggerContext): Unit = &#123; ctx.getPartitionedState(desc).clear() ctx.deleteProcessingTimeTimer(window.maxTimestamp) &#125; override def canMerge: Boolean = true override def onMerge(window: TimeWindow, ctx: Trigger.OnMergeContext): Unit = &#123; ctx.mergePartitionedState(desc) &#125;&#125; 测试 [hadoop@hadoop000 ~]$ nc -lk 9999 1）直接输入4个a 1234aaaa 返回结果： 12---count触发---3&gt; (a,4) 2）输入1个a，等待10s 返回结果： 12~~~time触发~~~3&gt; (a,1)","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://zzuuriel.github.io/tags/Flink/"}]},{"title":"深入理解Spark算子aggregate","date":"2019-12-15T06:15:39.000Z","path":"大数据技术/60.深入理解Spark算子aggregate/","text":"深入理解Spark算子aggregate 源码解读 123456789101112131415161718192021222324 /*** Aggregate the elements of each partition, and then the results for all the partitions, using* given combine functions and a neutral \"zero value\". This function can return a different result* type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U* and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are* allowed to modify and return their first argument instead of creating a new U to avoid memory* allocation.* * @param zeroValue the initial value for the accumulated result of each partition for the* `seqOp` operator, and also the initial value for the combine results from* different partitions for the `combOp` operator - this will typically be the* neutral element (e.g. `Nil` for list concatenation or `0` for summation)* @param seqOp an operator used to accumulate results within a partition* @param combOp an associative operator used to combine results from different partitions */ def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U = withScope &#123; // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) =&gt; it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (index: Int, taskResult: U) =&gt; jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult &#125; 翻译： aggregate先对每个分区的元素做聚集，然后对所有分区的结果做聚集，聚集过程中，使用的是给定的聚集函数以及初始值”zero value”。这个函数能返回一个与原始RDD不同的类型U，因此，需要一个合并RDD类型T到结果类型U的函数，还需要一个合并类型U的函数。这两个函数都可以修改和返回他们的第一个参数，而不是重新新建一个U类型的参数以避免重新分配内存。 参数zeroValue：seqOp运算符的每个分区的累积结果的初始值以及combOp运算符的不同分区的组合结果的初始值 - 这通常将是初始元素（例如“Nil”表的列表 连接或“0”表示求和） 参数seqOp： 每个分区累积结果的聚集函数。 参数combOp： 一个关联运算符用于组合不同分区的结果 解读： 这个算子一共有两个参数列表，第一个参数列表中传递 zeroValue （第零个值）第二个参数列表中传递两个函数，传入的第一个函数seqOp函数会作用于每个分区,第二个函数combOp函数在第一个函数执行完之后汇总所有分区结果。 这两个函数的第一个参数都是累加器，第一次执行时，会把zeroValue赋给累加器。第一次j计算之后会把返回值赋给累加器，作为下一次运算的第一个参数。seqOP函数每个分区都有一个累加器，combOp函数只有一个累加器。 示例 1234567891011object test &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val rdd1 = sc.parallelize(1 to 5, 1) def func1(a:Int, b:Int) = a * b def func2(a:Int, b:Int) = a + b println(rdd1.aggregate(3)(func1, func2)) sc.stop() &#125;&#125; 分析： 1）seqOp seqOp对分区内的所有元素遍历计算 这个分区有几个元素执行几次这个方法 def func1(a:Int, b:Int) = a * b 当第一个元素1传进来时 a是seqOp累加器(第一次执行时，会把zeroValue赋给累加器，zeroValue=3），y是第一个元素1，之后将返回值赋回给累加器。 当第二个元素2传进来时 a：上一次运算之后赋了新值的累加器，3 * 1 =3，y：传入的第二个元素2，之后将返回值赋回给累加器。 当第三个元素3传进来时 a：上一次运算之后赋了新值的累加器，3 * 2 =6，y：传入的第二个元素2，之后将返回值赋回给累加器。 … 最后seqOp累加计算得到360 2）combOp 之后combOp会合并所有分区的结果。 def func2(a:Int, b:Int) = a + b 这个函数遍历所有中间结果（累加器：一个分区一个） 第一次执行时， a是combOp累加器（第一次执行时，会把zeroValue赋给累加器，zeroValue=3），b是第一个分区的累加器（360)，之后将返回值赋回给累加器。。 第二次执行时， a是combOp累加器，b是第二个分区的累加器，之后将返回值赋回给累加器。 … 有几个分区，就执行几次combOp函数。 本例只有一个分区，最后计算结果为363 应用 1.两两求和 12345678910object test &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val rdd = sc.parallelize(List(1, 3, 2, 4, 3, 5), 3) println(rdd.sum) //println（rdd.aggregate(0)(_+_,_+_)） sc.stop() &#125;&#125; rdd.sum可以用rdd.aggregate(0)(+,+)替代，最后运行结果都是18 2.求每个分区的最大值之和 12345678910111213object test &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val rdd2 = sc.parallelize(List(List(1,3),List(2,4),List(3,5)), 3) def fun01(x:Int,y:List[Int]) = &#123; x.max(y.max) &#125; def fun02(a:Int, b:Int) = a + b println(rdd2.aggregate(0)(fun01,fun02)) sc.stop() &#125;&#125; 初始值为0，最后运行结果为12， 初始值为10，运行结果为40。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Kudu常用Api(java) ","date":"2019-12-07T12:35:39.000Z","path":"大数据技术/60.Kudu常用Api(java)/","text":"Kudu常用Api(java) 添加依赖 在pom.xml文件中添加以下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-client&lt;/artifactId&gt; &lt;version&gt;$&#123;kudu.version&#125;&lt;/version&gt;&lt;/dependency&gt; 测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120public class KuduAPIApp &#123; String masterAddress = \"hadoop000:7051\"; KuduClient client = null; private ColumnSchema newColumn(String name, Type type, Boolean pk) &#123; ColumnSchema.ColumnSchemaBuilder column = new ColumnSchema.ColumnSchemaBuilder(name, type); column.key(pk); return column.build(); &#125; /** * 创建表格 * @throws Exception */ @Test public void CreateTable() throws Exception &#123; LinkedList&lt;ColumnSchema&gt; columns = new LinkedList&lt;&gt;(); columns.add(newColumn(\"id\", Type.INT32, true)); columns.add(newColumn(\"name\", Type.STRING, false)); Schema schema = new Schema(columns); CreateTableOptions options = new CreateTableOptions(); options.setNumReplicas(1); LinkedList&lt;String&gt; partitions = new LinkedList&lt;&gt;(); partitions.add(\"id\"); options.addHashPartitions(partitions, 3); client.createTable(\"student\", schema, options); &#125; /** * 插入数据 * @throws Exception */ @Test public void insert() throws Exception &#123; KuduTable table = client.openTable(\"student\"); KuduSession kuduSession = client.newSession(); kuduSession.setMutationBufferSpace(3000); for (int i = 0; i &lt; 10; i++) &#123; Insert insert = table.newInsert(); insert.getRow().addInt(\"id\", i + 1); insert.getRow().addString(\"name\", \"PK\" + i); kuduSession.flush(); kuduSession.apply(insert); &#125; &#125; /** * 查询表 * @throws Exception */ @Test public void query()throws Exception&#123; KuduTable table = client.openTable(\"student\"); KuduScanner scanner = client.newScannerBuilder(table).build(); while(scanner.hasMoreRows()) &#123; for(RowResult result : scanner.nextRows()) &#123; System.out.println(result.getInt(\"id\") + \"\\t\" + result.getString(\"name\")); &#125; &#125; &#125; /** * 更改表数据 * @throws Exception */ @Test public void update() throws Exception&#123; KuduTable table = client.openTable(\"student\"); KuduSession kuduSession = client.newSession(); Update update = table.newUpdate(); // update xxx set id=1 PartialRow row = update.getRow(); row.addInt(\"id\",1); row.addString(\"name\", \"ruozedata\"); kuduSession.apply(update); &#125; /** * 删除指定行 * @throws Exception */ @Test public void DeleteRow() throws Exception&#123; KuduTable table = client.openTable(\"student\"); KuduSession kuduSession = client.newSession(); Delete delete = table.newDelete(); delete.getRow().addInt(\"id\",2); kuduSession.apply(delete); &#125; /** * 删掉整张表 * @throws Exception */ @Test public void DropTable() throws Exception&#123; client.deleteTable(\"student\"); &#125; @Before public void setUp() &#123; client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); &#125; @After public void tearDown() &#123; if (null != client) &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.urieldata.kudu;import org.apache.kudu.ColumnSchema;import org.apache.kudu.Schema;import org.apache.kudu.Type;import org.apache.kudu.client.CreateTableOptions;import org.apache.kudu.client.KuduClient;import org.apache.kudu.client.KuduException;import java.util.LinkedList;public class CreateTable &#123; private static ColumnSchema newColumn(String name, Type type, Boolean isKey)&#123; ColumnSchema.ColumnSchemaBuilder column = new ColumnSchema.ColumnSchemaBuilder(name, type); column.key(isKey); return column.build(); &#125; public static void main(String[] args) &#123; //master地址 String masterAddress = \"hadoop000:7051\"; //创建kudu的数据库连接 KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); //设置表的schema LinkedList&lt;ColumnSchema&gt; columns = new LinkedList&lt;&gt;(); /* *和RDBMS不同的是，Kudu不提供自动递增列功能，因此应用程序必须始终 *在插入期间提供完整的主键 */ columns.add(newColumn(\"id\",Type.INT32,true)); columns.add(newColumn(\"name\",Type.STRING,false)); Schema schema = new Schema(columns); //创建表时提供的所有选项 CreateTableOptions options = new CreateTableOptions(); //设置表的replica备份和分区规则 LinkedList&lt;String&gt; parcols = new LinkedList&lt;&gt;(); parcols.add(\"id\"); //设置表的备份数 options.setNumReplicas(1); //设置hash分区和数量 options.addHashPartitions(parcols,3); try &#123; client.createTable(\"student\",schema,options); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 插入数据 1234567891011121314151617181920212223242526272829303132333435363738package com.urieldata.kudu;import org.apache.kudu.client.*;public class insert &#123; public static void main(String[] args) &#123; String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; KuduTable table = client.openTable(\"student\"); KuduSession kuduSession = client.newSession(); kuduSession.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH); kuduSession.setMutationBufferSpace(3000); for (int i = 0; i &lt; 10; i++) &#123; Insert insert = table.newInsert(); insert.getRow().addInt(\"id\", i + 1); insert.getRow().addString(\"name\", \"Uriel\" + i); kuduSession.flush(); kuduSession.apply(insert); &#125; kuduSession.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 查询数据 12345678910111213141516171819202122232425262728293031323334353637package com.urieldata.kudu;import org.apache.kudu.client.*;public class query &#123; public static void main(String[] args) &#123; //master地址 String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; KuduTable table = client.openTable(\"student\"); //创建scanner扫描 KuduScanner scanner = client.newScannerBuilder(table).build(); //遍历数据 while (scanner.hasMoreRows()) &#123; for (RowResult rowResult : scanner.nextRows()) &#123; System.out.println(rowResult.getInt(\"id\") + \"\\t\" + rowResult.getString(\"name\")); &#125; &#125; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 更改表数据 1234567891011121314151617181920212223242526272829303132333435363738package com.urieldata.kudu;import org.apache.kudu.client.*;public class update &#123; public static void main(String[] args) &#123; String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; KuduTable table = client.openTable(\"student\"); KuduSession session = client.newSession(); session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC); //更新数据 Update update = table.newUpdate(); PartialRow row = update.getRow(); row.addInt(\"id\",1); row.addString(\"name\",\"IU\"); session.apply(update); session.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 删除指定行 123456789101112131415161718192021222324252627282930313233343536package com.urieldata.kudu;import org.apache.kudu.client.*;public class DeleteRow &#123; public static void main(String[] args) &#123; //master地址 String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; KuduTable table = client.openTable(\"student\"); KuduSession session = client.newSession(); //删除数据 Delete delete = table.newDelete(); //行删除和更新操作必须指定要更改行的完整主键 delete.getRow().addInt(\"id\",1); session.flush(); session.apply(delete); session.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 删除整张表 123456789101112131415161718192021222324252627package com.urieldata.kudu;import org.apache.kudu.client.KuduClient;import org.apache.kudu.client.KuduException;public class DropTable &#123; public static void main(String[] args) &#123; String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; client.deleteTable(\"student\"); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Kudu","slug":"Kudu","permalink":"https://zzuuriel.github.io/tags/Kudu/"}]},{"title":"CentOS7安装单机版Kudu ","date":"2019-12-06T12:20:39.000Z","path":"大数据技术/60.CentOS7安装单机版Kudu/","text":"CentOS7安装单机版Kudu 下载rpm包 地址：http://archive.cloudera.com/cdh5/redhat/7/x86_64/cdh/5.16.2/RPMS/x86_64/ package: 123456kudu-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpmkudu-client-devel-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm kudu-client0-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm kudu-debuginfo-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm kudu-master-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpmkudu-tserver-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm 安装rpm包 1[root@hadoop000 kudu]$sudo rpm -ivh kudu* 123456789warning: kudu-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID e8f86acd: NOKEYPreparing... ################################# [100%]Updating / installing... 1:kudu-1.7.0+cdh5.16.2+0-1.cdh5.16.################################# [ 17%] 2:kudu-client0-1.7.0+cdh5.16.2+0-1.################################# [ 33%] 3:kudu-client-devel-1.7.0+cdh5.16.2################################# [ 50%] 4:kudu-master-1.7.0+cdh5.16.2+0-1.c################################# [ 67%] 5:kudu-tserver-1.7.0+cdh5.16.2+0-1.################################# [ 83%] 6:kudu-debuginfo-1.7.0+cdh5.16.2+0-################################# [100%] 创建目录 [root@hadoop000 ~]#mkdir /data/kudu/kudu_master_dat [root@hadoop000 ~]#mkdir /data/kudu/kudu_tserver_data 将/data/kudu目录的权限修改为kudu [root@hadoop000 ~]# chown -R kudu:kudu /data/kudu 修改配置 进入/etc/kudu/conf 1） vi master.gflagfile 12345--trusted_subnets=0.0.0.0/0--fs_wal_dir=/data/kudu/kudu_master_data--fs_data_dirs=/data/kudu/kudu_master_data #设置备份数 不设置默认为3--default_num_replicas=1 2） vi tserver.gflagfile 123456--trusted_subnets=0.0.0.0/0--fs_wal_dir=/data/kudu/kudu_tserver_data--fs_data_dirs=/data/kudu/kudu_tserver_data #设置备份数 不设置默认为3--default_num_replicas=1--tserver_master_addrs=hadoop000:7051 启动/停止kudu 12/etc/init.d/kudu-master start/stop/etc/init.d/kudu-tserver start/stop 查看WEB-UI 通过master的8051端口查看WEB-UI：http://hadoop000:8051/","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Kudu","slug":"Kudu","permalink":"https://zzuuriel.github.io/tags/Kudu/"}]},{"title":"CDH5.16.1安装Spark2.x，简称CDS安装","date":"2019-10-26T14:19:36.000Z","path":"大数据技术/CDH5.16.1安装Spark2.x，简称CDS安装/","text":"CDH5.16.1安装Spark2.x，简称CDS安装 官方文档 官网的文档地址：https://docs.cloudera.com/documentation/ 点击上面的Apache Spark2 如下图，第一点是安装Spark的一些版本要求，我们点进去看下 如下图，是安装spark之前的一些版本要求，自己去看下对应关系 安装CDS 安装Service Descriptor 步骤a 先要安装service descriptor，我们点击链接进去看看 先下载service descriptor 1wget http://archive.cloudera.com/spark2/csd/SPARK2_ON_YARN-2.4.0.cloudera2.jar 然后顺便把包裹Parcel也下载了，点击右边的链接 下载命令 123wget http://archive.cloudera.com/spark2/parcels/2.4.0.cloudera2/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012-el7.parcelwget http://archive.cloudera.com/spark2/parcels/2.4.0.cloudera2/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012-el7.parcel.sha1wget http://archive.cloudera.com/spark2/parcels/2.4.0.cloudera2/manifest.json 下载之后的文件 步骤b 然后我们接着安装service descriptor，刚刚步骤a我们已经下载好了 从上图可以看出，步骤b是让我们登陆到Cloudera Manager Server主机，然后把下载好的service descriptor拷贝到location configured，我们点进去看看位置在哪里 创建目录 1mkdir /opt/cloudera/csd 移动service descriptor文件 1mv SPARK2_ON_YARN-2.4.0.cloudera2.jar /opt/cloudera/csd 步骤c 设置service descriptor文件的拥有者和权限644 12chown -R cloudera-scm:cloudera-scm /opt/cloudera/csd/SPARK2_ON_YARN-2.4.0.cloudera2.jarchmod -R 644 /opt/cloudera/csd/SPARK2_ON_YARN-2.4.0.cloudera2.jar 步骤d 重启Cloudera Manager Server 1sudo systemctl restart cloudera-scm-server 添加包裹仓库(parcel repository) 第二点，我们已经做了，下面我们看第三点 上面描述，在CM管理员控制界面，添加远程仓库URL，我们看下面的Note注意：如果我们的CMS不能互联网访问，可以把parcel文件放到一个新的仓库，也就是我们自己内部搭建一个仓库地址，然后内网安装（因为有的企业只能内网集群），我们点击new parcel repository 下面我们就跟着官网一起配置自己的http服务 先安装httpd 1sudo yum install -y httpd 然后启动httpd 1systemctl start httpd 下面这一步是要我们下载parcel和manifest.json,这个下载，我们刚刚在前面已经下载了 下图，让我们把.parcel文件和manifest.json文件移动到我们刚刚安装的httpd server目录 然后我们执行下面的命令： 1234sudo mkdir /var/www/html/spark2_parcelsudo mv *.parcel* /var/www/html/spark2_parcelsudo mv manifest.json /var/www/html/spark2_parcelsudo chmod -R ugo+rX /var/www/html/spark2_parcel 然后我们查看httpd服务，看能不能访问到parcel 从下图可以看出，我们自己的parcel repository搭建好了 CMS配置Parcel URL 继续看官网的操作说明 进入cloudera manager界面 点击Parcels 点击Configuration 然后我们继续看看官网说怎么配置 意思是让我们输入我们自己的httpd搭建好的包裹文件路径，然后保证Changes 到这里可以看到Spark,本地URL已经设置好了 添加Spark2 服务 点击Add Service spark2出现了 选择History Server节点 选择hadoop003，因为其他机器服务较多，以后提交spark作业，到hadoop003提交即可。 然后一直下一步即可 最后重启下yarn即可，因为yarn不仅要支持mapreduce,还要支持spark on yarn 到这里安装结束 运行example cloudera公司Spark的文档：https://docs.cloudera.com/documentation/spark2/latest.html 点进去看看,spark2的命令为spark2-submit 用CRT连接hadoop003机器，因为我们刚刚在hadoop003安装spark2的 1234567spark2-submit \\--master yarn \\--executor-memory 1G \\--executor-cores 1 \\--num-executors 1 \\--class org.apache.spark.examples.SparkPi \\/opt/cloudera/parcels/SPARK2/lib/spark2/examples/jars/spark-examples_2.11-2.4.0.cloudera2.jar 执行的时候出现如下的错误(遇到错误的时候，不要害怕，看日志)： Exception in thread “main” java.lang.IllegalArgumentException: Required executor memory (1024), overhead (384 MB), and PySpark memory (0 MB) is above the max threshold (1208 MB) of this cluster! Please check the values of ‘yarn.scheduler.maximum-allocation-mb’ and/or ‘yarn.nodemanager.resource.memory-mb’ 从上面错误可以看出，是内存不够，而且上面已经给我们提示，这个参数需要检查一下 再设置下Container内存，因为executor是跑在Container里的 重启yarn之后，我们再执行 1234567spark2-submit \\--master yarn \\--executor-memory 1G \\--executor-cores 1 \\--num-executors 1 \\--class org.apache.spark.examples.SparkPi \\/opt/cloudera/parcels/SPARK2/lib/spark2/examples/jars/spark-examples_2.11-2.4.0.cloudera2.jar 发现又出现如下错误： org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode=&quot;/user&quot;:hdfs:supergroup:drwxr-xr-x 上面错误的意思是：权限拒绝，我们使用的用户root，期望进行写的操作 但是节点只允许hdfs用户，进行写，那么我们切换到hdfs去执行名 执行结果如下，说明我们的spark2可以正常使用","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"},{"name":"CDH","slug":"CDH","permalink":"https://zzuuriel.github.io/tags/CDH/"}]},{"title":"CDH安装Kafka","date":"2019-10-24T14:19:36.000Z","path":"大数据技术/CDH安装Kafka,简称CDK安装/","text":"CDH安装Kafka 下载包裹文件 下载地址：http://archive.cloudera.com/kafka/parcels/4.1.0/ 创建下载的目录 12[root@hadoop001 ~]# mkdir kafka_parcel[root@hadoop001 ~]# cd kafka_parcel/ 下载三个文件 123wget http://archive.cloudera.com/kafka/parcels/4.1.0/KAFKA-4.1.0-1.4.1.0.p0.4-el7.parcelwget http://archive.cloudera.com/kafka/parcels/4.1.0/KAFKA-4.1.0-1.4.1.0.p0.4-el7.parcel.sha1wget http://archive.cloudera.com/kafka/parcels/4.1.0/manifest.json 下载完成后，把.sha1改成.sha 1mv KAFKA-4.1.0-1.4.1.0.p0.4-el7.parcel.sha1 KAFKA-4.1.0-1.4.1.0.p0.4-el7.parcel.sha 安装httpd 这里配置httpd是为了制作本地仓库，后续可以通过界面配置进行分发 安装httpd 1yum install -y httpd 然后启动httpd服务 1systemctl start httpd 查看是否安装成功 1systemctl status httpd web界面再看下是否成功，默认是80端口 然后移动包裹文件 1mv kafka_parcel /var/www/html/ 然后查看下本地仓库地址，打开http://hadoop001/kafka_parcel/ 对parcel进行分发和激活 首先配置下parcel本地仓库地址 单击Configuration 配置kafka的url 然后点击下载 点击Distribute分发 点击Activate激活 添加服务 进入cloudera manager，点击add service 选择kafka 选择kafka broker 选择要部署的机器 点击Continue 设置kafka元数据存储在zk的目录 到这里kafka安装成功 测试数据 1.先创建一个topic 123456/opt/cloudera/parcels/KAFKA/lib/kafka/bin/kafka-topics.sh \\--create \\--zookeeper hadoop001:2181,hadoop002:2181,hadoop003:2181/kafka \\--replication-factor 3 \\--partitions 3 \\--topic test_topic 2.使用生产者 123./kafka-console-producer.sh \\--broker-list hadoop001:9092,hadoop002:9092,hadoop003:9092 \\--topic test_topic 3.使用消费者 1234./kafka-console-consumer.sh \\--bootstrap-server hadoop001:9092,hadoop002:9092,hadoop003:9092 \\--from-beginning \\--topic test_topic","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"CDH","slug":"CDH","permalink":"https://zzuuriel.github.io/tags/CDH/"},{"name":"Kafka","slug":"Kafka","permalink":"https://zzuuriel.github.io/tags/Kafka/"}]},{"title":"HBase架构和读写流程","date":"2019-08-31T12:12:36.000Z","path":"大数据技术/HBase架构和读写流程/","text":"HBase架构和读写流程 HBase 简介 HBase 是一个分布式，可扩展，面向列的适合存储海量数据的数据库，其最主要的功能是解决海量数据下的实时随机读写的问题。 通常HBase依赖HDFS做为底层分布式文件系统，本文以此做前提并展开，详细介绍HBase的架构和读写流程。 HBase关键进程 HBase是一个Master/Slave架构的分布式数据库，内部主要有HMaster， HRegionServer两个核心服务，依赖HDFS做底层存储，依赖zookeeper做一致性等协调工作。 HMaster是一个轻量级进程，负责所有DDL操作，负载均衡，region信息管理，并在宕机恢复中起主导作用。 HRegionServer管理HRegion，与客户端点对点通信，负责实时数据的读写。 zookeeper做HMaster选举，关键信息如meta-region地址，replication进度，Regionserver地址与端口等存储。 HBase架构 HBase架构图如下： 架构浅析: 1.HMaster 负责管理HBase元数据，即表的结构、表存储的Region等元信息。 负责表的创建，删除和修改（因为这些操作会导致HBase元数据的变动）。 负责为HRegionServer分配Region，分配好后也会将元数据写入相应位置。 负责HRegionserver失效后的region迁移等。 如果对可用性要求较高，它需要做HA高可用（通过Zookeeper）。但是HMaster不会去处理Client端的数据读写请求，因为这样会加大其负载压力，具体的读写请求它会交给HRegionServer来做。 2.HRegionServer 一个RegionServer里有多个Region。 处理Client端的读写请求（根据从HMaster返回的元数据找到对应的Region来读写数据）。 管理Region的Split分裂、StoreFile的Compaction合并。 RegionServer要和【DN】一起部署 一个RegionServer管理着多个Region对象，在HBase运行期间，可以动态添加、删除HRegionServer 3.HRegion 一个HRegion里可能有1个或多个Store，是通过列族划分的。 HRegionServer维护一个HLog。 HRegion是分布式存储和负载的最小单元。 每个Hregion对应table的一个region，表通常被保存在多个HRegionServer的多个HRegion中。 因为HBase用于存储海量数据，故一张表中数据量非常之大，单机一般存不下这么大的数据，故HBase会将一张表按照行水平将大表划分为多个Region，每个Region保存表的一段连续数据。 初始只有1个Region，当一个Region增大到某个阈值后，便分割为两个。 4.Store Store是存储落盘的最小单元，由内存中的MemStore和磁盘中的若干StoreFile组成。 一个Store里有1个或多个StoreFile和一个memStore。 每个Store存储一个列族。 5.HLog 预写日志，Write Ahead Log，WAL。 每个HRegionServer中都有一个HLog对象。 HLog记录数据的所有变更，可以用来做数据恢复。 6.MemStore MemStore作为HBase的写缓存，保存着数据的最近一次更新，同时是HBase能够实现高性能随机读写的重要组成。 HBase Table的每 Column family 维护一个MemStore，当满足一定条件时MemStore会执行一次flush，文件系统中生成新的HFile。而每次Flush的最小单位是Region。 7.StoreFile memStore内存中的数据写到文件后就是StoreFile（即memstore的每次flush操作都会生成一个新的StoreFile），StoreFile底层是以HFile的格式保存。 storefiles合并后逐步形成越来越大的storefile文件，当region内所有的storefile(hfile)的总大小超过 hbase.hregion.max.filesize触发split,一个region变为2个。 父region下线，新的split的2个region被HMaster分配到合适的RegionServer机器上。使得原先1个region的压力分流到2个region上。 8.Block Cache Block Cache作为HBase的读缓存，会将一次文件查找的Block块缓存到Cache中，以便后续同一请求或者邻近数据查找请求，可以直接从内存中获取，避免昂贵的IO操作 Block Cache是RegionServer级别 ，一个RegionServer只有一个Block Cache ,在RegionServer启动时完成Block Cache的初始化工作。 9.zookeeper 集群存储meta表的地址，而不是内容。 regionserver主动向zk组成，使得master可随时感知各个RegionServer的健康状态。 避免master单点故障(SPOF)。 10.HBase Client HBase Client使用HBase的RPC机制与HMaster和HRegionServer进行通信。 对于管理类操作(DDL)，Client与HMaster进行RPC;对于数据读写类操作(DML)，Client与HRegionServer进行RPC。 写流程 HBase写流程 HBase写流程图如下： Client访问ZK，根据ROOT表获取hbase:meta表所在Region的位置信息，并将该位置信息写入Client Cache。（注：为了加快数据访问速度，我们将元数据、Region位置等信息缓存在Client Cache中。） Client读取meta表，再根据meta表中查询得到的Namespace、表名和RowKey等相关信息，获取将要写入Region的位置信息，最后client端会将meta表写入Client Cache。 Client向上一步HRegionServer发出写请求，HRegionServer先将操作和数据写入HLog（预写日志，Write Ahead Log，WAL），再将数据写入对应的region的store的MemStore，MemStore里面的数据也是对rowkey进行字典排序的。（联想：HDFS中也是如此，EditLog写入时机也是在真实读写之前发生） 当MemStore的数据量超过阈值时，将数据溢写磁盘，生成一个StoreFile文件。 当Store中StoreFile的数量超过阈值时，将若干小StoreFile合并（Compact）为一个大StoreFile。 当Region中最大Store的大小超过阈值时，Region分裂（Split），等分成两个子Region。 memstore flush触发条件（调优关键） memstore级别限制： 当region的任意一个store的memstore的size，达到 hbase.hregion.memstore.flush.size（默认128M）， 会触发memstore flush操作,将数据溢写磁盘，生成一个StoreFile文件。 region级别限制: 当region所有的memstore的size和，达到 hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size= 4*128=512M 会触发memstore flush，同时会阻塞所有的写入该store的写请求！ 继续对该region写请求，会抛错 region too busy exception异常。 regionserver级别限制： 1）当rs节点上所有的memstore的size和 ，超过低水位线阈值 hbase.regionserver内存大小* hbase.regionserver.global.memstore.size* hbase.regionserver.global.memstore.size.lower.limit =48G * 0.45 * 0.91 = 19.656G， rs强制执行flush。先flush memstore最大的region，再第二大的， 直到总的memstore大小下降到低水位线的阈值。 2）如果此时写入非常繁忙，导致总的memstore大小达到 hbase.regionserver内存大小* hbase.regionserver.global.memstore.size = 21.6G rs会阻塞写、读的请求，并强制flush，达到低水位阈值(安全阈值) Hlog级别限制 当rs的hlog数量达到hbase.regionserver.max.logs 32 会选择最早的hlog的对应的一个或多个region进行flush。 定期级别限制 hbase.regionserver.optionalcacheflushinterval 默认1h 为避免所有的memstore在同一个时间点进行flush导致的问题， 定期的flush其实会有一定的随机时间延时。设置为0就是禁用。 手动级别限制 flush命令，可以封装脚本。 总结： 在生产上，唯独触发rs级别的限制导致flush,是属于灾难级别的，会阻塞所有落在该rs节点的读写请求，直到总的memstore大小降到低水位线，阻塞时间较长；其他的级别限制，只会阻塞对应的region的读写请求，阻塞时间较短。 读流程 HBase读流程图如下： 先去zk获取hbase:meta表所在的rs节点， 在hbase:meta表根据读rk确定所在的目标rs节点和region 将读请求封装，发送给目标的rs节点，进行处理。 先到memstore查数据，查不到到blockcache查，再查不到就访问磁盘的StoreFile读数据。 compaction 合并 每次flush操作都是将一个memstore数据写到HFile文件，导致hdfs上有很多的hfile文件，小文件多了对后面的读操作有影响，所以hbase会定时将hfile文件合并。 分类 Compaction分为两种： minor compaction 小合并: 选取部分小的 相邻的hfile合并为一个更大的hfile major compaction 大合并：将一个store的所有的hfile文件合并一个hfile。 major compaction过程会清理： 1）TTL过期数据 2）版本号超过设定的数据 3）被删除的数据(delete) 所以 major compaction持续时间较长，整个过程消费大量的系统资源（带宽和短时间的IO压力），对上层业务会有较大的影响！ 生产上尽可能的避免发生major compaction，一般通过关闭自动触发大合并，改为手动触发，在业务低谷时期，执行。（一般在凌晨调度脚本，去执行） 作用 随着hflie文件越来越多，查询需要更多的IO，读取延迟较大。所以需要compaction，主要为了消费带宽和短时间的IO压力，来换取以后查询的低延迟。 合并作用： 合并小文件 减少文件数量 减小稳定度的延迟。 消除无效数据 降低存储空间。 提高数据的本地化率。 触发条件 1.minor compaction合并的触发条件： 1）memstore flush： 合并根源来自flush，当memstore达到阈值或者其他条件就触发flush，将数据写到hflie，正是因为文件多，才需要合 并。 每次flush之后，就当前的store文件数进行校验判断，一旦store的总文件数超过 hbase.hstore.compactionThreshold（默认3），就触发合并，该参数一般需要调大。 一次minor cmpaction最多合并hbase.hstore.compaction.max个文件（默认值10）。 2）后台线程定期检查 后台线程compactchecker 定期检查是否需要执行合并。检查周期为 hbase.server.thread.wakefrequency* hbase.server.compactchecker.interval.multiplier =10000ms*1000。在不做参数修改情况的下，compactchecker 大概是2h,46min,40s执行一次。 当文件小于 hbase.hstore.compaction.min.size（默认128M）会被立即添加到合并的队列。 当合并队列中的StoreFile数量超过参数hbase.hstore.compaction.min（更早的版本中这个的参数的名字为hbase.hstore.compactionThreshold）的值（默认3）时会触发compaction操作。一次minor cmpaction最多合并hbase.hstore.compaction.max个文件（默认值10）。 如果一个文件的大小超过hbase.hstore.compaction.max.size的值（默认值LONG.MAX_VALUE），则会被compaction操作排除。 通过hbase.hstore.compaction.ratio参数（默认值1.2）确定大小超过hbase.hstore.compaction.min.size的文件是否需要进行compaction。如果一个文件的大小小于它后面（按文件产生的先后顺序，总是从新产生的文件开始选择即“老文件”）的hbase.hstore.compaction.max个StoreFile的大小之和乘以hbase.hstore.compaction.ratio，则该StoreFile文件也会加入到合并队列中。 12345678910111213141516171819202122232425&lt;!--表示至少需要三个满足条件的store file时，minor compaction才会启动--&gt;&lt;property&gt; &lt;name&gt;hbase.hstore.compactionThreshold&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!--表示一次minor compaction中最多选取10个store file--&gt;&lt;property&gt; &lt;name&gt;hbase.hstore.compaction.max&lt;/name&gt; &lt;value&gt;10&lt;/value&gt;&lt;/property&gt;&lt;!--默认值为128M,表示文件大小小于该值的store file 一定会加入到minor compaction的store file中--&gt;&lt;property&gt; &lt;name&gt;hbase.hstore.compaction.min.size&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt;&lt;/property&gt;&lt;!--默认值为LONG.MAX_VALUE，表示文件大小大于该值的store file 会被minor compaction排除--&gt;&lt;property&gt; &lt;name&gt;hbase.hstore.compaction.max.size&lt;/name&gt; &lt;value&gt;9223372036854775807&lt;/value&gt;&lt;/property&gt; 2.major compaction合并的触发条件： 12345&lt;!--默认值为7天进行一次大合并，--&gt;&lt;property&gt; &lt;name&gt;hbase.hregion.majorcompaction&lt;/name&gt; &lt;value&gt;604800000&lt;/value&gt;&lt;/property&gt; 一般是使用命令major_compact手动进行，将参数hbase.hregion.majorcompaction的值设为0，表示禁用major compaction。其默认值为7天。 生产上： hbase.hregion.majorcompaction 0 大合并关闭 hbase.hstore.compactionThreshold 6（默认3） 小合并","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://zzuuriel.github.io/tags/HBase/"}]},{"title":"HBase的Rowkey设计","date":"2019-08-29T12:12:36.000Z","path":"大数据技术/HBase的Rowkey设计/","text":"HBase的Rowkey设计 为什么Rowkey这么重要 RowKey 到底是什么 我们常说看一张 HBase 表设计的好不好，就看它的 RowKey 设计的好不好。可见 RowKey 在 HBase 中的地位。那么 RowKey 到底是什么？RowKey 的特点如下： 类似于 MySQL、Oracle中的主键，用于标示唯一的行； 完全是由用户指定的一串不重复的字符串； HBase 中的数据永远是根据 Rowkey 的字典排序来排序的 RowKey的作用 读写数据时通过 RowKey 找到对应的 Region； MemStore 中的数据按 RowKey 字典顺序排序； HFile 中的数据按 RowKey 字典顺序排序。 Rowkey对查询的影响 如果我们的 RowKey 设计为 uid+phone+name，那么这种设计可以很好的支持以下的场景： uid = 111 AND phone = 123 AND name = iteblog uid = 111 AND phone = 123 uid = 111 AND phone = 12? uid = 111 难以支持的场景： phone = 123 AND name = iteblog phone = 123 name = iteblog Rowkey对Region划分影响 HBase 表的数据是按照 Rowkey 来分散到不同 Region，不合理的 Rowkey 设计会导致热点问题。热点问题是大量的 Client 直接访问集群的一个或极少数个节点，而集群中的其他节点却处于相对空闲状态。 如果Region1 上的数据是 Region 2 的5倍，这样会导致Region1 的访问频率比较高，进而影响这Region1 所在机器的其他 Region。 RowKey设计技巧 我们如何避免上面说到的热点问题呢？下面介绍三种方法。 避免热点的方法 - Salting 这里的加盐不是密码学中的加盐，而是在rowkey 的前面增加随机数。具体就是给 rowkey 分配一个随机前缀 以使得它和之前排序不同。分配的前缀种类数量应该和你想使数据分散到不同的 region 的数量一致。 如果你有一些 热点 rowkey 反复出现在其他分布均匀的 rwokey 中，加盐是很有用的。考虑下面的例子：它将写请求分散到多个 RegionServers，但是对读造成了一些负面影响。 假如你有下列 rowkey，你表中每一个 region 对应字母表中每一个字母。 以 ‘a’ 开头是同一个region, 'b’开头的是同一个region。在表中，所有以 'f’开头的都在同一个 region， 它们的 rowkey 像下面这样： 1234foo0001foo0002foo0003foo0004 现在，假如你需要将上面这个 region 分散到 4个 region。你可以用4个不同的盐：‘a’, ‘b’, ‘c’, ‘d’.在这个方案下，每一个字母前缀都会在不同的 region 中。加盐之后，你有了下面的 rowkey: 1234a-foo0003b-foo0001c-foo0004d-foo0002 所以，你可以向4个不同的 region 写。理论上说，如果这四个 Region 存放在不同的机器上，经过加盐之后你将拥有之前4倍的吞吐量。 现在，如果再增加一行，它将随机分配a,b,c,d中的一个作为前缀，并以一个现有行作为尾部结束： 12345a-foo0003b-foo0001c-foo0003c-foo0004d-foo0002 因为分配是随机的，所以如果你想要以字典序取回数据，你需要做更多工作。加盐这种方式增加了写时的吞吐量，但是当读时有了额外代价。 避免热点的方法 - Hashing Hashing 的原理是计算 RowKey 的 hash 值，然后取 hash 的部分字符串和原来的 RowKey 进行拼接。这里说的 hash 包含 MD5、sha1、sha256或sha512等算法。比如我们有如下的 RowKey: 1234foo0001foo0002foo0003foo0004 我们使用 md5 计算这些 RowKey 的 hash 值，然后取前 6 位和原来的 RowKey 拼接得到新的 RowKey： 123495f18cfoo00016ccc20foo0002b61d00foo00031a7475foo0004 优缺点：可以一定程度打散整个数据集，但是不利于 Scan；比如我们使用 md5 算法，来计算Rowkey的md5值，然后截取前几位的字符串。subString(MD5(设备ID), 0, x) + 设备ID，其中x一般取5或6。 避免热点的方法 - Reversing Reversing 的原理是反转一段固定长度或者全部的键。比如我们有以下 URL ，并作为 RowKey： 1234flink.iteblog.comwww.iteblog.comcarbondata.iteblog.comdef.iteblog.com 这些 URL 其实属于同一个域名，但是由于前面不一样，导致数据不在一起存放。我们可以对其进行反转，如下： 1234moc.golbeti.knilfmoc.golbeti.wwwmoc.golbeti.atadnobracmoc.golbeti.fed 经过这个之后，这些 URL 的数据就可以放一起了。 RowKey的长度 RowKey 可以是任意的字符串，最大长度64KB（因为 Rowlength 占2字节）。建议越短越好，原因如下： 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率； MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率； 目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。 RowKey 设计案例剖析 交易类表 Rowkey 设计 查询某个卖家某段时间内的交易记录 sellerId + timestamp + orderId 查询某个买家某段时间内的交易记录 buyerId + timestamp ＋orderId 根据订单号查询 orderNo 如果某个商家卖了很多商品，可以如下设计 Rowkey 实现快速搜索 salt + sellerId + timestamp 其中，salt 是随机数。 可以支持的场景： 1)全表 Scan 2)按照 sellerId 查询 3)按照 sellerId + timestamp 查询 金融风控 Rowkey 设计 查询某个用户的用户画像数据 prefix + uid prefix + idcard prefix + tele 其中 prefix = substr(md5(uid),0 ,x)， x 取 5-6。uid、idcard以及 tele 分别表示用户唯一标识符、身份证、手机号码。 车联网 Rowkey 设计 查询某辆车在某个时间范围的交易记录 carId + timestamp 某批次的车太多，造成热点 prefix + carId + timestamp 其中 prefix = substr(md5(uid),0 ,x) 查询最近的数据 查询用户最新的操作记录或者查询用户某段时间的操作记录，RowKey 设计如下： uid + Long.Max_Value - timestamp 查询用户最新的操作记录 Scan [uid] startRow [uid][000000000000] stopRow [uid][Long.Max_Value - timestamp] 查询用户某段时间的操作记录 Scan [uid] startRow [uid][Long.Max_Value – startTime] stopRow [uid][Long.Max_Value - endTime] 如果 RowKey 无法满足我们的需求，可以尝试二级索引。Phoenix、Solr 以及 ElasticSearch 都可以用于构建二级索引","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://zzuuriel.github.io/tags/HBase/"}]},{"title":"Spark Streaming+Kafka提交服务器&窗口函数&SS调优","date":"2019-08-25T14:17:36.000Z","path":"大数据技术/Spark Streaming+Kafka提交服务器&窗口函数&SS调优/","text":"Spark Streaming+Kafka提交服务器&amp;窗口函数&amp;SS调优 Spark Streaming+Kafka提交服务器 开发好的代码打包到服务器运行时，有时会缺少相关依赖，下面列举了三种添加依赖的方法： 直接在IDEA中打胖包，但是服务器上有的东西需要标识为provided，不然依赖重复了，这种方式不推荐 提交Application的时候使用–packages参数，格式为: groupId:artifactId:version，这种方式需要在有网络的情况下才能使用 推荐使用–jars 传入依赖，当需要传入的jar包过多时，可以将需要的 jar 包放在固定目录下，只需要使用 $(echo /home/hadoop/lib/*.jar | tr ‘ ‘ ‘,’)即可以将目录下的 jar 包全都拼接上去 1.WC案例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849object StreamingWCApp &#123; var groupId:String = _ var topic:String = _ var brokers:String = _ def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() .setMaster(\"local[2]\") //本地测试用，打包到服务器时传入 .setAppName(this.getClass.getSimpleName) //打包到服务器时传入 .set(\"spark.streaming.kafka.maxRatePerPartition\",\"10\") //设置每个Kafka分区读取数据的最大速率 .set(\"spark.streaming.backpressure.enabled\",\"true\") //开启背压 .set(\"spark.streaming.stopGracefullyOnShutdown\",\"true\") //优雅地关闭“StreamingContext” val ssc = new StreamingContext(conf, Seconds(5)) if (args.length&lt;3)&#123; throw new IllegalArgumentException(\"Usage: com.tunan.spark.streming.kafka.wc.wc_ss_kafka &lt;groupId&gt; &lt;topic&gt; &lt;brokers&gt;\") &#125; groupId = args(0) topic = args(1) brokers = args(2) val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; brokers, \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; groupId, \"auto.offset.reset\" -&gt; \"earliest\", //latest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(topic) val stream = KafkaUtils.createDirectStream[String, String]( ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) stream.map(x=&gt;(x.value(),1)).reduceByKey(_+_).foreachRDD(rdd =&gt;&#123; rdd.foreach(println) &#125;) ssc.start() ssc.awaitTermination() &#125;&#125; 2.打包到服务器前在IDEA进行测试 Program arguments:wc_group_id_for_each_stream ruozedata3partstopic hadoop000:9092,hadoop000:9093,hadoop000:9094 3.在libtest下传入依赖的jar包 1234[hadoop@hadoop000 libtest]$ lltotal 1988-rw-r--r-- 1 hadoop hadoop 1893564 Aug 22 11:09 kafka-clients-2.0.0.jar-rw-r--r-- 1 hadoop hadoop 138970 Aug 22 11:09 spark-streaming-kafka-0-10_2.12-2.4.5.jar 4.提交作业 1234567spark-submit \\--class com.ruozedata.spark.streaming.StreamingWCApp \\--master local[2] \\--deploy-mode client \\--jars $(echo /home/hadoop/libtest/*.jar | tr ' ' ',') \\/home/hadoop/lib/ruozedata-spark-streaming-1.0.jar \\wc_group_id_for_each_stream ruozedata3partstopic hadoop000:9092,hadoop000:9093,hadoop000:9094 窗口函数 在工作中常常有这样的需求: 每隔5秒钟统计前10秒钟的数据 每隔1分钟统计前5分钟的数据 这类每隔多少统计前多少时间的操作就是窗口操作 我们以一个例子来说明窗口操作。 对之前的单词计数的示例进行扩展，每10秒钟对过去30秒的数据进行wordcount。为此，我们必须在最近30秒的DStream数据中对键值对应用reduceByKey操作。这是通过使用reduceByKeyAndWindow操作完成的 12345// 每隔10秒统计前30秒的数据stream.map(x=&gt;(x.value(),1)).reduceByKeyAndWindow((a:Int,b:Int)=&gt;a+b,Seconds(30),Seconds(10)).foreachRDD(rdd =&gt;&#123; rdd.foreach(println)&#125;) 一些常见的窗口操作如下所示。所有这些操作都用到了两个参数：windowLength和slideInterval。 window(windowLength, slideInterval) 基于源DStream产生的窗口化的批数据计算一个新的DStream countByWindow(windowLength, slideInterval) 返回流中元素的一个滑动窗口数 reduceByWindow(func, windowLength, slideInterval) 返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数必须是相关联的以使计算能够正确的并行计算。 reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks]) 应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每一个key的值均由给定的reduce函数聚集起来。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。你可以用numTasks参数设置不同的任务数 reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks]) 上述reduceByKeyAndWindow()的更高效的版本，其中使用前一窗口的reduce计算结果递增地计算每个窗口的reduce值。这是通过对进入滑动窗口的新数据进行reduce操作，以及“逆减（inverse reducing）”离开窗口的旧数据来完成的。一个例子是当窗口滑动时对键对应的值进行“一加一减”操作。但是，它仅适用于“可逆减函数（invertible reduce functions）”，即具有相应“反减”功能的减函数（作为参数invFunc）。 像reduceByKeyAndWindow一样，通过可选参数可以配置reduce任务的数量。 请注意，使用此操作必须启用检查点。 countByValueAndWindow(windowLength, slideInterval, [numTasks]) 应用到一个(K,V)对组成的DStream上，返回一个由(K,V)对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率 Spark Streaming调优 通过spark-shell运行Spark Streaming + Kafka程序，查看Web UI界面 1.我们需要作业的性能最高，那么需要一个最佳batch time 在下一个批次启动作业之前一定要运行完前一个批次数据的处理 根据需求确定合适的batch time 2.影响任务运行时长的要素： 数据规模(增加kafka分区数==&gt;增加Spark分区==&gt;增加task) batch time 业务复杂度 3.kafka限速 我们看到在设置auto.offset.reset = earliest后，即从头消费，如果累积的数据量特别大，那么在第一次消费的就会撑爆Kafka，必须限制从每个Kafka分区读取数据的速率 Property Name Default Meaning spark.streaming.kafka.maxRatePerPartition not set 每个Kafka分区读取数据的最大速率 在设置maxRatePerPartition的值时，数据量=设置的值分区数读取时间，加入设置的值为10,分区为3，读取时间为10s，那么每个批次出来的数据量: 103*10=300 优点 当有很多数据量没有处理，或者每次都从头开始数据的时候，可以防止过载 高峰期限速，防止Kafka处理能力不够挂掉 缺点 是个固定值 基于于spark.streaming.kafka.maxRatePerPartition的局限性，在Spark1.5引入了背压(backpressure )的概念，，它可以在运行时根据前一个批次数据的运行情况动态调整后续批次读入的数据量 打开参数：spark.streaming.backpressure.enabled 上限参数：spark.streaming.kafka.maxRatePerPartition 初始参数：spark.streaming.backpressure.initialRate 到此，Kafka数据量过载的问题完全解决 4.最后引入一个关于StreamingContext关闭时的参数","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"},{"name":"Kafka","slug":"Kafka","permalink":"https://zzuuriel.github.io/tags/Kafka/"}]},{"title":"Spark Streaming读取kafka数据的两种方式(Receiver和Direct)","date":"2019-08-08T14:17:36.000Z","path":"大数据技术/Spark Streaming读取kafka数据的两种方式(Receiver和Direct)/","text":"Spark Streaming读取kafka数据的两种方式（Receiver和Direct） 概述 Spark Streaming从Kafka中读取数据的方式分有两种，Receiver读取和Direct读取。 Receiver模式 Receiver是使用的高级API，需要消费者连接Zookeeper来读取数据。是由Zookeeper来维护偏移量，不用我们来手动维护，这样的话就比较简单一些，减少了代码量。但它也有很多缺点： 导致丢失数据。它是由Executor内的Receiver来拉取数据并存放在内存中，再由Driver端提交的job来处理数据。这样的话，如果底层节点出现错误，就会发生数据丢失。 浪费资源。可以采取WALs方式将数据同步到高可用数据存储平台上（HDFS，S3），那么如果再发生错误，就可以从中再次读取数据。但是这样会导致同样的数据存储了两份，浪费了资源。 可能会导致重复读取数据。对于公司来说，一些数据宁可丢失了一小小部分也不能重复读取，但是这种由Zookeeper来记录偏移量的方式，可能会因为Spark和Zookeeper不同步，导致一份数据读取了两次。 效率低。因为是分批次执行的，它是接收数据，直到达到了设定的时间间隔，才可是进行计算。而且我们在KafkaUtils.createStream()中设定的partition数量，只会增加receiver的数量，不能提高并行计算的效率，但我们可以设定不同的Group和Topic创建DStream，然后再用Union合并DStream，提高并行效率。 Direct模式 Direct方式则采用的是低层次的API，直接连接kafka服务器上读取数据。需要我们自己去手动维护偏移量，代码量稍微大些。不过这种方式的优点有： 当我们读取Topic下的数据时，它会自动对应Topic下的Partition生成相对应数量的RDD Partition，提高了计算时的并行度，提高了效率。 它不需要通过WAL来维持数据的完整性。采取Direct直连方式时，当数据发生丢失，只要kafka上的数据进行了复制，就可以根据副本来进行数据重新拉取。 它保证了数据只消费一次。因为我们将偏移量保存在一个地方，当我们读取数据时，从这里拿到数据的起始偏移量和读取偏移量确定读取范围，通过这些我们可以读取数据，当读取完成后会更新偏移量，这就保证了数据只消费一次。 总结 在spark1.3以后的版本中，Direct方式取代了Receiver方式，当然在生产上，使用的都是Direct方式。从上面对比也能看出Receiver方式的效率低，而且数据完整性也很让人担忧，当我们采取Direct方式时，完全不用为这两点所担忧，可以根据自己想读取的范围进行读取。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"},{"name":"Kafka","slug":"Kafka","permalink":"https://zzuuriel.github.io/tags/Kafka/"}]},{"title":"Spark Streaming对接Kafka偏移量管理","date":"2019-08-06T12:17:36.000Z","path":"大数据技术/Spark Streaming对接Kafka偏移量管理/","text":"Spark Streaming对接Kafka偏移量管理 官网：http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets 获取 Offsets 1234567stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges rdd.foreachPartition &#123; iter =&gt; val o: OffsetRange = offsetRanges(TaskContext.get.partitionId) println(s\"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;\") &#125;&#125; 请注意，hasoffsetranges 的类型转换只有在第一个方法中执行时才会成功，这个方法是在创建直接流的结果上调用的，而不是在以后的一系列方法中。 请注意，rdd 分区和 kafka 分区之间的一对一映射在任何进行洗牌或重新分区的方法(例如 reducebykey ()或 window ())之后不会保留。 存储 Offsets 首先我们了解一下Kafka消息的三种语义： At most once: 每个记录要么处理一次，要么根本不处理。==&gt;提交偏移量在业务处理之前,偏移量提交了，但是业务未处理 At least once: 每个记录将被处理一次或多次。这比最多一次强，因为它确保不会丢失任何数据。但是可能有重复的。==&gt;业务处理在提交偏移量之前，业务处理了，但是偏移量没有提交 Exactly once: 每条记录将被精确处理一次——没有数据会丢失，也没有数据会被多次处理。这显然是三者中最有力的保证。==&gt;pipeline，保证提交偏移量和业务处理同时成功 下面介绍3种方法存储Offsets的方法： 1.Checkpoints 使用Spark Streaming的checkpoint是最简单的存储方式，并且在Spark 框架中很容易实现。Spark Streaming checkpoints就是为保存应用状态而设计的，我们将路径这在HDFS上，所以能够从失败中恢复数据。 如果是应用挂掉的话，那么Spark Streamig应用功能可以从保存的offset中开始读取消息。但是，如果是对Spark Streaming应用进行升级的话，那么很抱歉，checkpoint的数据没法使用，所以这种机制并不可靠，特别是在严格的生产环境中，我们不推荐这种方式。 2.Kafka itself 这种方式支持大吞吐量的 offset 更新，又不需要手动编写 offset 管理程序或者维护一套额外的集群，但是Kafka不支持事物，不能保证输出的幂等性(Exactly once)。 123456stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges // some time later, after outputs have completed stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)&#125; 3.手动维护Offsets 对于支持事务的数据存储，在相同的事务中保存偏移量可以使两者保持同步，即使在故障情况下也是如此。 如果小心地检测重复或跳过的偏移量范围，回滚事务可以防止重复或丢失的消息影响结果。 这就等同于一次性语义。即使是聚合产生的结果也可以使用这种策略，因为聚合产生的结果通常很难产生幂等性(Exactly once)。 1234567891011121314151617181920212223242526// The details depend on your data store, but the general idea looks like this// begin from the offsets committed to the databaseval fromOffsets = selectOffsetsFromYourDatabase.map &#123; resultSet =&gt; new TopicPartition(resultSet.string(\"topic\"), resultSet.int(\"partition\")) -&gt; resultSet.long(\"offset\")&#125;.toMapval stream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets))stream.foreachRDD &#123; rdd =&gt; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges val results = yourCalculation(rdd) // begin your transaction // update results // update offsets where the end of existing offsets matches the beginning of this batch of offsets // assert that offsets were updated correctly // end your transaction&#125; Redis实现Offsets幂等性消费 下面我们用Redis实现Offsets幂等性消费，手动维护Offsets 1.OffsetsManager Trait 定义一个接口，用来获取和存储Offsets 1234567trait OffsetsManager &#123; def obtainOffsets(topic: String, groupId: String): Map[TopicPartition, Long] def storeOffsets(pipeline: Pipeline, offsetRanges: Array[OffsetRange], groupId: String): Unit&#125; 2.RedisOffsetsManager Object 使用Redis实现Offsets的获取和存储，定义一个RedisOffsetsManager Object继承OffsetsManager Trait 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * offsetsManager实现类 * 1) 获取offset * 2) 保存offset */object RedisOffsetsManager extends OffsetsManager &#123; var isReset = false /** * 从redis、kafka获取offset * * if(redisOffset&gt;kafkaOffset) * return allPartition.offset)=0 * * @param topic * @param groupId * @return */ override def obtainOffsets(topic: String, groupId: String): Map[TopicPartition, Long] = &#123; var offsets = Map[TopicPartition, Long]() //返回值 val jedis = RedisUtils.getJedis //1.1 从redis获取offset val redisMap: util.Map[String, String] = jedis.hgetAll(topic + \"_\" + groupId) //1.2 从kafka获取到的offset val kafkaMap: mutable.Map[TopicPartition, lang.Long] = KafkaOffsetsTool.getLastOffset(topic) //2、进行比较判断，如果fromOffset&gt;untilOffset isReset=true kafkaMap.foreach(x =&gt; &#123; val redisOffset = redisMap.getOrDefault(x._1.partition() + \"\", \"0\") if (redisOffset.toInt &gt; x._2) &#123; isReset = true &#125; &#125;) //redisOffset转换为fromOffset //if isReset==true 设置所有partition的offset为0 import scala.collection.JavaConversions._ redisMap.foreach(pair =&gt; &#123;// val redisMap: util.Map[String, String] = jedis.hgetAll(topic + \"_\" + groupId) val topicPartition = new TopicPartition(topic, pair._1.toInt) var offset = pair._2 if (isReset == true) &#123; //重置offset offset = \"0\" &#125; offsets += topicPartition -&gt; offset.toLong &#125;) offsets &#125; /** * 保存offset到Redis * * @param offsetRanges * @param groupId */ override def storeOffsets(pipeline: Pipeline, offsetRanges: Array[OffsetRange], groupId: String): Unit = &#123; offsetRanges.foreach(o =&gt; &#123; pipeline.hset(o.topic + \"_\" + groupId, o.partition + \"\", o.untilOffset + \"\") &#125;) &#125;&#125; 3.KafkaOffsetsTool Object 当Kafka的数据丢失时，消费数据的偏移量可能会大于Kafka数据的最大偏移量，所以我们需要定义一个KafkaOffsetsTool Object将Kafka数据的最大偏移量取出来，与我们存储在Redis的偏移量作比较，具体处理方式见RedisOffsetsManager Object。 123456789101112131415161718192021222324252627282930313233343536373839object KafkaOffsetsTool &#123; def getLastOffset(topic: String): mutable.Map[TopicPartition, lang.Long] = &#123; var partitionToLongMap: mutable.Map[TopicPartition, lang.Long] = null val props = new Properties props.put(\"bootstrap.servers\", \"hadoop000:9092,hadoop000:9093,hadoop000:9094\") props.put(\"group.id\", \"asd\") props.put(\"enable.auto.commit\", \"false\") props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\") props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\") var kafkaConsumer: KafkaConsumer[String, String] = null var topicPartitions: ListBuffer[TopicPartition] = null try &#123; //1、初始化 kafkaConsumer = new KafkaConsumer[String, String](props) topicPartitions = ListBuffer[TopicPartition]() //2、获取partition，构建 topicPartition val partitionInfos: util.List[PartitionInfo] = kafkaConsumer.partitionsFor(topic) import scala.collection.JavaConversions._ partitionInfos.foreach(partitionInfo =&gt; &#123; topicPartitions.append(new TopicPartition(topic, partitionInfo.partition())) &#125;) //3、根据topicPartition获取endoffset val partitionToLongJavaMap: util.Map[TopicPartition, lang.Long] = kafkaConsumer.endOffsets(topicPartitions) partitionToLongMap = JavaConverters.mapAsScalaMap(partitionToLongJavaMap) &#125; catch &#123; case e: Exception =&gt; e.printStackTrace() &#125; finally &#123; if (kafkaConsumer != null) kafkaConsumer.close() &#125; partitionToLongMap &#125;&#125; 4.最终实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * SS消费Kafka数据，使用Redis维护offset * offset存储格式 topic_groupid partition offset * 1) 添加offset矫正,与kafka offset对比 * 2) 使用pipeline保证业务数据和offset的提交的一致性 */object StreamingKafkaApp05 &#123; def main(args: Array[String]): Unit = &#123; //1、获取StreamingContext、以及设置KafkaParams和topics val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val ssc = new StreamingContext(sparkConf, Seconds(5)) val groupId = \"asd\" val wcRedisKeyName = \"wc_redis\" val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"hadoop000:9092,hadoop000:9093,hadoop000:9094\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"asd\", \"auto.offset.reset\" -&gt; \"earliest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"ruozedata3partstopic\") //2、获取offset val fromOffset: collection.Map[TopicPartition, Long] = RedisOffsetsManager.obtainOffsets(topics(0), groupId) //3、业务处理、offset提交 val stream = KafkaUtils.createDirectStream( ssc, LocationStrategies.PreferConsistent, Subscribe[String, String](topics, kafkaParams, fromOffset)) stream.foreachRDD(rdd =&gt; &#123; println(\"接收数据条数: \" + rdd.count()) if (!rdd.isEmpty()) &#123; val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges //获取最新的offset信息 val result = rdd.map(x =&gt; (x.value(), 1)).reduceByKey(_ + _).collect() //业务逻辑处理：词频统计 var jedis: Jedis = null var pipeline: Pipeline = null try &#123; //4、开启jedis pipelined jedis = RedisUtils.getJedis pipeline = jedis.pipelined() pipeline.multi() //JedisDataException: DISCARD without MULTI //if isReset 把旧数据放到tmp中 if (RedisOffsetsManager.isReset) &#123; pipeline.rename(wcRedisKeyName, wcRedisKeyName + \"_tmp\") RedisOffsetsManager.isReset = false &#125; //4.1 保存计算结果 result.map(pair =&gt; &#123; pipeline.hincrBy(wcRedisKeyName, pair._1, pair._2) &#125;) //4.2 保存offset RedisOffsetsManager.storeOffsets(pipeline, offsetRanges, groupId) //4.3 pipe提交 pipeline.exec() pipeline.sync() &#125; catch &#123; case e: Exception =&gt; pipeline.discard() //不成功，清除 e.printStackTrace() &#125; finally &#123; pipeline.close() jedis.close() &#125; &#125; else &#123; println(\"当前批次无数据\") &#125; &#125;) //5、开始作业 ssc.start() ssc.awaitTermination() &#125;&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"},{"name":"Kafka","slug":"Kafka","permalink":"https://zzuuriel.github.io/tags/Kafka/"}]},{"title":" Spark Streaming中foreachRDD的使用 ","date":"2019-08-06T12:15:39.000Z","path":"大数据技术/9.Spark Streaming中foreachRDD的使用/","text":"Spark Streaming中foreachRDD的使用 概述 在Spark 官网中，foreachRDD被划分到Output Operations on DStreams中，所以我们首先要明确的是，它是一个输出操作的算子，然后再来看官网对它的含义解释： 最常用的输出操作 需要一个函数作为参数，函数作用于DStream中的每一个RDD 函数将RDD中的数据输出到外部系统，如文件、数据库 函数在driver上执行 函数中通常要有action算子，因为foreachRDD本身是transform算子 测试 需求：用Spark Streaming把WC结果写到MySQL MySQLUtils 12345678910111213object MySQLUtils &#123; def getConnection() = &#123; Class.forName(\"com.mysql.jdbc.Driver\") DriverManager.getConnection(\"jdbc:mysql://hadoop000:3306/test\", \"root\", \"123456\") &#125; def closeConnection(connection: Connection): Unit =&#123; if(null != connection) &#123; connection.close() &#125; &#125;&#125; Version1.0 123456789101112 val result = stream.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _) result.foreachRDD(rdd =&gt; &#123; val connection = MySQLUtils.getConnection() //executed at the driver rdd.foreach(pair =&gt; &#123; val sql = \"insert into wc(word,cnt) values(?,?)\" val statement = connection.prepareStatement(sql) //executed at the worker statement.setString(1, pair._1) statement.setInt(2, pair._2) statement.execute() &#125;) MySQLUtils.closeConnection(connection)&#125;) 执行后会报错，Task not serializable 我们使用foreachRDD向外部系统输出数据时，通常要创建一个连接对象，如果像上面的代码中创建在driver上，而foreach这个算子是在executor端执行，connection不是序列化的。通常会报序列化错误或者初始化错误。其实这是一个闭包问题,在函数内部引用了一个外部的变量。 Version2.0 为了解决闭包问题，我们把connection放到foreach中。 123456789val result = stream.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _)result .foreachRDD(rdd =&gt; &#123; rdd.foreach(pair =&gt; &#123; val connection = MySQLUtils.getConnection() //executed at the driver val sql = s\"insert into wc(word,cnt) values('$&#123;pair._1&#125;', $&#123;pair._2&#125;)\" connection.createStatement().execute(sql) MySQLUtils.closeConnection(connection) &#125;)&#125;) 这样虽然不会报错，但是foreach中的每一个元素都会创建连接对象，浪费资源，foreach适用于对每一个元素进行操作的场景。 Version3.0 要创建连接对象时一般使用foreachPartition来解决这个问题，这样每个partition中只创建一个连接对象，使用它来对该partition内的每个元素进行输出。 12345678910val result = stream.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _)result.foreachRDD(rdd =&gt; &#123; rdd.foreachPartition(partition =&gt; &#123; val connection = MySQLUtils.getConnection() //每个分区创建一个partition partition.foreach(pair =&gt; &#123; val sql = s\"insert into wc(word,cnt) values('$&#123;pair._1&#125;',$&#123;pair._2&#125;)\" connection.createStatement().execute(sql) &#125;) MySQLUtils.closeConnection(connection)&#125;) Version4.0 更进一步的话，在处理一批RDD时，可以使用数据库连接池来重复使用连接对象，注意连接池必须是静态、懒加载的。 可以借助Scalikejdbc来实现：http://scalikejdbc.org/ 1）加入依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc_$&#123;scala.tools.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;scalikejdbc.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc-config_$&#123;scala.tools.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;scalikejdbc.version&#125;&lt;/version&gt;&lt;/dependency&gt; 2）创建数据库连接配置信息 resources下创建application.conf 12345678910# JDBC Settingsdb.default.driver = \"com.mysql.jdbc.Driver\"db.default.url = \"jdbc:mysql://localhost:3306/demo?useSSL=false&amp;serverTimezone=UTC\"db.default.user = \"root\"db.default.password = \"123456\" # Connection Pool settingsdb.default.poolInitialSize = 10db.default.poolMaxSize = 20db.default.connectionTimeoutMillis = 1000 3）使用 1234567891011121314151617val result = stream.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _)DBs.setupAll() //解析配置文件application.confresult.foreachRDD(rdd =&gt; &#123; rdd.foreachPartition(partition =&gt; &#123; partition.foreach(pair =&gt; &#123; DB.autoCommit &#123;implicit session =&gt; &#123; // 默认就使用了连接池 SQL(\"insert into wc(word,cnt) values(?,?)\") .bind(pair._1, pair._2) .update() .apply() &#125; &#125; &#125;) &#125;)&#125;)","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Streaming transform的使用&闭包&Spark Streaming对接Kafka","date":"2019-08-03T12:17:36.000Z","path":"大数据技术/Spark Streaming transform的使用&闭包&Spark Streaming对接Kafka/","text":"Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka Spark Streaming transform的使用 官网：http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams transform 操作(及其变体如 transformWith)允许在 DStream 上应用任意的 RDD-to-RDD 函数。 它可以用于应用未在 DStream API 中公开的任何 RDD操作。 例如，DStream API没有将数据流中的每个批处理与另一个数据集联接的功能。 然而，你可以很容易地使用transform 来做到这一点，并提供了非常强大的可能性。 例如，可以通过将输入数据流与预先计算的垃圾信息(也可能是由 spark 生成的)联接起来，然后根据这些信息进行过滤，从而实现实时数据清理。 示例：黑名单过滤 123456789101112131415161718192021222324252627def main(args: Array[String]): Unit = &#123; val checkpoint = \"./transform-checkpoint\" def functionToCreateContext(): StreamingContext = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.checkpoint(checkpoint) //黑名单用户：xingxing，转换成RDD val blacks = List(\"xingxing\") val blacksRDD = ssc.sparkContext.parallelize(blacks).map((_,true)) // 从socket拿到流式数据：xingxing,60 val stream = ssc.socketTextStream(\"hadoop000\", 9999) stream.map(x =&gt; (x.split(\",\")(0), x)) // (xingxing, (xingxing,60)) .transform(rdd =&gt; &#123; rdd.leftOuterJoin(blacksRDD) //(xingxing, (xingxing,60,true)) .filter(x =&gt; &#123; x._2._2.getOrElse(false) != true &#125;).map(_._2._1) &#125;).print() ssc &#125; val ssc = StreamingContext.getOrCreate(checkpoint, functionToCreateContext _) ssc.start() // Start the computation ssc.awaitTermination() // Wait for the computation to terminate&#125; 闭包 官网：http://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures- 1.Understanding closures Spark 的难点之一是理解跨集群执行代码时变量和方法的作用域和生命周期。 在作用域外修改变量的 rdd 操作可能经常引起混淆。 在下面的示例中，我们将查看使用 foreach ()增加计数器的代码，但其他操作也可能出现类似的问题。 2.Example 考虑一下下面那个天真的 rdd 元素 sum，它的行为可能会因为是否在同一个 jvm 中执行而有所不同。 一个常见的例子是在本地模式下运行 spark (-- master local [ n ])和在集群中部署 spark 应用程序(例如通过 spark-submit to YARN) : 1234567var counter = 0var rdd = sc.parallelize(data)// Wrong: Don't do this!!rdd.foreach(x =&gt; counter += x)println(\"Counter value: \" + counter) 3.Local vs. cluster modes 上述代码的行为是未定义的，可能无法按预期工作。为了执行作业，Spark将RDD操作的处理分解为tasks，每个任务由executor执行。在执行之前，Spark计算task的闭包。闭包是那些executor在RDD上执行其计算时必须可见的变量和方法(在本例中为foreach())。这个闭包被序列化并发送给每个executor 。 闭包中发送给每个executor 的变量现在都是副本，因此，当在foreach函数中引用counter时，它不再是driver 上的计数器。在executors的内存中仍然有一个计数器，但它对executor不再可见!executor只看到来自序列化闭包的副本。因此，counter的最终值仍然是零，因为counter上的所有操作都引用了序列化闭包中的值。 在本地模式下，在某些情况下，foreach 函数实际上将在与驱动程序相同的 jvm 中执行，并引用相同的原始计数器，并可能实际更新它。 为了确保在这些场景中定义良好的行为，应该使用累加器。 Spark 中的累加器专门用于提供一种机制，当执行在集群中的工作节点之间分离时，可以安全地更新变量。 一般来说，像循环或局部定义方法这样的闭包结构不应该用来改变全局状态。Spark不保证闭包外部引用的对象的突变行为。一些这样做的代码可能在本地模式下工作，但那只是偶然的，而且这样的代码在分布式模式下不会像预期的那样工作。如果需要全局聚合，则使用Accumulator。 Spark Streaming对接Kafka 官网：http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#spark-streaming-kafka-integration-guide-kafka-broker-version-010 Kafka 0.10的Spark Streaming集成在设计上与0.8 Direct Stream方法类似。 它提供简单的并行性，Kafka分区和Spark分区之间的1：1对应关系，以及对偏移和元数据的访问。 但是，由于较新的集成使用新的Kafka消费者API而不是简单的API，因此使用方法存在显着差异。 这个版本的集成被标记为实验性的，因此 api 可能会发生变化。 1.Linking 对于使用 SBT/Maven 项目定义的 Scala/Java 应用程序，将您的流式应用程序链接到以下工件。 123groupId = org.apache.sparkartifactId = spark-streaming-kafka-0-10_2.12version = 2.4.5 不要手动添加对 org.apache.kafka 工件的依赖(例如 kafka-clients)。 kafka-0-10工件已经具有适当的传递依赖性，不同的版本可能难以诊断的方式是不兼容的。 2.Creating a Direct Stream 1234567891011121314151617181920212223import org.apache.kafka.clients.consumer.ConsumerRecordimport org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.streaming.kafka010._import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeval kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"localhost:9092,anotherhost:9092\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"use_a_separate_group_id_for_each_stream\", \"auto.offset.reset\" -&gt; \"latest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean))val topics = Array(\"topicA\", \"topicB\")val stream = KafkaUtils.createDirectStream[String, String]( streamingContext, PreferConsistent, Subscribe[String, String](topics, kafkaParams))stream.map(record =&gt; (record.key, record.value)) 3.示例 1）Kafka Product API 1234567891011121314151617181920212223 def main(args: Array[String]): Unit = &#123; // 设置配置文件 val props = new Properties() props.put(\"bootstrap.servers\", \"hadoop000:9092,hadoop000:9093,hadoop000:9094\"); props.put(\"acks\", \"all\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); // 创建producer val producer = new KafkaProducer[String, String](props) // 循环发送数据 for(i&lt;-1 to 100) &#123; Thread.sleep(100) val part = i%3 // 分区 val word = String.valueOf((new Random().nextInt(6) + 'a').toChar) logger.error(word) val record = new ProducerRecord[String,String](\"ruozedata3partstopic\", part, \"\", word) producer.send(record) &#125; // 关闭producer producer.close(); &#125; 2）Spark Streaming Consumer 123456789101112131415161718192021222324252627282930313233343536373839404142def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val ssc = new StreamingContext(sparkConf, Seconds(5)) val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"hadoop000:9092,hadoop000:9093,hadoop000:9094\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"use_a_separate_group_id_for_each_stream\", \"auto.offset.reset\" -&gt; \"earliest\", \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) // 可以设置多个topic val topics = Array(\"ruozedata3partstopic\") // 创建DirectStream val stream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String]( ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) // 业务逻辑 val result = stream.map(record =&gt; (record.key, record.value)) .map(_._2) .flatMap(_.split(\",\")) .map((_, 1)) .reduceByKey(_ + _) .foreachRDD(rdd =&gt; &#123; rdd.foreachPartition(partition =&gt; &#123; val connection = MySQLUtils.getConnection() partition.foreach(pair =&gt; &#123; val sql = s\"insert into wc(word,cnt) values('$&#123;pair._1&#125;', $&#123;pair._2&#125;)\" connection.createStatement().execute(sql) &#125;) MySQLUtils.closeConnection(connection) &#125;) &#125;) // 启动程序 ssc.start() ssc.awaitTermination() &#125; 3）MySQLUtils API 1234567891011121314object MySQLUtils &#123; def getConnection() = &#123; Class.forName(\"com.mysql.jdbc.Driver\") DriverManager.getConnection(\"jdbc:mysql://hadoop000:3306/test\", \"root\", \"123456\") &#125; def closeConnection(connection: Connection): Unit =&#123; if(null != connection) &#123; connection.close() &#125; &#125;&#125; 这样，从Kafka Product API制造的数据就可以写进Kafka，通过Spark Streaming Consumer消费，最终写到Mysql中。 4）拿到Kafka的Offset 12345678910111213141516171819202122232425262728293031323334def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val ssc = new StreamingContext(conf, Seconds(5)) val kafkaParams = Map[String, Object]( \"bootstrap.servers\" -&gt; \"hadoop000:9092,hadoop000:9093,hadoop000:9094\", \"key.deserializer\" -&gt; classOf[StringDeserializer], \"value.deserializer\" -&gt; classOf[StringDeserializer], \"group.id\" -&gt; \"use_a_separate_group_id_for_each_stream\", \"auto.offset.reset\" -&gt; \"earliest\", //latest \"enable.auto.commit\" -&gt; (false: java.lang.Boolean) ) val topics = Array(\"ruozedata3partstopic\") // stream不能做任何操作，否则得到的不是一个KafkaRDD val stream = KafkaUtils.createDirectStream[String, String]( ssc, PreferConsistent, Subscribe[String, String](topics, kafkaParams) ) // 必须先拿到HasOffsetRanges，才能开始业务逻辑 stream.foreachRDD &#123; rdd =&gt; // 通过rdd.asInstanceOf[HasOffsetRanges]拿到KafkaRDD，它保存了每个分区的offset val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges // KafkaRDD维护了topic、partition、fromOffset、untilOffset offsetRanges.foreach &#123; o =&gt; println(s\"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;\") &#125; &#125; // 启动程序 ssc.start() ssc.awaitTermination()&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Streaming基础","date":"2019-08-01T12:17:36.000Z","path":"大数据技术/Spark Streaming基础/","text":"Spark Streaming简介&amp;Spark Streaming的内部结构&amp;StreamingContext对象&amp;离散流（DStream）&amp;Spark Streaming开发示例&amp;state维护 Spark Streaming简介 Spark Streaming是核心Spark API的扩展，可实现可扩展、高吞吐量、可容错的实时数据流处理。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等众多来源获取，并且可以使用由高级函数（如map，reduce，join和window）开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。而且还可以在数据流上应用Spark提供的机器学习和图处理算法。 Spark Streaming的内部结构 在内部，它的工作原理如下。Spark Streaming接收实时输入数据流，并将数据切分成批，然后由Spark引擎对其进行处理，最后生成“批”形式的结果流。 Spark Streaming将连续的数据流抽象为discretizedstream(DStream)。在内部，DStream由一个RDD序列表示。 StreamingContext对象 初始化StreamingContext： 方式一，从SparkConf对象中创建： 1234//创建一个Context对象：StreamingContextval conf = new SparkConf().setAppName(\"MyNetworkWordCount\").setMaster(\"local[2]\")//指定批处理的时间间隔val ssc = new StreamingContext(conf, Seconds(5)) 方式二，从现有的SparkContext实例中创建 1val ssc = new StreamingContext(sc, Seconds(1)) 说明： appName参数是应用程序在集群UI上显示的名称。 master是Spark，Mesos或YARN集群的URL，或者一个特殊的“local [*]”字符串来让程序以本地模式运行。 当在集群上运行程序时，不需要在程序中硬编码master参数，而是使用spark-submit提交应用程序并将master的URL以脚本参数的形式传入。但是，对于本地测试和单元测试，您可以通过“local[*]”来运行Spark Streaming程序（请确保本地系统中的cpu核心数够用）。 StreamingContext会内在的创建一个SparkContext的实例（所有Spark功能的起始点），你可以通过ssc.sparkContext访问到这个实例。 批处理的时间窗口长度必须根据应用程序的延迟要求和可用的集群资源进行设置。 注意： 一旦一个StreamingContext开始运作，就不能设置或添加新的流计算。 一旦一个上下文被停止，它将无法重新启动。 同一时刻，一个JVM中只能有一个StreamingContext处于活动状态。 StreamingContext上的stop()方法也会停止SparkContext。 要仅停止StreamingContext（保持SparkContext活跃），请将stop() 方法的可选参数stopSparkContext设置为false。 只要前一个StreamingContext在下一个StreamingContext被创建之前停止（不停止SparkContext），SparkContext就可以被重用来创建多个StreamingContext。 离散流（DStream） DiscretizedStream(DStream) 是Spark Streaming对流式数据的基本抽象。它表示连续的数据流，这些连续的数据流可以是从数据源接收的输入数据流，也可以是通过对输入数据流执行转换操作而生成的经处理的数据流。在内部，DStream由一系列连续的RDD表示，如下图： 我们将一行行文本组成的流转换为单词流，具体做法为：将flatMap操作应用于名为lines的 DStream中的每个RDD上，以生成words DStream的RDD。如下图所示： 但是DStream和RDD也有区别，下面画图说明： RDD的结构： DStream的结构： Spark Streaming开发示例 1.要编写自己的Spark流程序，必须将以下依赖项添加到Maven项目中。 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 2.SocketFile实现简单的词频统计 12345678910111213141516171819202122232425def main(args: Array[String]): Unit = &#123; // 拿到StreamingContext对象 val conf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val ssc = new StreamingContext(conf, Seconds(5)) dispose(ssc) //开启StreamingContext ssc.start() ssc.awaitTermination()&#125;private def dispose(ssc: StreamingContext) = &#123; //输入记录 val lines = ssc.socketTextStream(\"hadoop000\", 9998) //逻辑处理 val words = text.flatMap(_.split(\" \")) val pair = words.map(x =&gt; (x, 1)) val result = pair.reduceByKey(_ + _) //输出记录 result.print()&#125; 3.使用nc发送消息 123$ nc -lk 91001,1,1,2,2 4.客户端接收消息 1234567...-------------------------------------------Time: 1357008430000 ms-------------------------------------------(1,3)(2,2)... DStreams 是表示从源端接收的输入数据的数据流。在这个简单的示例中，行是一个输入DStream，因为它表示从netcat服务器接收到的数据流。每个输入DStream(本节后面讨论的文件流除外)都与接收方(Scala doc、Java doc)对象相关联，接收方接收来自源的数据并将其存储在Spark内存中进行处理。 注意：Spark流应用程序需要分配足够的Core来处理接收到的数据，以及运行接收方。设置core的数量要大于Receivers的数量。 Checkpoint维护state updateStateByKey维护State 1.一些基本概念 什么是updateStateByKey? updateStateByKey(func)可以返回一个新“state”的DStream，其中通过对键的前一个状态和键的新值应用给定的函数来更新每个键的状态。这可以用来维护每个键的任意状态数据。 什么是Checkpoint? Checkpoint可以通过在一个容错的、可靠的文件系统中设置一个目录来启用，Checkpoint信息将被保存到这个目录中。这是通过使用streamingContext.checkpoint(checkpointDirectory)实现的。 2.下面案例也是词频统计，只不过带了state信息 12345678910111213141516171819202122232425262728def main(args: Array[String]): Unit = &#123; val conf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val ssc = new StreamingContext(conf, Seconds(5)) dispose(ssc) ssc.start() ssc.awaitTermination()&#125;// 处理逻辑private def dispose(ssc: StreamingContext) = &#123; val lines = ssc.socketTextStream(\"hadoop000\", 9998) // 设置checkpoint目录，保存offset ssc.checkpoint(\"./chk\") // updateStateByKey：维护记录的state lines.flatMap(_.split(\",\")).map((_, 1)).updateStateByKey(updateFunction) .print()&#125;// 实现对新值和旧值的累加def updateFunction(newValues: Seq[Int], oldValues: Option[Int]): Option[Int] = &#123; val curr = newValues.sum val old = oldValues.getOrElse(0) val count = curr + old Some(count)&#125; 3.使用nc发送消息 1234$ nc -lk 9998a a a b b ca a a b b c 客户端接收消息 12345678910111213-------------------------------------------Time: 1587439170000 ms-------------------------------------------(b,2)(a,3)(c,1)-------------------------------------------Time: 1587439175000 ms-------------------------------------------(b,4)(a,6)(c,2) 上面的程序如果一直运行，结果可以一直累加，但是程序一旦停止运行，重新启动时，结果就不会接着上一次进行计算，主要原因上每次程序运行都会初始化一个程序入口，而两次运行的程序入口不是同一个入口，所以会导致第一次计算的结果丢失。 使用checkpoint方法可以把上一次Driver里面的运算结果状态保存在checkpoint的目录里面，我们在第二次启动程序时，就可以从checkpoint里面取出上一次的运行结果状态，把这次的Driver状态恢复成和上一次Driver一样的状态。 updateStateByKey维护State（优化版本） 以下代码参考官网：http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing 如果想让应用程序从驱动程序故障中恢复，我们应该重写代码，让它具备下面的功能 当程序第一次启动时，它将创建一个新的StreamingContext，设置所有的流，然后调用start()。 当程序在失败后重新启动时，它将从Checkpoint目录中的Checkpoint数据重新创建一个StreamingContext。 1234567891011121314151617181920212223242526272829303132333435363738val checkpoint = \"./chk_v2\"def main(args: Array[String]): Unit = &#123; // 拿到StreamingContext val ssc = StreamingContext.getOrCreate(checkpoint, functionToCreateContext) ssc.start() ssc.awaitTermination()&#125;// 创建StreamingContext带业务逻辑def functionToCreateContext(): StreamingContext = &#123; val conf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val ssc = new StreamingContext(conf,Seconds(5)) // new context dispose(ssc) ssc.checkpoint(checkpoint) // set checkpoint directory ssc&#125;// 业务逻辑处理private def dispose(ssc: StreamingContext) = &#123; val lines = ssc.socketTextStream(\"hadoop000\", 9998) // create DStreams lines .flatMap(_.split(\",\")) .map((_, 1)) .updateStateByKey(updateFunction) .print()&#125;// 更新statedef updateFunction(newValues: Seq[Int], oldValues: Option[Int]): Option[Int] = &#123; val curr = newValues.sum val old = oldValues.getOrElse(0) val count = curr + old Some(count)&#125; mapWithState维护State的方法 官网中描述用updateStateByKey维护State是一种过时的方法，但并没有举例说明最新的方法，这种新方法就是mapWithState，我们可以在源码中找到它的使用方法。 123456789101112131415161718192021222324252627282930313233343536373839// 指定checkpoint路径val checkpoint = \"./chk_v3\"def main(args: Array[String]): Unit = &#123; // 拿到 StreamingContext val ssc = StreamingContext.getOrCreate(checkpoint, functionToCreateContext) ssc.start() ssc.awaitTermination()&#125; // 创建 StreamingContextdef functionToCreateContext(): StreamingContext = &#123; val conf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val ssc = new StreamingContext(conf, Seconds(5)) ssc.checkpoint(checkpoint) // 对记录做累加操作 val mappingFunc = (word: String, one: Option[Int], state: State[Int]) =&gt; &#123; if(state.isTimingOut())&#123; println(\"超时3秒没拿到数据\") &#125;else&#123; val sum = one.getOrElse(0) + state.getOption.getOrElse(0) val output = (word, sum) state.update(sum) output &#125; &#125; // 业务逻辑处理 val lines = ssc.socketTextStream(\"hadoop000\", 9998) lines .flatMap(_.split(\" \")) .map((_,1)) .mapWithState(StateSpec.function(mappingFunc) .timeout(Seconds(3)) ).print() ssc&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark SQL-自定义外部text数据源","date":"2019-07-27T12:17:36.000Z","path":"大数据技术/Spark SQL自定义外部text数据源/","text":"Spark SQL-自定义外部text数据源 需求 准备一份数据 123451,pk,1,15000,30002,xinxing,1,10000,10003,J哥,1,12000,20004,xx,3,10000,20005,lisa,2,11000,2000 第3列为性别，自定义外部text数据源，将性别按1==&gt;男，2==&gt;女，3==&gt;未知解析出来 实现 1.创建DefaultSource类继承RelationProvider Trait，类名必须是DefaultSource，源码写死了。 1234567891011class DefaultSource extends RelationProvider&#123; override def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation = &#123; //拿到client传入的参数path val path = parameters.get(\"path\") //判断path是否存在 path match &#123; case Some(p) =&gt; new RuozedataTextDataSourceRelation(sqlContext, p) case _ =&gt; throw new IllegalArgumentException(\"path is required...\") &#125; &#125;&#125; 2.自定义Relation，继承BashRelation和TableScan，拿到Schema和RDD[Row] 123456789101112131415161718192021222324252627282930313233343536373839404142434445class TextDataSourceRelation(val sqlContext: SQLContext, val path: String) extends BaseRelation with TableScan with Logging &#123; //重写StructType接口实现Schema override def schema: StructType = StructType( StructField(\"id\", LongType, false) :: StructField(\"name\", StringType, false) :: StructField(\"sex\", StringType, false) :: StructField(\"sal\", DoubleType, false) :: StructField(\"comm\", DoubleType, false) :: Nil ) //重写buildScan拿到RDD[Row] override def buildScan(): RDD[Row] = &#123; logError(\"自定义数据源实现:buildScan\") //拿到文本数据 val info = sqlContext.sparkContext.textFile(path) //拿到每个StructField val schemaFields: Array[StructField] = schema.fields //对每行数据逗号切分，并且去掉空格，返回集合 info.map(_.split(\",\").map(_.trim)) //对集合中的每个元素操作，通过zipWithIndex算子可以拿到元素的内容和对应的索引号 .map(x =&gt; x.zipWithIndex.map&#123; case (value, index) =&gt; &#123; //通过schemaField和index拿到列名 val columnName = schemaFields(index).name //判断当前的列名是否是sex，并在castTo工具类中做匹配，对value转换类型 Utils.castTo(if(columnName.equalsIgnoreCase(\"sex\"))&#123; //如果列名是sex，列下元素是1、2或者3，则返回对应的字符 if(value == \"1\") &#123; \"男\" &#125; else if(value == \"2\") &#123; \"女\" &#125; else &#123; \"未知\" &#125; &#125; else &#123; value //如果列名不是sex，则直接返回元素 &#125;, schemaFields(index).dataType) //传入dataType的类型，在工具类中做匹配 &#125; &#125;).map(x =&gt; Row.fromSeq(x)) //转换成RDD[Row] &#125;&#125; 3.自定义Utils类 1234567891011object Utils &#123; def caseTo(value:String,dataType: DataType) =&#123; //模式匹配，转换value的类型 dataType match &#123; case _:DoubleType =&gt; value.toDouble case _:LongType =&gt; value.toLong case _:StringType =&gt; value &#125; &#125;&#125; 4.测试 123456789101112131415161718object TestApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName) .getOrCreate() //方法1： spark.read.format(\"extds\") .load(\"ruozedata-spark-sql/data/extds.txt\") .show() //方法2： spark.read.format(\"extds\") .option(\"path\",\"ruozedata-spark-sql/data/extds.txt\") .load().show spark.stop() &#125;&#125; 运行结果如下： 123456789+---+-------+----+-------+------+| id| name| sex| sal| comm|+---+-------+----+-------+------+| 1| pk| 男|15000.0|3000.0|| 2|xinxing| 男|10000.0|1000.0|| 3| J哥| 男|12000.0|2000.0|| 4| xx|未知|10000.0|2000.0|| 5| lisa| 女|11000.0|2000.0|+---+-------+----+-------+------+","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark SQL-从jdbc的角度解读外部数据源","date":"2019-07-26T12:17:36.000Z","path":"大数据技术/Spark SQL从jdbc的角度解读外部数据源/","text":"Spark SQL-从jdbc的角度解读外部数据源 接口 首先了解一些trait，分别是BaseRelation、TableScan/PrunedScan/PrunedFilteredScan、RelationProvider，下面是他们的功能在源码中的解读 123456789101112131415161718192021222324252627282930313233343536373839404142//代表了一个抽象的数据源。该数据源由一行行有着已知schema的数据组成（关系表）。abstract class BaseRelation &#123; def sqlContext: SQLContext def schema: StructType //schema * def sizeInBytes: Long = sqlContext.conf.defaultSizeInBytes def needConversion: Boolean = true def unhandledFilters(filters: Array[Filter]): Array[Filter] = filters&#125;//用于扫描整张表，将数据返回成RDD[Row]。@InterfaceStability.Stabletrait TableScan &#123; def buildScan(): RDD[Row]&#125;//用于裁剪整张表，将数据返回成RDD[Row]。@InterfaceStability.Stabletrait PrunedScan &#123; def buildScan(requiredColumns: Array[String]): RDD[Row]&#125;//用于裁剪并过滤整张表，将数据返回成RDD[Row]。@InterfaceStability.Stabletrait PrunedFilteredScan &#123; def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row]&#125;//插入数据的时候实现，设置overwrite是否为true@InterfaceStability.Stabletrait InsertableRelation &#123; def insert(data: DataFrame, overwrite: Boolean): Unit&#125;//为自定义的数据源类型生成一个新的Relation对象trait RelationProvider &#123;//创建一个新的Relation def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation&#125; jdbc实现 JdbcRelationProvider (拿到schema和RDD[Row]) 12345678910111213141516class JdbcRelationProvider extends CreatableRelationProviderwith RelationProvider with DataSourceRegister &#123; override def shortName(): String = \"jdbc\" //简称 override def createRelation( sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation = &#123; //所有options参数以map形式传入 val jdbcOptions = new JDBCOptions(parameters) //把参数传入和系统参数匹配 val resolver = sqlContext.conf.resolver //忽略大小写 val timeZoneId = sqlContext.conf.sessionLocalTimeZone //拿到时区 val schema = JDBCRelation.getSchema(resolver, jdbcOptions) //传入参数，拿到schema val parts = JDBCRelation.columnPartition(schema, resolver, timeZoneId, jdbcOptions) //拿到分区 JDBCRelation(schema, parts, jdbcOptions)(sqlContext.sparkSession) //拿到RDD[R] &#125;&#125; 第一步: 拿到Schema 1.在val schema = JDBCRelation.getSchema(resolver, jdbcOptions)处打断点Debug 12345678def getSchema(resolver: Resolver, jdbcOptions: JDBCOptions): StructType = &#123; val tableSchema = JDBCRDD.resolveTable(jdbcOptions) //传入参数，解析table，拿到Schame jdbcOptions.customSchema match &#123; //模式匹配 case Some(customSchema) =&gt; JdbcUtils.getCustomSchema( tableSchema, customSchema, resolver) // 返回定制的Schema case None =&gt; tableSchema //返回直接的Schema &#125;&#125; 2.val tableSchema = JDBCRDD.resolveTable(jdbcOptions) 12345678910111213141516171819202122def resolveTable(options: JDBCOptions): StructType = &#123; //传入参数，拿到Schame val url = options.url //拿到url：jdbc:mysql://hadoop:3306/ val table = options.tableOrQuery //拿到table：access_dw.dws_ad_phone_type_dist val dialect = JdbcDialects.get(url) //拿到方言：MySQLDialect val conn: Connection = JdbcUtils.createConnectionFactory(options)() //创建连接 try &#123; val statement = conn.prepareStatement(dialect.getSchemaQuery(table)) //拿到sql：com.mysql.jdbc.JDBC42PreparedStatement@5bda157e: SELECT * FROM access_dw.dws_ad_phone_type_dist WHERE 1=0 try &#123; statement.setQueryTimeout(options.queryTimeout) //设置超时时间 val rs = statement.executeQuery() //执行查询，返回一个查询产生的数据的ResultSet对象 try &#123; JdbcUtils.getSchema(rs, dialect, alwaysNullable = true) //传入数据rs，拿到schema，接着下面的内容 &#125; finally &#123; rs.close() &#125; &#125; finally &#123; statement.close() &#125; &#125; finally &#123; conn.close() &#125;&#125; 3.JdbcUtils.getSchema(rs, dialect, alwaysNullable = true) 1234567891011121314151617181920212223242526272829303132333435363738def getSchema( resultSet: ResultSet, //查询表返回的rs(表结构) dialect: JdbcDialect, //MySQL方言 alwaysNullable: Boolean = false): StructType = &#123; val rsmd = resultSet.getMetaData //拿到表的元数据 val ncols = rsmd.getColumnCount //拿到需要的字段的列的数量 val fields = new Array[StructField](ncols) //创建一个StructField类型的数组，拼接fields var i = 0 while (i &lt; ncols) &#123; //循环出每个column val columnName = rsmd.getColumnLabel(i + 1) //返回列名：phoneSystemType val dataType = rsmd.getColumnType(i + 1) //返回数据类型：12 val typeName = rsmd.getColumnTypeName(i + 1) //返回数据类型的名称：VARCHAR val fieldSize = rsmd.getPrecision(i + 1) //返回字段大小：64 val fieldScale = rsmd.getScale(i + 1) //返回scale：0 val isSigned = &#123; //判断是否有符号 try &#123; rsmd.isSigned(i + 1) //是否有符号：false &#125; catch &#123; // Workaround for HIVE-14684: case e: SQLException if e.getMessage == \"Method not supported\" &amp;&amp; rsmd.getClass.getName == \"org.apache.hive.jdbc.HiveResultSetMetaData\" =&gt; true &#125; &#125; val nullable = if (alwaysNullable) &#123; //判断是否可为空 true &#125; else &#123; rsmd.isNullable(i + 1) != ResultSetMetaData.columnNoNulls &#125; val metadata = new MetadataBuilder().putLong(\"scale\", fieldScale) val columnType = dialect.getCatalystType(dataType, typeName, fieldSize, metadata).getOrElse( getCatalystType(dataType, fieldSize, fieldScale, isSigned)) // 传入参数拿到类型：StringType fields(i) = StructField(columnName, columnType, nullable) //传入列名，数据类型，是否可为空，创建StructField，并加入到fields中 i = i + 1 &#125; new StructType(fields) //传入所有的StructField构建StructType，并返回，到这里拿到最终的Schema&#125; 第二步: 拿RDD[Row] 1.JDBCRelation(schema, parts, jdbcOptions)(sqlContext.sparkSession) 123456789101112131415161718192021222324252627282930313233343536373839404142434445private[sql] case class JDBCRelation( override val schema: StructType, //拿到Schema parts: Array[Partition], //得到分区 jdbcOptions: JDBCOptions)(@transient val sparkSession: SparkSession) extends BaseRelation //实现BaseRelation，必然拿到了Schema with PrunedFilteredScan //实现裁剪并且过滤的扫描表 with InsertableRelation &#123; //实现插入的模式 override def sqlContext: SQLContext = sparkSession.sqlContext override val needConversion: Boolean = false //检查JDBCRDD.compileFilter是否可以接受输入过滤器 override def unhandledFilters(filters: Array[Filter]): Array[Filter] = &#123; if (jdbcOptions.pushDownPredicate) &#123; filters.filter(JDBCRDD.compileFilter(_, JdbcDialects.get(jdbcOptions.url)).isEmpty) &#125; else &#123; filters &#125; &#125; // 构建Scan override def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row] = &#123; //requiredColumns：需要的列，filters：过滤条件 // 依赖类型擦除：将RDD[InternalRow]传递回RDD[Row] JDBCRDD.scanTable( sparkSession.sparkContext, //上下文环境 schema, //Schema requiredColumns, //需要的列 filters, //过滤条件 parts, //分区 jdbcOptions).asInstanceOf[RDD[Row]] //最终的结果转换成RDD[Row]类型 &#125; override def insert(data: DataFrame, overwrite: Boolean): Unit = &#123; data.write .mode(if (overwrite) SaveMode.Overwrite else SaveMode.Append) .jdbc(jdbcOptions.url, jdbcOptions.tableOrQuery, jdbcOptions.asProperties) &#125; override def toString: String = &#123; val partitioningInfo = if (parts.nonEmpty) s\" [numPartitions=$&#123;parts.length&#125;]\" else \"\" // 计划输出中不应包含凭据，表信息就足够了。 s\"JDBCRelation($&#123;jdbcOptions.tableOrQuery&#125;)\" + partitioningInfo &#125;&#125; 2.JDBCRDD.scanTable 123456789101112131415161718192021def scanTable( sc: SparkContext, schema: StructType, requiredColumns: Array[String], filters: Array[Filter], parts: Array[Partition], options: JDBCOptions): RDD[InternalRow] = &#123; val url = options.url //拿到客户端传入的rul val dialect = JdbcDialects.get(url) //拿到方言 val quotedColumns = requiredColumns.map(colName =&gt; dialect.quoteIdentifier(colName)) //拿到需要的列 new JDBCRDD( //传入参数，返回RDD[InternalRow] sc, JdbcUtils.createConnectionFactory(options), pruneSchema(schema, requiredColumns), quotedColumns, filters, parts, url, options)&#125;&#125; 3.new JDBCRDD 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110// 表示通过JDBC访问的数据库中的表的RDD。private[jdbc] class JDBCRDD( sc: SparkContext, getConnection: () =&gt; Connection, schema: StructType, columns: Array[String], filters: Array[Filter], partitions: Array[Partition], url: String, options: JDBCOptions)extends RDD[InternalRow](sc, Nil) &#123; // 索引与此RDD对应的分区列表。 override def getPartitions: Array[Partition] = partitions // `columns` 作为一个字符串注入到SQL查询 private val columnList: String = &#123; val sb = new StringBuilder() columns.foreach(x =&gt; sb.append(\",\").append(x)) if (sb.isEmpty) \"1\" else sb.substring(1) &#125; // `filters`, 作为一个where语句注入到SQL查询 private val filterWhereClause: String = filters .flatMap(JDBCRDD.compileFilter(_, JdbcDialects.get(url))) .map(p =&gt; s\"($p)\").mkString(\" AND \") // 如果当前分区有where语句，那么就拼接 private def getWhereClause(part: JDBCPartition): String = &#123; if (part.whereClause != null &amp;&amp; filterWhereClause.length &gt; 0) &#123; \"WHERE \" + s\"($filterWhereClause)\" + \" AND \" + s\"($&#123;part.whereClause&#125;)\" &#125; else if (part.whereClause != null) &#123; \"WHERE \" + part.whereClause &#125; else if (filterWhereClause.length &gt; 0) &#123; \"WHERE \" + filterWhereClause &#125; else &#123; \"\" &#125; &#125; // 对JDBC驱动程序运行SQL查询。 override def compute(thePart: Partition, context: TaskContext): Iterator[InternalRow] = &#123; var closed = false var rs: ResultSet = null var stmt: PreparedStatement = null var conn: Connection = null def close() &#123; if (closed) return try &#123; if (null != rs) &#123; rs.close() &#125; &#125; catch &#123; case e: Exception =&gt; logWarning(\"Exception closing resultset\", e) &#125; try &#123; if (null != stmt) &#123; stmt.close() &#125; &#125; catch &#123; case e: Exception =&gt; logWarning(\"Exception closing statement\", e) &#125; try &#123; if (null != conn) &#123; if (!conn.isClosed &amp;&amp; !conn.getAutoCommit) &#123; try &#123; conn.commit() &#125; catch &#123; case NonFatal(e) =&gt; logWarning(\"Exception committing transaction\", e) &#125; &#125; conn.close() &#125; logInfo(\"closed connection\") &#125; catch &#123; case e: Exception =&gt; logWarning(\"Exception closing connection\", e) &#125; closed = true &#125; context.addTaskCompletionListener[Unit]&#123; context =&gt; close() &#125; val inputMetrics = context.taskMetrics().inputMetrics val part = thePart.asInstanceOf[JDBCPartition] conn = getConnection() val dialect = JdbcDialects.get(url) import scala.collection.JavaConverters._ dialect.beforeFetch(conn, options.asProperties.asScala.toMap) // 这在通过JDBC读取表/查询之前执行一个通用的SQL语句(或PL/SQL块)。 // 使用此功能初始化数据库会话环境，例如用于优化和/或故障排除。 options.sessionInitStatement match &#123; case Some(sql) =&gt; val statement = conn.prepareStatement(sql) logInfo(s\"Executing sessionInitStatement: $sql\") try &#123; statement.setQueryTimeout(options.queryTimeout) statement.execute() //最终执行的就是jdbc &#125; finally &#123; statement.close() &#125; case None =&gt; &#125; // 返回RDD[InternalRow] CompletionIterator[InternalRow, Iterator[InternalRow]]( new InterruptibleIterator(context, rowsIterator), close()) &#125;&#125; 总结下来，大致可以分为两步： 通过jdbc查元数据，拿到Schema 通过jdbc查数据拿到RDD[Row] 最终创建DataFrame由框架解决","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"数据列自动推导&数据错误执行模式&UDAF&UDTF&解读Spark SQL执行计划","date":"2019-07-22T12:17:36.000Z","path":"大数据技术/数据列自动推导&数据错误执行模式&UDAF&UDTF&解读Spark SQL执行计划/","text":"数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划 数据列自动推导 1.准备一份数据 1234a|b|c1|2|34|uriel|67|8|9.0 2.代码测试 12345678910111213141516def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master(\"local[2]\") .appName(this.getClass.getSimpleName) .getOrCreate() val csvDF: DataFrame = spark.read .format(\"csv\") .option(\"header\",\"true\") .option(\"sep\",\"|\") .option(\"interSchema\",\"true\") .load(\"ruozedata-spark-sql/data/test.csv\") csvDF.printSchema()&#125; 3.打印数据Schema信息 1234root |-- a: integer (nullable = true) |-- b: string (nullable = true) |-- c: double (nullable = true) 数据错误执行模式 在Spark SQL中，读取数据时，遇到错误数据或者脏数据时，我们可以使用option设置mode，处理些数据的模式有3种，分别为PERMISSIVE（默认处理方式）、DROPMALFORMED（丢弃数据）、FAILFAST（快速失败），这些模式可以在ParseMode.scala源码中查看。 1.准备一份数据test.json 123&#123;\"a\":1,\"b\":2,\"c\":3&#125;&#123;\"a\":4,:5,\"c\":6&#125;&#123;\"a\":7,\"b\":8,\"c\":9&#125; 2.默认模式 1）读取数据 12val jsonDF: DataFrame = spark.read.json(\"ruozedata-spark-sql/data/test.json\")jsonDF.show() 2）运行结果： 1234567+----------------+----+----+----+| _corrupt_record| a| b| c|+----------------+----+----+----+| null| 1| 2| 3||&#123;\"a\":4,:5,\"c\":6&#125;|null|null|null|| null| 7| 8| 9|+----------------+----+----+----+ 如果没有在option中设置mode选项，默认为PERMISSIVE，通过_corrupt_record列打印出错误信息 3.使用option设置mode为DROPMALFORMED，如果碰到错误的数据，则自动丢弃 12val jsonDF: DataFrame = spark.read.option(\"mode\",\"DROPMALFORMED\").json(\"ruozedata-spark-sql/data/test.json\")jsonDF.show() 运行结果： 123456+---+---+---+| a| b| c|+---+---+---+| 1| 2| 3|| 7| 8| 9|+---+---+---+ 自定义UDAF函数 1.自定义一个UDAF的class或者object，作为具体的逻辑实现，需要继承UserDefinedAggregateFunction 123456789101112131415161718192021222324252627282930313233343536373839404142object AgeAvgUDAF extends UserDefinedAggregateFunction&#123; //输入类型 override def inputSchema: StructType = StructType( StructField(\"input\",DoubleType,true)::Nil ) //聚合内部中的buffer类型 override def bufferSchema: StructType = StructType( StructField(\"sums\",DoubleType,true):: //年龄和 StructField(\"num\",LongType,true)::Nil //人数 ) //输入数据类型 override def dataType: DataType = DoubleType //输入数据类型是否和输出数据类型相等 override def deterministic: Boolean = true //聚合内部buffer的初始化 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0.0 buffer(1) = 0L //buffer.update(0,0.0) //buffer.update(1,0L) &#125; //分区内更新聚合buffer override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer.update(0,buffer.getDouble(0)+input.getDouble(0)) buffer.update(1,buffer.getLong(1)+1) &#125; //分区间合并 override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1.update(0,buffer1.getDouble(0)+buffer2.getDouble(0)) buffer1.update(1,buffer1.getLong(1)+buffer2.getLong(1)) &#125; //最终计算 override def evaluate(buffer: Row): Any = &#123; buffer.getDouble(0)/buffer.getLong(1) &#125; &#125; 2.注册并使用UDAF 12345678910111213141516171819202122232425262728293031323334def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master(\"local[2]\") .appName(this.getClass.getSimpleName) .getOrCreate() // 自定义数据源 val list = new util.ArrayList[Row]() list.add(Row(\"pk\",28,\"男\")) list.add(Row(\"xingxing\",60,\"男\")) list.add(Row(\"J哥\",18,\"男\")) list.add(Row(\"ailsa\",18,\"女\")) list.add(Row(\"laura\",26,\"女\")) // 自定义Schema val schema = StructType( StructField(\"name\", StringType, true):: StructField(\"age\", IntegerType, true):: StructField(\"sex\", StringType, true)::Nil ) //创建df val df = spark.createDataFrame(list, schema) //创建视图 df.createOrReplaceTempView(\"people\") //注册UDAF spark.udf.register(\"age_avg_udaf\",AgeAvgUDAF) //使用UDAF spark.sql(\"select sex,age_avg_udaf(age) as ave_age from people group by sex\").show()&#125; 3.运行结果 123456+---+---------+|sex| ave_age|+---+---------+| 男| 35.33|| 女| 22.0|+---+---------+ 自定义UDTF函数 1.示例 1234567891011121314151617181920212223242526272829303132333435object ExplodeUDTF &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession .builder() .master(\"local[2]\") .appName(this.getClass.getSimpleName) .getOrCreate() // 自定义schema val schema = StructType( StructField(\"teacher\", StringType, true) :: StructField(\"sources\", StringType, true) :: Nil ) // 自定义数据源 val list = new util.ArrayList[Row]() list.add(Row(\"pk\", \"hive,spark,flink\")) list.add(Row(\"J哥\", \"cdh,kafka,hbase\")) // 创建临时视图 val df = spark.createDataFrame(list, schema) import spark.implicits._ // 使用flatMap拆分 df.flatMap(x =&gt; &#123; val line = new ListBuffer[(String, String)]() val sources = x.getString(1).split(\",\") for (source &lt;- sources)&#123; line.append((x.getString(0),source)) &#125; //返回 line &#125;).toDF(\"teacher\",\"source\").show() &#125;&#125; 2.运行结果 123456789|teacher|source|+-------+------+| PK| hive|| PK| spark|| PK| flink|| J哥| cdh|| J哥| kafka|| J哥| hbase|+-------+------+ 解读Spark SQL执行计划优化 1.建空表 1create table sqltest (key string,value string) 2.执行SQL 1explain extended select a.key*(3*5),b.value from sqltest a join sqltest b on a.key=b.key and a.key &gt;3; 3.解读执行计划 12345678910111213141516171819202122232425262728293031323334353637383940414243// 解析逻辑计划，做些简单的解析== Parsed Logical Plan =='Project [unresolvedalias(('a.key * (3 * 5)), None), 'b.value]+- 'Join Inner, (('a.key = 'b.key) &amp;&amp; ('a.key &gt; 3)) :- 'SubqueryAlias `a` : +- 'UnresolvedRelation `sqltest` +- 'SubqueryAlias `b` +- 'UnresolvedRelation `sqltest`// 分析逻辑计划，解析出了数据类型，拿到数据库和表，拿到了序列化方式 == Analyzed Logical Plan ==(CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE)): double, value: stringProject [(cast(key#2 as double) * cast((3 * 5) as double)) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]+- Join Inner, ((key#2 = key#4) &amp;&amp; (cast(key#2 as int) &gt; 3)) :- SubqueryAlias `a` : +- SubqueryAlias `default`.`sqltest` : +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3] +- SubqueryAlias `b` +- SubqueryAlias `default`.`sqltest` +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5]// 优化逻辑计划，数值类型的运算直接拿到结果，解析过滤条件== Optimized Logical Plan ==Project [(cast(key#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]+- Join Inner, (key#2 = key#4) :- Project [key#2] : +- Filter (isnotnull(key#2) &amp;&amp; (cast(key#2 as int) &gt; 3)) : +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3] +- Filter ((cast(key#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4)) +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5]// 物理计划，join方式为SortMergeJoin，数据使用hashpartitioning保存，扫描表的方式是HiveTableRelation== Physical Plan ==*(5) Project [(cast(key#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]+- *(5) SortMergeJoin [key#2], [key#4], Inner :- *(2) Sort [key#2 ASC NULLS FIRST], false, 0 : +- Exchange hashpartitioning(key#2, 200) : +- *(1) Filter (isnotnull(key#2) &amp;&amp; (cast(key#2 as int) &gt; 3)) : +- Scan hive default.sqltest [key#2], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3] +- *(4) Sort [key#4 ASC NULLS FIRST], false, 0 +- Exchange hashpartitioning(key#4, 200) +- *(3) Filter ((cast(key#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4)) +- Scan hive default.sqltest [key#4, value#5], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5] 可以简单的看做四步，分别是解析逻辑计划、分析逻辑计划、优化逻辑计划、物理执行计划","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark SQL之RDD转换DataFrame&DF和DS的转换&跨数据源操作&用SQL方式操作数据源&元数据catalog","date":"2019-07-20T14:19:36.000Z","path":"大数据技术/Spark SQL之RDD转换DataFrame&DF和DS的转换&跨数据源操作&用SQL方式操作数据源&元数据catalog/","text":"Spark SQL之RDD转换DataFrame&amp;DF和DS的转换&amp;跨数据源操作&amp;用SQL方式操作数据源&amp;catalog显示元数据信息 RDD转换DataFrame 方式1：使用反射来推断RDD的schema 这个方式简单，但是不建议使用，因为在工作当中，使用这种方式是有限制的。 对于以前的版本来说，case class最多支持22个字段如果超过了22个字段，我们就必须要自己开发一个类，实现product接口才行。因此这种方式虽然简单，但是不通用；因为生产中的字段是非常多的，不可能只有20来个字段，且不适用于创建外部数据源的场景。 123456789101112131415161718192021222324 /**1. 使用反射方式将RDD转成DF */ def reflection(spark: SparkSession): Unit = &#123; // RDD=&gt;DF时需要的隐式转换 import spark.implicits._ // 创建RDD val rdd = spark.sparkContext.textFile(\"ruozedata-spark-sql/data/info.txt\") // RDD[String] ==&gt; case class val infoDF = rdd.map(x =&gt; &#123; val splits = x.split(\",\") val id = splits(0).trim.toInt val name = splits(1).trim val age = splits(2).trim.toInt Info(id, name, age) &#125;).toDF() // 最终转成DF infoDF.printSchema() infoDF.show() &#125; case class Info(id: Int, name: String, age: Int) 方式2：通过编程接口，StructType可以构造Schema,然后将其应用于现有的RDD 官网：http://spark.apache.org/docs/latest/sql-getting-started.html#creating-dataframes When case classes cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps. Create an RDD of Rows from the original RDD; Create the schema represented by a StructType matching the structure of Rows in the RDD created in Step 1. Apply the schema to the RDD of Rows via createDataFrame method provided by SparkSession. 1234567891011121314151617181920212223242526272829/** * 使用编程方式 */def programmatically(spark: SparkSession): Unit = &#123; // RDD=&gt;DF时需要的隐式转换 import spark.implicits._ // 创建RDD val rdd = spark.sparkContext.textFile(\"ruozedata-spark-sql/data/info.txt\") // STEP1: RDD[String] ==&gt; RDD[Row] val infoRDD: RDD[Row] = rdd.map(x =&gt; &#123; val splits = x.split(\",\") val id = splits(0).trim.toInt val name = splits(1).trim val age = splits(2).trim.toInt Row(id, name, age) &#125;) // STEP2: schema val schema = StructType( StructField(\"id\", IntegerType, true) :: StructField(\"name\", StringType, false) :: StructField(\"age\", IntegerType, false) :: Nil) // STEP3: createDataFrame val df = spark.createDataFrame(infoRDD, schema) df.printSchema() df.show()&#125; DF和DS的转换 1234567891011121314151617181920212223242526272829303132333435/** * DF DS相互转换 */object DSApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName) .getOrCreate() import spark.implicits._ val df = spark.read.format(\"csv\") .option(\"header\",\"true\") .option(\"inferSchema\", \"true\") .load(\"ruozedata-spark-sql/data/sales.csv\") //DF转成DS val ds: Dataset[Sales] = df.as[Sales] val selectDF = df.select(\"customerId\") val selectDS = ds.map(_.customerId) //执行计划 println(selectDF.queryExecution.optimizedPlan.numberedTreeString) println(\".....................\") println(selectDS.queryExecution.optimizedPlan.numberedTreeString) spark.stop() &#125; case class Sales(transactionId:Int, customerId:Int, itemId:Int, amountPaid:Double )&#125; 跨数据源操作 跨数据源操作是Spark非常好用的一个特性，从不同的数据源拿到spark中，再从spark写出去 案例：从hive和mysql中分别拿出一个表join 1.jdbc 123456val mysqlDF = spark.read.format(\"jdbc\") .option(\"url\", \"jdbc:mysql://localhost:3306\") .option(\"dbtable\", \"sqoop.dept\") .option(\"user\", \"root\") .option(\"password\", \"ruozedata\") .load().show 2.hive idea里面连接hive需要如下操作 1)启动hive的metastore:hive --service metastore -p 9083 &amp; 2)把集群中的windows运行hive的配置文件放入到resources目录下 hive-site.xml 3.代码需要开启enableHiveSupport() 12345val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName) .enableHiveSupport() .getOrCreate() 4、join 123val hiveDF = spark.sql(\"select * from ruozedata_hive.emp\")// mysqlDF.join(hiveDF，mysql.col(\"deptno\")===hiveDF.col(\"deptno\")).show()mysqlDF.join(hiveDF，\"deptno\").show() 用SQL方式操作数据源 Spark官网每个数据源下都有一个Sql选项卡，提供了用SQL方式操作数据源的方法。 官网：http://spark.apache.org/docs/latest/sql-data-sources-json.html 1234567CREATE TEMPORARY VIEW jsonTableUSING org.apache.spark.sql.jsonOPTIONS ( path \"examples/src/main/resources/people.json\")SELECT * FROM jsonTable catalog显示元数据信息 12345678910111213141516171819202122232425262728/** * catalog显示元数据信息 */object CatalogApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName) .getOrCreate()import spark.implicits._ val catalog = spark.catalog catalog.listDatabases().show(false) catalog.listDatabases().map(_.name).show catalog.listTables(\"ruozedata_hive\").filter('name.contains(\"emp\")).show catalog.listTables(\"ruozedata_hive\").printSchema() catalog.isCached(\"ruozedata_hive.emp\") catalog.cacheTable(\"ruozedata_hive.emp\") catalog.uncacheTable(\"ruozedata_hive.emp\") catalog.listFunctions().filter('name === \"str_length\").show() spark.udf.register(\"str_length\",(str:String)=&gt;str.length) spark.stop() &#125;&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark SQL分组求topN&UDF函数","date":"2019-07-15T14:19:36.000Z","path":"大数据技术/Spark SQL分组求topN&UDF函数/","text":"本文介绍了用Spark SQL分组求topN的方式，初步了解Spark SQL的使用，并通过定义一个简单的UDF函数，让我们了解Spark SQL中UDF函数的定义方法。 分组求和 代码实现如下： 12345678910111213141516171819202122232425262728293031323334object LogApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName)// .enableHiveSupport() //开启HiveContext .getOrCreate() import spark.implicits._ val df = spark.read.textFile(\"ruozedata-spark-sql/data/access.log\") .map(x =&gt; &#123; val splits = x.split(\"\\t\") val platform = splits(4) val traffic = splits(8).toLong val province = splits(10) val city = splits(11) val isp = splits(12) (platform,traffic,province,city,isp) &#125;).toDF(\"platform\",\"traffic\",\"province\",\"city\",\"isp\")//方法1，SQL方式 df.createOrReplaceTempView(\"log\") spark.sql(\"select platform,province,city,sum(traffic) as traffics from log group by platform,province,city order by traffics desc\") .show(false)//方法2，API方式，需要导入org.apache.spark.sql.functions已经定义好的函数 import org.apache.spark.sql.functions._ df.groupBy(\"platform\",\"province\",\"city\") .agg(sum(\"traffic\").as(\"traffics\")) .sort('traffics.desc) //降序 .show() spark.stop() &#125;&#125; 运行结果： 12345678910111213141516171819202122232425+--------+--------+------+--------+|platform|province|city |traffics|+--------+--------+------+--------+|Androd |北京 |北京市|3606966 ||IOS |北京 |北京市|3476256 ||Androd |上海 |上海市|2844236 ||IOS |上海 |上海市|2720308 ||IOS |广东省 |广州市|1248001 ||Androd |江苏省 |苏州市|1188222 ||IOS |江苏省 |苏州市|1123646 ||Androd |广东省 |广州市|1070594 ||IOS |湖北省 |武汉市|1012666 ||Androd |湖北省 |武汉市|997467 ||IOS |福建省 |厦门市|843507 ||IOS |福建省 |漳州市|817904 ||Androd |江苏省 |无锡市|801299 ||IOS |四川省 |成都市|777718 ||Androd |四川省 |成都市|774126 ||IOS |江苏省 |无锡市|769022 ||Androd |福建省 |厦门市|767532 ||IOS |辽宁省 |朝阳市|745148 ||Androd |福建省 |漳州市|719662 ||Androd |辽宁省 |朝阳市|651604 |+--------+--------+------+--------+only showing top 20 rows 分组求topN 代码实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 分组求topn，SQL和API方式 */object topNApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName)// .enableHiveSupport() //开启HiveContext .getOrCreate() import spark.implicits._ val df = spark.read.textFile(\"ruozedata-spark-sql/data/access.log\") .map(x =&gt; &#123; val splits = x.split(\"\\t\") val platform = splits(4) val traffic = splits(8).toLong val province = splits(10) val city = splits(11) val isp = splits(12) (platform,traffic,province,city,isp) &#125;).toDF(\"platform\",\"traffic\",\"province\",\"city\",\"isp\") df.createOrReplaceTempView(\"log\") // 方法1:按照platform分组，province访问次数最多的TopN,SQL方式 val topNSQL =\"\"\" |select * from |( |select t.*,row_number() over(partition by platform order by cnt desc) as rank |from |(select platform,province,city,count(1) cnt from log group by platform,province,city) t |) a where a.rank&lt;3 |\"\"\".stripMargin spark.sql(topNSQL).show() // 方法2：按照platform分组，province访问次数最多的TopN,API方式 val df2 = df.groupBy(\"platform\", \"province\", \"city\").count() val windowRule: WindowSpec = Window.partitionBy(\"platform\").orderBy($\"count\".desc) import org.apache.spark.sql.functions._ df2.withColumn(\"rank\",row_number().over(windowRule)).where(\"rank&lt;3\").show() spark.stop() &#125;&#125; 运行结果： 1234567891011+--------+--------+------+-----+----+|platform|province| city|count|rank|+--------+--------+------+-----+----+| Androd| 北京|北京市| 1200| 1|| Androd| 上海|上海市| 953| 2|| IOS| 北京|北京市| 1164| 1|| IOS| 上海|上海市| 911| 2|+--------+--------+------+-----+----+Process finished with exit code 0 UDF函数 1、数据 123大狗 小破车,渣团,热刺,我纯桶子 利物浦二条 南大王,西班牙人 2、需求：求出每个人的爱好个数。（大狗4，桶子1，二娃2） 3、实现 1234567891011121314151617181920212223242526272829303132/** * 定义UDF函数 */object UDFApp &#123; def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName) .getOrCreate() import spark.implicits._ val df = spark.sparkContext.textFile(\"ruozedata-spark-sql/data/likes.txt\") .map(_.split(\"\\t\")) .map(x =&gt; Likes(x(0), x(1))) .toDF df.createOrReplaceTempView(\"likes\")//定义UDF函数 val teamsLengthUDF = spark.udf.register(\"teams_length\",(input:String) =&gt;&#123; input.split(\",\").length &#125;)//方法1：SQL方式 spark.sql(\"select name,teams,teams_length(teams) as teams_length from likes\").show(false)//方法2：API方式 df.select($\"name\",$\"teams\",teamsLengthUDF($\"teams\")).show(false) spark.stop() &#125; case class Likes(name: String, teams: String)&#125; 4.运行结果 12345678910+----+---------------------+-----------------------+|name|teams |UDF:teams_length(teams)|+----+---------------------+-----------------------+|pk |小破车,查团,热刺,我纯|4 ||桶子|利物浦 |1 ||二娃|男大王,西班牙人 |2 |+----+---------------------+-----------------------+Process finished with exit code 0","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":" Spark工作模式详解(local/standalone/yarn) ","date":"2019-07-15T06:15:39.000Z","path":"大数据技术/8.spark工作模式详解(localstandaloneyarn)/","text":"Spark工作模式详解(local/standalone/yarn) Spark 运行模式分类 本地模式 standalone模式 spark on yarn 模式，又分未yarn-client和yarn-cluster 本地模式-local Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类 local：只启动一个executor local[k]:启动k个executor local[*]：启动跟cpu数目相同的executor 注意： Spark单机运行，一般用于开发测试。 Local模式又称为本地模式，运行该模式非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用，而不用启动Spark的Master、Worker守护进程( 只有集群的Standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别 。 SparkSubmit进程，既是客户提交任务的Client进程、又是Spark的driver程序、还充当着Spark执行Task的Executor角色。 standalone模式 构建一个由Master+Slave构成的Spark集群，Spark运行在集群中。 分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础。 和单机运行的模式不同，这里必须在执行应用程序前，先启动Spark的Master和Worker守护进程。不用启动Hadoop服务，除非你用到了HDFS的内容 。 standalone模式角色说明: Master进程做为cluster manager，用来对应用程序申请的资源进行管理。 SparkSubmit做为Client端和运行driver程序。 CoarseGrainedExecutorBackend 用来并发执行应用程序。 Driver运行的位置: Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点、Master节点和 Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。 当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行； 当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。 standalone运行流程: 1.SparkContext连接到Master，向Master注册并申请资源（CPU Core 和Memory）； 2.Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend； 3.StandaloneExecutorBackend向SparkContext注册； 4.SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），DAG Scheduler将TaskSet提交给Task Scheduler，Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行； 5.StandaloneExecutorBackend会建立Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成； 6.所有Task完成后，SparkContext向Master注销，释放资源。 Spark on Yarn模式 Spark客户端直接连接Yarn。不需要额外构建Spark集群。 分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端。 Spark on yarn client模式 Driver运行位置: Driver在本地运行，并没有在nodemanager上，在nodemanager上启动的applicationMaster仅仅是一个ExecutorLanucher，功能十分有限。 运行流程: 1.Spark Yarn Client向YARN的ResourceManager发送请求，申请启动Application Master。同时在SparkContext初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend； 2.ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster（实际启动的是ExecutorLanucher，功能十分有限），与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派的ExecutorLanucher； 3.Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）； 4.一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task； 5.Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 6.应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。 spark on yarn cluster模式 Driver 运行位置: Driver运行在nodemanager上。 运行流程: Spark Yarn Client向YARN中resourcemanager提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等； ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster(相当于Driver客户端)，其中ApplicationMaster进行SparkContext等的初始化； ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束； 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等； ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。 Spark on yarn client与Spark on yarn Cluster之间的区别 yarn-client： 用于测试，因为driver运行在本地客户端，负责调度application，会与yarn集群产生超大量的网络通信。好处是直接执行时，本地可以看到所有的log，方便调试。 Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。 yarn-cluster： 生产环境使用， 因为driver运行在nodemanager上，缺点在于调试不方便，本地用spark-submit提价以后，看不到log，只能通过yarn application-logs application_id这种命令查看，很麻烦。 Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业。 总结： 理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Hive分区partition详解（静态分区，动态分区）","date":"2019-07-13T12:35:36.000Z","path":"大数据技术/Hive分区partition详解(静态分区，动态分区)/","text":"Hive分区partition详解（静态分区，动态分区） 参考博客：Hive分区partition详解（静态分区，动态分区） Hive分区是为了方便数据管理 Hive的分区方式：由于Hive实际是存储在HDFS上的抽象，Hive的一个分区名对应一个目录名，子分区名就是子目录名，并不是一个实际字段。 所以可以这样理解，当我们在插入数据的时候指定分区，其实就是新建一个目录或者子目录，或者在原有的目录上添加数据文件。 Hive分区的创建 Hive创建分区时，是通过PARTITIONED BY关键字进行创建，要注意的是这个关键字定义的列是表中正式的列，不能与表中其他列名重复，但是Hive下的数据文件不包含这些列，他们是目录（分区）名，目录下放的才是数据。 静态分区 1.单分区 1）创建单分区表 123456789101112create table table1(id int,name string,hobby array&lt;string&gt;,address map&lt;string,string&gt;) partitioned by (part string)row format delimitedfields terminated by ','collection items terminated by '-'map keys terminated by ':'lines terminated by '\\n'; 2）加载数据 数据如下： 1231,Tim,football-book-movie,beijing:hd-shanghai:pd2,Tom,book-movie-run,beijing:hd-shanghai:pd3,Aim,football-movie,beijing:hd-shanghai:pd 执行load 装载数据（其实load操作相当于把文件移动到HDFS的Hive目录下） 1load data local inpath '/opt/aaa' overwrite into table table1 partition (part='p1'); 2.多分区 分区逻辑大致是 部门分区下有性别分区 目录:dept/sex 1）创建多分区表 123456789101112create table table2(id int,name string,hobby array&lt;string&gt;,address map&lt;string,string&gt;) partitioned by (dept string,sex string)row format delimitedfields terminated by ','collection items terminated by '-'map keys terminated by ':'lines terminated by '\\n'; 2）加载数据到表中 分区字段都要加，否则会报错FAILED: SemanticException [Error 10006]: Line 1:88 Partition not found ”0” 12load data local inpath '/opt/aaa' overwrite into table table2 partition (dept='TD',sex='male');load data local inpath '/opt/aaa' overwrite into table table2 partition (dept='TD',sex='female'); 动态分区 如果用上述的静态分区，插入的时候必须首先要知道有什么分区类型，而且每个分区写一个load data，太烦人。使用动态分区可解决以上问题，其可以根据查询得到的数据动态分配到分区里。其实动态分区与静态分区区别就是不指定分区目录，由系统自己选择。 首先，启动动态分区功能 注意，动态分区不允许主分区采用动态列而副分区采用静态列，这样将导致所有的主分区都要创建副分区静态列所定义的分区。 动态分区可以允许所有的分区列都是动态分区列，但是要设置一个参数hive.exec.dynamic.partition.mode 12set hive.exec.dynamic.partition=true;set hive.exec.dynamic.partition.mode=nostrick; 表分区的增删改查 1.添加分区 1alter table table3 add partition (dept='HR',sex='male'); 2.删除分区 1alter table table3 drop partition (dept='TD',sex='male'); 删除相应的分区，该分区的内容也会消失 3.查询分区 1show partitions table_name; 4.修复分区 修复分区就是重新同步hdfs上的分区信息。 1msck repair table table_name;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Hive","slug":"Hive","permalink":"https://zzuuriel.github.io/tags/Hive/"}]},{"title":"Spark SQL常用数据源","date":"2019-07-09T14:19:36.000Z","path":"大数据技术/Spark SQL常用数据源/","text":"Spark SQL的DataFrame接口支持多种数据源的操作。本文介绍了json、text、csv、jdbc等常用的数据源。 Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。 当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：org.apache.spark.sql.parquet），如果数据源格式为内置格式，则只需要指定简称json, parquet, jdbc, orc, libsvm, csv, text来指定数据的格式。 JSON文件 1.读文件 123456789101112131415def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName) .getOrCreate() val df: DataFrame = spark.read.json(\"ruozedata-spark-sql/data/access.json\") import spark.implicits._ //方法1：需要加隐式转换，列名前加$，colume类型，可以使用UDF， df.select($\"appId\",$\"platform\",$\"traffic\",$\"user\").show() //方法2：适用于大部分场景，不可以使用UDF df.select(\"appId\",\"platform\",\"traffic\",\"user\").show() //方法3：有点麻烦，不建议使用 df.select(df(\"appId\"),df(\"platform\"),df(\"traffic\"),df(\"user\")).show() spark.stop() &#125; select方法用于选择要输出的列，推荐使用$&quot;col&quot;和&quot;col&quot;的方法 使用select方法可以选取打印的列，空值为null show()默认打印20条数据，可以指定条数 show()的truncate默认true,截取长度，可以设置为false filter写法 123df.select($\"appId\",$\"platform\",$\"traffic\",$\"user\").filter('user === \"Andy\").show() //推荐使用df.select($\"appId\",$\"platform\",$\"traffic\",$\"user\").filter(df(\"user\") === \"Andy\").show()df.select($\"appId\",$\"platform\",$\"traffic\",$\"user\").filter(\"user = 'Andy'\").show() 2.写文件 Spark写文件的时候只需要在df.write.format(“json”).mode().save()中指定存储格式，如json 1df.write.format(\"json\").mode(SaveMode.Overwrite).save(\"out\") Save操作可以选择使用SaveMode，它指定目标如果存在，如何处理现有数据。重要的是要认识到，这些保存模式不利用任何锁定，也不是原子性的。此外，在执行覆盖时，在写入新数据之前将删除数据。 官网：http://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes Scala/Java Any Language Meaning SaveMode.ErrorIfExists (default) “error” or “errorifexists” (default) 在将DataFrame保存到数据源时，如果数据已经存在，则会抛出error。 SaveMode.Append “append” 在将DataFrame保存到数据源时，如果数据/表已经存在，则DataFrame的内容将被append到现有数据中。 SaveMode.Overwrite “overwrite” overwrite模式意味着在将DataFrame保存到数据源时，如果数据/表已经存在，则现有数据将被DataFrame的内容覆盖 SaveMode.Ignore “ignore” ignore模式意味着在将DataFrame保存到数据源时，如果数据已经存在，则save操作不保存DataFrame的内容，也不更改现有数据。这类似于SQL中的CREATE TABLE IF NOT EXISTS。 Text文件 1.读文件 123456789101112131415161718192021222324252627282930313233343536def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder .master(\"local\") .appName(this.getClass.getSimpleName) .getOrCreate() import spark.implicits._ val df: DataFrame = spark.read.format(\"text\").load(\"D:\\\\testlog\\\\infos.txt\") //转换成RDD df.rdd.map(row=&gt;&#123; val splits = row.toString().split(\",\") (splits(1),splits(2)) &#125;).foreach(println(_)) //dataFrame不能直接split //返回是dataSet val mapDS: Dataset[(String, String)] = df.map(row =&gt; &#123; val splits = row.toString().split(\",\") (splits(1), splits(2)) &#125;) mapDS.show() //DataSet转换成DataFrame val mapDF = mapDS.toDF() mapDF.show() //使用textFile方法读取文本文件直接返回是一个DataSet val textDS: Dataset[String] = spark.read.textFile(\"ruozedata-spark-sql/data/people.txt\") textDS.map(row =&gt;&#123; val rows = row.split(\",\") (splits(1),splits(2)) &#125;).show() spark.stop() &#125; 文本数据读进来的一行在一个字段里面，所以要使用map算子，在map中split 直接read.format()读进来的是DataFrame，map中不能直接split DataFrame通过.rdd的方式转换成RDD，map中也不能直接split 通过read.textFile()的方式读进来的是Dataset，map中可以split 2.写文件 12345678910val textDS: Dataset[String] = spark.read.textFile(\"ruozedata-spark-sql/data/people.txt\")val ds = textDS.map(row =&gt; &#123; val rows = row.split(\",\") //拼接成一列 (rows(1) + \",\" + rows(2))&#125;)ds.write.format(\"text\") // 添加压缩 .option(\"compression\",\"gzip\") .mode(\"overwrite\").save(\"out\") 文本数据写出去的时候 不支持int类型，如果存在int类型，会报错，解决办法是toString，转换成字符串 只能作为一列输出，如果是多列，会报错，解决办法是拼接起来，组成一列 文本数据压缩输出，只要是Spark支持的压缩的格式，都可以指定 csv文件 12345678910//定义加载csv数据的方法def csv(spark: SparkSession): Unit = &#123; val df = spark.read.format(\"csv\") .option(\"header\",\"true\") //有头部信息 .option(\"sep\",\";\") //分隔符；默认是， .option(\"inferSchema\",\"true\") //推导Schema的类型 .load(\"ruozedata-spark-sql/data/people.csv\") df.printSchema() df.show()&#125; csv读取数据注意使用几个参数 指定表头：option(“header”, “true”) 指定分隔符：option(“sep”, “;”) 类型自动推测：option(“interSchema”,“true”) JDBC 1.read 1）添加依赖 12345678&lt;dependency&gt; &lt;groupId&gt;com.typesafe&lt;/groupId&gt; &lt;artifactId&gt;config&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; 2）application.conf文件 12345678910111213# JDBC Settingsdb.default.driver=\"com.mysql.jdbc.Driver\"db.default.url=\"jdbc:mysql://hadoop000:3306\"db.default.user=\"root\"db.default.password=\"123456\"db.default.dbtable=\"mysql\"db.default.srctable=\"mysql.user\"db.default.targettable=\"mysql.user_2\"# Connection Pool Settingsdb.default.poolInitialSize=10db.default.poolMaxSize=20db.default.connectionTimeoutMillis=1000 3） 定义加载jdbc数据的方法 1234567891011121314151617181920212223 def jdbc(spark: SparkSession): Unit = &#123; val config = ConfigFactory.load() val url = config.getString(\"db.default.url\") val user = config.getString(\"db.default.user\") val password = config.getString(\"db.default.password\") val srctable = config.getString(\"db.default.srctable\") val targettable = config.getString(\"db.default.targettable\") val driver = config.getString(\"db.default.driver\") val database = config.getString(\"db.default.database\") val df = spark.read.format(\"jdbc\") .option(\"url\", url) .option(\"dbtable\", database+\".\"+srctable) .option(\"user\", user) .option(\"password\", password) .option(\"driver\", driver) .load() df.createOrReplaceTempView(\"tmp\") spark.sql(\"select * from tmp\").show() &#125; df.createOrReplaceTempView()方法创建一个DataFrame数据生成的临时表，提供spark.sql()使用SQL操作数据，返回的也是一个DataFrame 2.write 12345678910val df = spark.sql(\"select * from tmp\")df.filter('user === \"pk\") .write.format(\"jdbc\") .option(\"url\", url) .option(\"dbtable\", database+\".\"+targettable) .option(\"user\", user) .option(\"password\", password) .option(\"driver\", driver) .save()","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark SQL基本概念&&DataFrame&&Dataset","date":"2019-07-06T14:19:36.000Z","path":"大数据技术/Spark SQL基本概念&&DataFrame&&Dataset/","text":"Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame，并且可以充当分布式SQL查询引擎。 产生背景及特点 Hive是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性。 由于MapReduce这种计算模型执行效率比较慢，所以Spark SQL应运而生，它是将结构化数据转换成RDD，然后提交到集群执行，执行效率非常快！同时Spark SQL也支持从Hive中读取数据。 特点： 使用Spark编程无缝对接SQL查询 使用SQL或者DataFrameAPI查询处理结构化数据 统一数据访问 使用DataFrame和SQL可以访问多种数据源,包括Hive,Avro,Parquet,ORC,JSON,JDBC等 Hive集成 支持HiveQL语法,UDF等 标准连接 支持JDBC,ODBC连接 SparkSQL发展历程详见：SparkSQL的原理以及架构 DataFrame 在Spark中，DataFrame是一种以RDD为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame与RDD的主要区别在于，前者带有schema元信息，即DataFrame所表示的二维表数据集的每一列都带有名称和类型。这使得Spark SQL得以洞察更多的结构信息，从而对藏于DataFrame背后的数据源以及作用于DataFrame之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观RDD，由于无从得知所存数据元素的具体内部结构，Spark Core只能在stage层面进行简单、通用的流水线优化。 DataFrame在概念上等同于关系数据库中的表，但在底层具有更丰富的优化。DataFrame可以从各种来源构建，例如： 结构化数据文件 Hive中的表 外部数据库或现有RDD 从上图可以看出，DataFrame多了数据的结构信息，即schema。RDD是分布式的 Java对象的集合，DataFrame是分布式的Row对象的集合。 DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化。 DataFrame的优点： 结构化数据处理非常方便，支持Avro, CSV, elastic search, and Cassandra等kv数据，也支持HIVE tables, MySQL等传统数据表 有针对性的优化，如采用Kryo序列化，由于数据结构元信息spark已经保存，序列化时不需要带上元信息，大大的减少了序列化大小，而且数据保存在堆外内存中，减少了gc次数,所以运行更快。 hive兼容，支持hql、udf等 DataFrame的缺点： 编译时不能类型转化安全检查，运行时才能确定是否有问题 对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象 Dataset Dataset是数据的分布式集合。Dataset是在Spark 1.6中添加的一个新接口，是DataFrame之上更高一级的抽象。它提供了RDD的优点（强类型化，使用强大的lambda函数的能力）以及Spark SQL优化后的执行引擎的优点。DataSet引入了丰富的API来对高结构化的数据进行控制，一个Dataset 可以从JVM对象构造，然后使用函数转换（map， flatMap，filter等）去操作。 Dataset API 支持Scala和Java。 Python不支持Dataset API。 DataSet可以理解成DataFrame的一种特例，DataFrame = DataSet[Row]，主要区别是DataSet每一个record存储的是一个强类型值而不是一个Row，DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。 DateSet的优点： DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据 和RDD一样，支持自定义对象存储 和DataFrame一样，支持结构化数据的sql查询 采用堆外内存存储，gc友好 类型转化安全，代码友好 SparkSession Apache Spark 2.0引入了SparkSession，其为用户提供了一个统一的切入点来使用Spark的各项功能，并且允许用户通过它调用DataFrame和Dataset相关API来编写Spark程序。最重要的是，它减少了用户需要了解的一些概念，使得我们可以很容易地与Spark交互。 在2.0版本之前，与Spark交互之前必须先创建SparkConf和SparkContext。然而在Spark 2.0中，我们可以通过SparkSession来实现同样的功能，而不需要显式地创建SparkConf, SparkContext 以及 SQLContext，因为这些对象已经封装在SparkSession中。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"spark-shell脚本分析","date":"2019-07-06T12:12:36.000Z","path":"大数据技术/spark-shell脚本分析/","text":"Spark的bin目录下，spark-shell脚本是工业级脚本，可以供我们学习参考。本文通过spark-shell脚本，剖析了spark程序提交流程。 spark-shell脚本 可以看到在spark-shell脚本中main()方法调用了spark-submit脚本； 12345678910function main() &#123; if $cygwin; then stty -icanonmin 1 -echo &gt; /dev/null 2&gt;&amp;1 export SPARK_SUBMIT_OPTS=\"$SPARK_SUBMIT_OPTS -Djline.terminal=unix\" \"$FWDIR\"/bin/spark-submit --class org.apache.spark.repl.Main \"$&#123;SUBMISSION_OPTS[@]&#125;\" spark-shell \"$&#123;APPLICATION_OPTS[@]&#125;\"sttyicanon echo &gt; /dev/null 2&gt;&amp;1 else export SPARK_SUBMIT_OPTS \"$FWDIR\"/bin/spark-submit --class org.apache.spark.repl.Main \"$&#123;SUBMISSION_OPTS[@]&#125;\" spark-shell \"$&#123;APPLICATION_OPTS[@]&#125;\"fi spark-submit脚本 可以看到在spark-submit脚本中，首先检查是否设置了SPARK_HOME,然后调用了spark-class脚本，把SparkSubmit 类和输入的参数都作为spark-class的参数。 1exec \"$&#123;SPARK_HOME&#125;\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\" spark-class脚本 spark-class脚本里面的执行逻辑较多，整体上是：检查spark_home是否配置 -&gt; 执行load_spark-env.sh去加载spark-env.sh文件并设置scala环境 -&gt; 检查java的执行路径变量 -&gt; 寻找spark jars -&gt; 执行类文件org.apache.spark.launcher.Main，解析过滤参数，构建执行命令并将解析后的参数给返回给spark-class脚本的CMD -&gt; 最后通过 exec “${CMD[@]}” 真正调用org.apache.spark.deploy.SparkSubmit这个类，后面跟上所有参数。 1234567891011121314151617181920212223242526272829303132build_command() &#123; \"$RUNNER\" -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\"# echo \"$RUNNER -Xmx128m -cp $LAUNCH_CLASSPATH org.apache.spark.launcher.Main $@\" printf \"%d\\0\" $?&#125; # Turn off posix mode since it does not allow process substitutionset +o posixCMD=()while IFS= read -d '' -r ARG; do CMD+=(\"$ARG\")done &lt; &lt;(build_command \"$@\") COUNT=$&#123;#CMD[@]&#125;LAST=$((COUNT - 1))LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125; # Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes# the code that parses the output of the launcher to get confused. In those cases, check if the# exit code is an integer, and if it's not, handle it as a special error case.if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then echo \"$&#123;CMD[@]&#125;\" | head -n-1 1&gt;&amp;2 exit 1fi if [ $LAUNCHER_EXIT_CODE != 0 ]; then exit $LAUNCHER_EXIT_CODEfi CMD=(\"$&#123;CMD[@]:0:$LAST&#125;\")exec \"$&#123;CMD[@]&#125;\" org.apache.spark.launcher.Main类 spark-class脚本关键的一句就是build_command里面的函数里面的。&quot;$RUNNER&quot; -Xmx128m -cp “$LAUNCH_CLASSPATH” org.apache.spark.launcher.Main “$@” 这一句，可以看出，入口类就是org.apache.spark.launcher.Main，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public static void main(String[] argsArray) throws Exception &#123; checkArgument(argsArray.length &gt; 0, \"Not enough arguments: missing class name.\"); List&lt;String&gt; args = new ArrayList&lt;&gt;(Arrays.asList(argsArray)); String className = args.remove(0); boolean printLaunchCommand = !isEmpty(System.getenv(\"SPARK_PRINT_LAUNCH_COMMAND\")); AbstractCommandBuilder builder; //根据传进来的参数创建命令SparkSubmitCommandBuilder或者SparkClassCommandBuilder if (className.equals(\"org.apache.spark.deploy.SparkSubmit\")) &#123; try &#123; builder = new SparkSubmitCommandBuilder(args); &#125; catch (IllegalArgumentException e) &#123; printLaunchCommand = false; System.err.println(\"Error: \" + e.getMessage()); System.err.println(); MainClassOptionParser parser = new MainClassOptionParser(); try &#123; parser.parse(args); &#125; catch (Exception ignored) &#123; // Ignore parsing exceptions. &#125; List&lt;String&gt; help = new ArrayList&lt;&gt;(); if (parser.className != null) &#123; help.add(parser.CLASS); help.add(parser.className); &#125; help.add(parser.USAGE_ERROR); builder = new SparkSubmitCommandBuilder(help); &#125; &#125; else &#123; builder = new SparkClassCommandBuilder(className, args); &#125; Map&lt;String, String&gt; env = new HashMap&lt;&gt;(); //buildCommand List&lt;String&gt; cmd = builder.buildCommand(env); if (printLaunchCommand) &#123; System.err.println(\"Spark Command: \" + join(\" \", cmd)); System.err.println(\"========================================\"); &#125; //返回有效的参数，存到CMD中，最后spark-class脚本中执行exec \"$&#123;CMD[@]&#125;\" if (isWindows()) &#123; System.out.println(prepareWindowsCommand(cmd, env)); &#125; else &#123; // In bash, use NULL as the arg separator since it cannot be used in an argument. List&lt;String&gt; bashCmd = prepareBashCommand(cmd, env); for (String c : bashCmd) &#123; System.out.print(c); System.out.print('\\0'); &#125; &#125; &#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"},{"name":"Shell","slug":"Shell","permalink":"https://zzuuriel.github.io/tags/Shell/"}]},{"title":"Spark调优详图","date":"2019-07-03T12:19:36.000Z","path":"大数据技术/Spark调优详图/","text":"Spark调优详图 开发调优 资源调优 数据倾斜调优 shuffle相关参数调优","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Core-分组求topn","date":"2019-07-02T12:17:36.000Z","path":"大数据技术/Spark Core-分组求topn/","text":"Spark Core-分组求topn Spark Core-分组求topn 需求： 求相同域名下，访问次数最多的前2个url。 思考： 1.toList能不能用？toList容易引起OOM 2.域名domain如何取出？distinct 3.rdd能嵌套rdd么？ val domains = processRDD.map(_._1._1).distinct().collect()不加collect试试 4.自定义分区器的好处？Web UI查看 虽然做了distinct,但是域名如果多了，UI还是会爆掉(sortBy会很多)，故自定义一个分区器，相同域名放在同一个分区 5.take需要将所有数据都拉取到Driver上才能完成操作，如何优化？treeSet 方法1： 12345678910111213141516171819202122232425/** * toList会炸掉，生产上不能这么用 */object TopNApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val topN = 2; val lines = sc.textFile(\"ruozedata-spark-core/data/site.log\") lines.map(x =&gt; &#123; val splits = x.split(\",\") val domain = splits(10) val url = splits(14) ((domain, url), 1) &#125;).reduceByKey(_+_) .groupBy(_._1._1) // 取出domain，然后分组 .mapValues(x =&gt; &#123; x.toList.sortBy(-_._2).map(x =&gt; (x._1._2, x._2)).take(topN) &#125;) .foreach(println) sc.stop() &#125;&#125; 方法2： 12345678910111213val lines = sc.textFile(\"/ruozedata-spark-core/topn/site.log\")val processRDD = lines.map(x =&gt; &#123; val splits = x.split(\",\") val domain = splits(10) val url = splits(14) ((domain, url), 1)&#125;)val domains = processRDD.map(_._1._1).distinct().collect()domains.foreach(x =&gt; &#123; processRDD.filter(_._1._1 == x).reduceByKey(_ + _) .sortBy(-_._2).take(topN)&#125;) 每个domian进行sortBy、take都是会触发action，domian太多UI会炸掉 方法3、方法4： 自定义分区器，将所有domain放到一个分区，再触发action，UI上的job会少很多。 12345678910111213class RuozedataPartitioner(domains: Array[String]) extends Partitioner&#123; val map = mutable.HashMap[String, Int]()// map的key是domain，value是int for(i&lt;-0 until(domains.length)) &#123; map(domains(i)) = i &#125; override def numPartitions: Int = domains.length override def getPartition(key: Any): Int = &#123;// ((domain, url), 1)，key是 tuple类型 val domain = key.asInstanceOf[(String, String)]._1 map(domain) &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041object TopNApp03 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val topN = 1; val lines = sc.textFile(\"ruozedata-spark-core/data/site.log\") val processRDD = lines.map(x =&gt; &#123; val splits = x.split(\"\\t\") val domain = splits(10) val url = splits(14) ((domain, url), 1) &#125;) val domains = processRDD.map(_._1._1).distinct().collect() //相同的domain放到一个分区中去 val result = processRDD.reduceByKey(new RuozedataPartitioner(domains),_ + _) //方法3：take// result.mapPartitions(partition =&gt; &#123;// partition.toList.sortBy(-_._2).take(topN).iterator// &#125;).foreach(println)//方法4：使用treeSet // TODO... Tree... order var treeSet = new mutable.TreeSet[((String,String), Int)]()(new MyOrdering()) result.mapPartitions(partition =&gt; &#123; partition.foreach(x =&gt; &#123; // TODO... treeSet.add(x) treeSet.add(x) if(treeSet.size &gt; topN) &#123; treeSet.remove(treeSet.last) &#125; &#125;) treeSet.iterator &#125;).foreach(println) sc.stop() &#125;&#125; 自定义排序： 1234567class MyOrdering extends Ordering[((String,String),Int)]&#123; override def compare(x: ((String,String),Int), y: ((String,String),Int)): Int = &#123; val xField = x._2.toInt val yField = y._2.toInt yField - xField &#125;&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"累加器和广播变量&&持久化","date":"2019-06-28T12:17:36.000Z","path":"大数据技术/累加器和广播变量&&持久化/","text":"累加器和广播变量&amp;&amp;持久化 累加器和广播变量 Spark有两种共享变量：广播变量（broadcast variable）与累加器（accumulator） 累加器用来对信息进行聚合，而广播变量用来高效分发较大的对象。 共享变量出现的原因： 通常情况下，当向Spark操作(如map,reduce)传递一个函数时，它会在一个远程集群节点上执行，它会使用函数中所有变量的副本。这些变量被复制到所有的机器上，远程机器上并没有被更新的变量向驱动程序回传。通常跨任务的读写变量是低效的，但是，Spark还是提供了两种有限的共享变量：广播变量（broadcast variable）和累加器 累加器 在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到累加器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会在driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。 累加器的用法如下所示： (1)通过在Driver中调用 SparkContext.accumulator(initialValue) 方法，创建出存有初始值的累加器。返回值为 org.apache.spark.Accumulator[T] 对象，其中 T 是初始值initialValue 的类型。 (2)Spark闭包（函数序列化）里的excutor代码可以使用累加器的 += 方法（在Java中是 add ）增加累加器的值。 (3)Driver程序可以调用累加器的 value 属性（在 Java 中使用 value() 或 setValue() ）来访问累加器的值。 计数器种类很多，但是经常使用的就是两种，longAccumulator和collectionAccumulator 需要注意的是计数器是lazy的，只有触发action才会进行计数，在不持久的情况下重复触发action,计数器会重复累加 1、LongAccumulator 1234567891011121314val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"MyLongAccumulator\") val sc = new SparkContext(sparkConf) val acc = sc.longAccumulator(\"计数\") val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9)) val numberRDD = rdd.map(x=&gt;&#123; acc.add(1L) &#125;) numberRDD.count() println(acc.value) //9 numberRDD.count() println(acc.value) //18 numberRDD.count() println(acc.value) //27 sc.stop() 使用longAccumulator做计数的时候要小心重复执行action导致的acc.value的变化,这是因为重复执行了count,累加器的数量成倍增长，解决方法，在action操作之前调用rdd的cache方法（或persist）,这样在count后数据集就会被缓存下来，而无需从头开始计算 1234567numberRDD.cache() numberRDD.count() println(acc.value) //9 numberRDD.count() println(acc.value) //9 numberRDD.count() println(acc.value) //9 2、CollectionAccumulator CollectionAccumulator,集合计数器，计数器中保存的是集合元素，通过泛型指定 12345678910111213141516171819202122def main(args: Array[String]): Unit = &#123; /** * 需求:id后三位相同的加入计数器 */ val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"MyLongAccumulator\") val sc = new SparkContext(sparkConf) //生成集合计数器 val acc = sc.collectionAccumulator[People](\"集合计数器\") //生成RDD val rdd = sc.parallelize(Array(People(\"p1\", 100000), People(\"p2\", 100001), People(\"p3\", 100222), People(\"p4\", 100003))) rdd.map(x=&gt;&#123; val id = x.id.toString.reverse //满足条件就加入计数器 if(id(0) == id(1) &amp;&amp; id(0) == id(2))&#123; acc.add(x) &#125; &#125;).count() println(acc.value) //[People(p1,100000), People(p3,100222)] sc.stop() &#125; case class People(name:String,id:Long); 广播变量 如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么只是每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。 小表广播案例 spark有一种常见的优化方式就是小表广播，使用map join来代替reduce join,我们通过把小表的数据集广播到各个节点上，节省了shuffle操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\") .setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) // Fact table 航线(起点机场, 终点机场, 航空公司, 起飞时间) val flights = sc.parallelize(List( (\"SEA\", \"JFK\", \"DL\", \"7:00\"), (\"SFO\", \"LAX\", \"AA\", \"7:05\"), (\"SFO\", \"JFK\", \"VX\", \"7:05\"), (\"JFK\", \"LAX\", \"DL\", \"7:10\"), (\"LAX\", \"SEA\", \"DL\", \"7:10\"))) // Dimension table 机场(简称, 全称, 城市, 所处城市简称) val airports = sc.parallelize(List( (\"JFK\", \"John F. Kennedy International Airport\", \"New York\", \"NY\"), (\"LAX\", \"Los Angeles International Airport\", \"Los Angeles\", \"CA\"), (\"SEA\", \"Seattle-Tacoma International Airport\", \"Seattle\", \"WA\"), (\"SFO\", \"San Francisco International Airport\", \"San Francisco\", \"CA\"))) // Dimension table 航空公司(简称,全称) val airlines = sc.parallelize(List( (\"AA\", \"American Airlines\"), (\"DL\", \"Delta Airlines\"), (\"VX\", \"Virgin America\"))) //最终统计结果： //出发城市 终点城市 航空公司名称 起飞时间 //Seattle New York Delta Airlines 7:00 //San Francisco Los Angeles American Airlines 7:05 //San Francisco New York Virgin America 7:05 //New York Los Angeles Delta Airlines 7:10 //Los Angeles Seattle Delta Airlines 7:10 val airportsBc = sc.broadcast(airports.map(x =&gt; (x._1, x._3)).collectAsMap()) val airlinesBc = sc.broadcast(airlines.collectAsMap()) flights.map&#123; case (a,b,c,d) =&gt; (airportsBc.value.get(a).get, airportsBc.value.get(b).get, airlinesBc.value.get(c).get, d ) &#125;.foreach(println) sc.stop() &#125; 为什么只能 broadcast 只读的变量 这就涉及一致性的问题，如果变量可以被更新，那么变量被某个节点更新，其他节点需要一块更新，这涉及了事务一致性。 注意事项： 变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改 能不能将一个RDD使用广播变量广播出去？因为RDD是不存储数据的。可以将RDD的结果广播出去。 广播变量只能在Driver端定义，不能在Executor端定义。 在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。 如果Executor端用到了Driver的变量，不使用广播变量在Executor有多少task就有多少Driver端的变量副本。 如果Executor端用到了Driver的变量，使用广播变量在每个Executor中只有一份Driver端的变量副本。 持久化 Spark非常重要的一个功能特性就是可以将RDD持久化在内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，直接使用内存缓存的partition。这样的话，对于针对一个RDD反复执行多个操作的场景，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要反复计算多次该RDD。 巧妙使用RDD持久化，甚至在某些场景下，可以将spark应用程序的性能提升10倍。对于迭代式算法和快速交互式应用来说，RDD持久化，是非常重要的。 持久化的存储级别很多，常用的是MEMORY_ONLY、MEMORY_ONLY_SER、MEMORY_AND_DISK 如何选择存储级别？ Storage Level的选择是内存和CPU的权衡 1.如果内存足够，默认的存储级别（MEMORY_ONLY (不进行序列化)）是性能最优的，最高效的。 2.如果内存不够且CPU跟的上，可以尝试MEMORY_ONLY_SER 再加上一个序列化框架(kyro），这样内存的空间更好。 3.不要把数据写到磁盘，这样成本是非常高的，当数据太大的时候，可以过滤一部分的数据再存，这样的话可能会更快。 4.可以使用副本的存储级别能更快的容错，所有的storage level都提供了副本机制（从另外的节点拿）。 首选第1种方式，如果满足不了再使用第2种。后两种不推荐。 12345scala&gt; numberRDD.cacheres18: forRDD.type = MapPartitionsRDD[9] at map at &lt;console&gt;:27scala&gt; numberRDD.countres19: Long = 8 结果可以在Web UI的Storage中查看 Spark自动监视每个节点上的缓存使用情况，并以最近最少使用(LRU)的方式删除旧的数据分区。如果想要手动删除一个RDD，而不是等待它从缓存中消失，那么可以使用RDD.unpersist()方法,清除缓存数据是立即执行的 12scala&gt; numberRDD.unpersist()res8: numberRDD.type = MapPartitionsRDD[3] at map at &lt;console&gt;:28 修改存储级别 1234val forRDD = rdd.map(x =&gt; &#123; //计数器做累加 acc.add(1L)&#125;).persist(StorageLevel.MEMORY_ONLY_SER).count() cache和persist有什么区别和联系？ 使用cache()和persist()进行持久化操作，它们都是lazy的，需要action才能触发，默认使用MEMORY_ONLY cache调用的persist，persist调用的persist(storage level) 序列化和非序列化有什么区别？ 序列化将对象转换成字节数组了，节省空间，占CPU 测试：开启kyro序列化（需要注册） 1234567891011121314151617val sparkConf = new SparkConf() .setMaster(\"local[*]\").setAppName(\"SerApp\") ---------开启序列化------ .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") ------注册----- .registerKryoClasses(Array(classOf[Student])) /** * 10w条数据 原大小2908KB * 1:Kryo serialization 注册 2029.1KB * 2:Java serialization 3.1 MB * 3:Kryo serializationkryo 不注册 4.8 MB * */ rdd.persist(StorageLevel.MEMORY_ONLY_SER).count() case class Student(id: String, name: String, age: Int)","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":" Spark Core基础-WordCount增强","date":"2019-06-21T12:17:36.000Z","path":"大数据技术/Spark Core基础-WordCount增强/","text":"Spark Core基础-WordCount增强 生产上很多问题 都是WordCount或者WordCount的变形，下面列举了3个关于WordCount的例子。 WordCount01App 需求： 123456a,1,3a,2,3b,1,1==&gt;a,3,6b,1,1 实现： 123456789101112131415161718192021222324object WordCount01App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) sc.parallelize(List( List(\"a\",1,3), List(\"a\",2,4), List(\"b\",1,1) )).map(x =&gt; &#123; val key = x(0) val v1 = x(1).toString.toInt val v2 = x(2).toString.toInt (key,(v1,v2)) &#125;).reduceByKey((x,y) =&gt; &#123; (x._1+y._1, x._2+y._2) &#125;).map(x =&gt; (x._1,x._2._1,x._2._2)) .foreach(println) sc.stop() &#125;&#125; WordCount02App 需求： 123456789用户 节目 列表 点击001，一起看|电视剧|军旅|亮剑，1,1001，一起看|电视剧|军旅|亮剑，1,0002，一起看|电视剧|军旅|士兵突击，1,1==&gt;001,一起看，2,1001，电视剧，2,1001，军旅，2,1001，亮剑，2,1 实现： 123456789101112131415161718192021222324252627282930313233object WordCount02App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val lines = sc.parallelize(List( \"001,一起看|电视剧|军旅|亮剑,1,1\", \"001,一起看|电视剧|军旅|亮剑,1,0\", \"002,一起看|电视剧|军旅|士兵突击,1,1\" )) lines.flatMap(x =&gt; &#123; val splits = x.split(\",\") val id = splits(0) val word = splits(1) val imp = splits(2).toInt val click = splits(3).toInt val words = word.split(\"\\\\|\")//两种实现方法，一种是reducebykey，一种是groupbykey words.map(x =&gt; ((id,x),(imp,click))) &#125;).groupByKey().mapValues(x =&gt; &#123; val imps = x.map(_._1).sum val clicks = x.map(_._2).sum (imps, clicks) &#125;).foreach(println) //.reduceByKey((x,y) =&gt; (x._1+y._1, x._2+y._2)).foreach(println) sc.stop() &#125;&#125; WordCount03App 需求： 多目录输出文件，按照使用平台作为key，输出文件名为使用平台名。 实现： Spark内部没有多文件输出的函数供大家直接调用，需要我们自定义一个类继承与MultipleOutputFormat，代码如下： 123456789101112131415161718/** * 继承MultipleTextOutputFormat[Any,Any]，实现多目录输出 */class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123; override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123; s\"$key/$name\"// 自定义输出路径// s\"$key/20281022\" &#125; //key的输出 override def generateActualKey(key: Any, value: Any): Any = NullWritable.get() //value的输出 override def generateActualValue(key: Any, value: Any): Any = &#123; value.asInstanceOf[String] &#125;&#125; 调用saveAsHadoopFile函数并传入自定义的RuozedataMultipleTextOutputFormat类，代码如下 1234567891011121314151617object WordCount03App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val out = \"out\" FileUtils.delete(sc.hadoopConfiguration, out) sc.textFile(\"ruozedata-spark-core/data/access.log\",1) .map(x =&gt; &#123; val splits = x.split(\"\\t\") (splits(1), x) &#125;).saveAsHadoopFile(out, classOf[String],classOf[String],classOf[RuozedataMultipleTextOutputFormat]) sc.stop() &#125;&#125; access.log示例： 12345ruozedata Android 10.0.2ruozedata Symbian 10.0.1ruozedata Symbian 10.0.2ruozedata iOS 10.0.1ruozedata iOS 10.0.2","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Core基础之Spark监控","date":"2019-06-19T13:12:36.000Z","path":"大数据技术/Spark Core基础之Spark监控/","text":"Spark Core基础之Spark监控 官方文档:http://spark.apache.org/docs/latest/monitoring.html 历史日志监控 如果应用程序的事件日志存在，仍然可以通过 spark 的历史服务器构建应用程序的UI。 可以通过执行以下命令来启动历史服务器: 1./sbin/start-history-server.sh 这将创建一个默认 http://&lt; server-url &gt;:18080 的 web 界面，列出不完整和已完成的应用程序和尝试。 spark 作业本身必须配置为记录事件，并将它们记录到同一个共享的可写目录。 例如，如果服务器配置了hdfs://namenode/shared/spark-logs的日志目录， 那么客户端的选择就是: 在spark/conf下的spark-defaults.conf配置 12spark.eventLog.enabled truespark.eventLog.dir hdfs://namenode/shared/spark-logs 在spark-env.sh中配置 12默认端口为18080export SPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=hdfs://namenode/shared/spark-logs -Dspark.history.retainedApplications=30 -Dspark.history.ui.port=47653\" 参数说明： spark.eventLog.dir：Application在运行过程中所有的信息均记录在该属性指定的路径下； spark.history.ui.port=18080 WEBUI访问的端口号为18080 spark.history.fs.logDirectory 配置了该属性后，在start-history-server.sh时就无需再显式的指定路径，Spark History Server页面只展示该指定路径下的信息 spark.history.retainedApplications=30 指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。 web 界面地址： http://&lt; server-url &gt;:47653 代码监控（ metrics） 1.创建一个监听器，继承于SparkListener 1234567891011121314151617181920212223242526272829303132333435363738394041class RuozedataSparkListener(conf:SparkConf) extends SparkListener with Logging&#123; override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = &#123; /** * 从TaskMetrics能够拿到很多信息，可以把信息都持久化到Hbase/Redis/RDBMS * private val _executorDeserializeTime = new LongAccumulator * private val _executorDeserializeCpuTime = new LongAccumulator * private val _executorRunTime = new LongAccumulator * private val _executorCpuTime = new LongAccumulator * private val _resultSize = new LongAccumulator * private val _jvmGCTime = new LongAccumulator * private val _resultSerializationTime = new LongAccumulator * private val _memoryBytesSpilled = new LongAccumulator * private val _diskBytesSpilled = new LongAccumulator * private val _peakExecutionMemory = new LongAccumulator * private val _updatedBlockStatuses = new CollectionAccumulator[(BlockId, BlockStatus)] */ //通过SparkConf拿到Spark作业的名字 val appname = conf.get(\"spark.app.name\") val metrics = taskEnd.taskMetrics val taskMetricsMap = mutable.HashMap( \"executorDeserializeTime\" -&gt; metrics.executorDeserializeTime, \"executorDeserializeCpuTime\" -&gt; metrics.executorDeserializeCpuTime, \"executorRunTime\" -&gt; metrics.executorRunTime, \"resultSize\" -&gt; metrics.resultSize, \"taskId\" -&gt; taskEnd.taskInfo.taskId ) logError(appname) //如果spark.send.mail.enabled为true，发送邮件，否则不发送 if (\"true\" == conf.get(\"spark.send.mail.enabled\")) &#123; MsgUtils.send(\"桶子\",s\"$appname _task\", Json(DefaultFormats).write(taskMetricsMap)) &#125; &#125;&#125; 2.邮件发送 12345678910111213141516171819202122232425262728293031323334public class MsgUtils &#123; public static void send(String recivers, String title, String content) throws Exception &#123; Properties properties = new Properties(); properties.setProperty(\"mail.host\",\"smtp.qq.com\"); properties.setProperty(\"mail.transport.protocol\",\"smtp\"); properties.setProperty(\"mail.smtp.auth\", \"true\"); properties.setProperty(\"mail.smtp.ssl.enable\",\"true\"); MailSSLSocketFactory factory = new MailSSLSocketFactory(); factory.setTrustAllHosts(true); properties.put(\"mail.smtp.ssl.socketFactory\", factory); Authenticator authenticator = new Authenticator() &#123; @Override protected PasswordAuthentication getPasswordAuthentication() &#123; String username = \"123456@qq.com\"; String password = \"mmuvyreksgfudhhb\"; return new PasswordAuthentication(username, password); &#125; &#125;; Session session = Session.getInstance(properties, authenticator); MimeMessage message = new MimeMessage(session); InternetAddress from = new InternetAddress(\"123456@qq.com\"); message.setFrom(from); InternetAddress[] tos = InternetAddress.parse(\"5555555@qq.com\"); message.setRecipients(Message.RecipientType.TO, tos); message.setSubject(title); message.setContent(content, \"text/html;charset=UTF-8\"); Transport.send(message); &#125; 3.测试使用自定义监听器 1234567891011121314151617object SparkListenerWC &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf() .setMaster(\"local[2]\") //设置是否需要发送邮件 .set(\"spark.send.mail.enabled\",\"true\") //设置使用自定义的监听器 .set(\"spark.extraListeners\", \"com.ruozedata.spark.SparkListener.RuozedataSparkListener\") .setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val sc2 = sc.parallelize(List(\"pk,pk,pk\", \"J,J\", \"xingxing\")).repartition(3) val result = sc2.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _).collect() sc.stop() &#125;&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"窄依赖和宽依赖&Spark任务中的Stage(源码解读)","date":"2019-06-15T12:12:36.000Z","path":"大数据技术/窄依赖和宽依赖&Spark任务中的Stage(源码解读)/","text":"窄依赖和宽依赖&amp;Spark任务中的Stage(源码解读) RDD 的依赖关系 RDD的依赖 RDD之间的依赖关系分为窄依赖（narrow dependency）和宽依赖（wide dependency, 也称 shuffle dependency）. 窄依赖 窄依赖是指父RDD的每个分区只被子RDD的一个分区所使用，子RDD一般对应父RDD的一个或者多个分区，不会产生shuffle。 宽依赖 宽依赖指父RDD的多个分区可能被子RDD的一个分区所使用，子RDD分区通常对应所有的父RDD分区，会产生shuffle。 Lineage（血统） Lineage：RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。 Dependency源码 Dependency是一个抽象类： 1234// org.apache.spark.Dependency.scalaabstract class Dependency[T] extends Serializable &#123; def rdd: RDD[T]&#125; 它有两个子类：NarrowDependency 和 ShuffleDenpendency，分别对应窄依赖和宽依赖。 NarrowDependency抽象类 定义了抽象方法getParents，输入partitionId，用于获得child RDD 的某个partition依赖的parent RDD的所有 partitions。 12345678910// Dependency.scalaabstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123; /** * Get the parent partitions for a child partition. * @param partitionId a partition of the child RDD * @return the partitions of the parent RDD that the child partition depends upon */ def getParents(partitionId: Int): Seq[Int] override def rdd: RDD[T] = _rdd&#125; 窄依赖分为两种： 一种是一对一的依赖，即OneToOneDependency，指子RDD的partition只依赖于父RDD 的一个partition，产生OneToOneDependency的算子有map，filter，flatMap等。 还有一个是范围的依赖，即RangeDependency，它仅仅被org.apache.spark.rdd.UnionRDD使用。UnionRDD是把多个RDD合成一个RDD，这些RDD是被拼接而成，即每个parent RDD的Partition的相对顺序不会变，只不过每个parent RDD在UnionRDD中的Partition的起始位置不同。 1.OneToOneDependency源码 123class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123; override def getParents(partitionId: Int): List[Int] = List(partitionId)&#125; 可以看到getParents实现很简单，就是传进去一个partitionId，再把partitionId放在List里面传出去。 2.RangeDependency源码 1234567891011//inStart表示parent RDD的开始索引，outStart表示child RDD 的开始索引class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int) extends NarrowDependency[T](rdd) &#123; override def getParents(partitionId: Int): List[Int] = &#123; if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123; List(partitionId - outStart + inStart) //表示于当前索引的相对位置 &#125; else &#123; Nil &#125; &#125;&#125; ShuffleDependency抽象类 表示一个父RDD的partition会被子RDD的partition使用多次，需要经过shuffle。 1234567891011121314151617181920212223242526272829class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ &lt;: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false) extends Dependency[Product2[K, V]] &#123;//shuffle都是基于PairRDD进行的，所以传入的RDD要是key-value类型的 if (mapSideCombine) &#123; require(aggregator.isDefined, \"Map-side combine without Aggregator specified!\") &#125; override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]] private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName // Note: It's possible that the combiner class tag is null, if the combineByKey // methods in PairRDDFunctions are used instead of combineByKeyWithClassTag. private[spark] val combinerClassName: Option[String] = Option(reflect.classTag[C]).map(_.runtimeClass.getName) val shuffleId: Int = _rdd.context.newShuffleId() val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle( shuffleId, _rdd.partitions.length, this) _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))&#125;&#125; partitioner：重输出的数据如何分区，partition shuffle结果 serializer: 由于shuffle涉及到网络传输，所以要有序列化serializer keyOrdering: shuffle结果key如何排序 aggregator：map/reduce端的聚合 mapSideCombine: 是否map端聚合 为了减少网络传输，可以map端聚合，通过mapSideCombine和aggregator控制 两种依赖的区分 首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。例如，逐个元素地执行map、然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点之间进行Shuffle，这与MapReduce类似。 第二，窄依赖能够更有效地进行失效节点的恢复，即只需重新计算丢失RDD分区的父分区，而且不同节点之间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。 Spark任务中的Stage DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 RDD存在着依赖关系，这些依赖关系形成了有向无环图DAG，DAG通过DAGScheduler进行Stage的划分，并基于每个Stage生成了TaskSet，提交给TaskScheduler。 作业的提交 SparkContext.scala 12345678910111213141516171819//SparkContext.scaladef runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], resultHandler: (Int, U) =&gt; Unit): Unit = &#123; if (stopped.get()) &#123; throw new IllegalStateException(\"SparkContext has been shutdown\") &#125; val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) &#123; logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) &#125; dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() &#125; DAGScheduler.scala 123456789101112131415161718192021222324// DAGScheduler.scaladef runJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =&gt; Unit, properties: Properties): Unit = &#123; val start = System.nanoTime val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) ThreadUtils.awaitReady(waiter.completionFuture, Duration.Inf) waiter.completionFuture.value.get match &#123; case scala.util.Success(_) =&gt; logInfo(\"Job %d finished: %s, took %f s\".format (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9)) case scala.util.Failure(exception) =&gt; logInfo(\"Job %d failed: %s, took %f s\".format (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9)) // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler. val callerStackTrace = Thread.currentThread().getStackTrace.tail exception.setStackTrace(exception.getStackTrace ++ callerStackTrace) throw exception &#125; &#125; 可以看到，SparkContext的runjob方法调用了DAGScheduler的runjob方法正式向集群提交任务，最终调用了submitJob方法。 1234567891011121314151617181920212223242526272829// DAGScheduler.scaladef submitJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =&gt; Unit, properties: Properties): JobWaiter[U] = &#123; // Check to make sure we are not launching a task on a partition that does not exist. val maxPartitions = rdd.partitions.length partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt; throw new IllegalArgumentException( \"Attempting to access a non-existent partition: \" + p + \". \" + \"Total number of partitions: \" + maxPartitions) &#125; val jobId = nextJobId.getAndIncrement() if (partitions.size == 0) &#123; // Return immediately if the job is running 0 tasks return new JobWaiter[U](this, jobId, 0, resultHandler) &#125; assert(partitions.size &gt; 0) val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _] val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler) //给eventProcessLoop发送JobSubmitted消息 eventProcessLoop.post(JobSubmitted( jobId, rdd, func2, partitions.toArray, callSite, waiter, SerializationUtils.clone(properties))) waiter&#125; 这里向eventProcessLoop对象发送了JobSubmitted消息。 123456789101112131415161718192021222324252627282930313233343536// DAGScheduler.scala private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this) eventProcessLoop是DAGSchedulerEventProcessLoop类的一个对象。// DAGScheduler.scala private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123; case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt; dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt; dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties) case StageCancelled(stageId) =&gt; dagScheduler.handleStageCancellation(stageId) case JobCancelled(jobId) =&gt; dagScheduler.handleJobCancellation(jobId) case JobGroupCancelled(groupId) =&gt; dagScheduler.handleJobGroupCancelled(groupId) case AllJobsCancelled =&gt; dagScheduler.doCancelAllJobs() case ExecutorAdded(execId, host) =&gt; dagScheduler.handleExecutorAdded(execId, host) case ExecutorLost(execId, reason) =&gt; val filesLost = reason match &#123; case SlaveLost(_, true) =&gt; true case _ =&gt; false &#125; dagScheduler.handleExecutorLost(execId, filesLost) case BeginEvent(task, taskInfo) =&gt; dagScheduler.handleBeginEvent(task, taskInfo) case GettingResultEvent(taskInfo) =&gt; dagScheduler.handleGetTaskResult(taskInfo) case completion: CompletionEvent =&gt; dagScheduler.handleTaskCompletion(completion) case TaskSetFailed(taskSet, reason, exception) =&gt; dagScheduler.handleTaskSetFailed(taskSet, reason, exception) case ResubmitFailedStages =&gt; dagScheduler.resubmitFailedStages() &#125; DAGSchedulerEventProcessLoop对接收到的消息进行处理，在doOnReceive方法中形成一个event loop。 接下来将调用submitStage()方法进行stage的划分。 stage的划分 12345678910111213141516171819202122// DAGScheduler.scala private def submitStage(stage: Stage) &#123; val jobId = activeJobForStage(stage)//查找该Stage的所有激活的job if (jobId.isDefined) &#123; logDebug(\"submitStage(\" + stage + \")\") if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123; val missing = getMissingParentStages(stage).sortBy(_.id)//得到Stage的父Stage，并排序 logDebug(\"missing: \" + missing) if (missing.isEmpty) &#123; logInfo(\"Submitting \" + stage + \" (\" + stage.rdd + \"), which has no missing parents\") submitMissingTasks(stage, jobId.get)//如果Stage没有父Stage，则提交任务集 &#125; else &#123; for (parent &lt;- missing) &#123;//如果有父Stage，递归调用submiStage submitStage(parent) &#125; waitingStages += stage//将其标记为等待状态，等待下次提交 &#125; &#125; &#125; else &#123; abortStage(stage, \"No active job for stage \" + stage.id, None)//如果该Stage没有激活的job，则丢弃该Stage &#125; &#125; 在submitStage方法中判断Stage的父Stage有没有被提交，直到所有父Stage都被提交，只有等父Stage完成后才能调度子Stage。 123456789101112131415161718192021222324252627282930// DAGScheduler.scalaprivate def getMissingParentStages(stage: Stage): List[Stage] = &#123; val missing = new HashSet[Stage] //用于存放父Stage val visited = new HashSet[RDD[_]] //用于存放已访问过的RDD val waitingForVisit = new Stack[RDD[_]] def visit(rdd: RDD[_]) &#123; if (!visited(rdd)) &#123; //如果RDD没有被访问过，则进行访问 visited += rdd //添加到已访问RDD的HashSet中 val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil) if (rddHasUncachedPartitions) &#123; for (dep &lt;- rdd.dependencies) &#123; //获取该RDD的依赖 dep match &#123; case shufDep: ShuffleDependency[_, _, _] =&gt;//若为宽依赖，则该RDD依赖的RDD所在的stage为父stage val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)//生成父Stage if (!mapStage.isAvailable) &#123;//若父Stage不存在，则添加到父Stage的HashSET中 missing += mapStage &#125; case narrowDep: NarrowDependency[_] =&gt;//若为窄依赖，则继续访问父RDD waitingForVisit.push(narrowDep.rdd) &#125; &#125; &#125; &#125; &#125; waitingForVisit.push(stage.rdd) while (waitingForVisit.nonEmpty) &#123;//循环遍历所有RDD visit(waitingForVisit.pop()) &#125; missing.toList &#125; getmissingParentStages()方法为核心方法。 Stage是通过shuffle划分的，所以每一个Stage都是以shuffle开始的，若一个RDD是宽依赖，则必然说明该RDD的父RDD在另一个Stage中，若一个RDD是窄依赖，则该RDD所依赖的父RDD还在同一个Stage中，我们可以根据这个逻辑，找到该Stage的父Stage。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Shell脚本高级","date":"2019-06-13T12:35:36.000Z","path":"article/Shell脚本高级/","text":"Shell脚本高级 生产shell规范 脚本后缀 .sh 第一行 #!/bin/bash [hadoop@ruozedata002 shell]$ vi standard.sh #!/bin/bash 脚本解释器 开头加作者版本等信息 12345678910111213141516171819202122232425262728293031#!/bin/bash# ./standard.sh db table where#---------------------------------------------#FileName: standard.sh#Version: 1.0#Date: 2019-06-13#Author: ruozedata-J#Description: example of shell script#Notes: project ....#---------------------------------------------USAGE=\"Usage : $0 db table where\"[ $# -ne 3 ] &amp;&amp; echo \"$USAGE\" &amp;&amp; exit 1#startecho \"www.ruozedata.com\"if elifelifelse exit fi#endexit 0 exit 0 正常结束 非0 异常退出 脚本尽量不要有中文字符 变量定义要大写 = 前后不能空格 成对的符号尽量一次性写出来 防止遗漏 {} [] [[]] ‘’ “” [] 里面两端要有空格[ “$RZ” == “RUOZEDATA” ] 公共模块 抽离出来 debug 1234567[hadoop@ruozedata002 shell]$ sh -x standard.sh+ USAGE='Usage : standard.sh db table where'+ '[' 0 -ne 3 ']'+ echo 'Usage : standard.sh db table where'Usage : standard.sh db table where+ exit 1[hadoop@ruozedata002 shell]$ 函数式编程 Shell 函数的本质是一段可以重复使用的脚本代码，这段代码被提前编写好了，放在了指定的位置，使用时直接调取即可。 Shell 函数定义的语法格式如下： 1234function name() &#123; statements [return value]&#125; 对各个部分的说明： function是 Shell 中的关键字，专门用来定义函数； name是函数名； statements是函数要执行的代码，也就是一组语句； return value表示函数的返回值，其中 return 是 Shell 关键字，专门用在函数中返回一个值；这一部分可以写也可以不写。 由{ }包围的部分称为函数体，调用一个函数，实际上就是执行函数体中的代码。 示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[hadoop@ruozedata002 shell]$ vi function.shfunction ruozedata()&#123; echo \"hello ruozedata\"&#125;function ruozedataByParameter()&#123; echo \"$&#123;1&#125;,$&#123;2&#125;\" SUM=$(( $&#123;1&#125; + $&#123;2&#125; )) return $&#123;SUM&#125;&#125;function ruozedataByParameter1()&#123; echo \"$&#123;1&#125;,$&#123;2&#125;\" SUM=$(( $&#123;1&#125; + $&#123;2&#125; )) echo \"$&#123;SUM&#125;\"&#125;#COMMsource /home/hadoop/shell/mysqlconn.shecho $&#123;URL&#125;#call funtionruozedata#use $? 0-255ruozedataByParameter 1000 2000echo $?echo \"---------------\"RESULT=$(ruozedataByParameter 1000 2000)echo $RESULTecho \"---------------\"RESULT1=$(ruozedataByParameter1 1000 2000)echo $RESULT1echo \"---------------\"RESULT2=$(ruozedata)echo $RESULT2","categories":[],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://zzuuriel.github.io/tags/Shell/"}]},{"title":"Shell脚本if判断（数字条件，字符串条件，文件或文件夹是否存在","date":"2019-06-12T14:12:36.000Z","path":"article/Shell脚本if判断（数字条件，字符串条件，文件或文件夹是否存在/","text":"Shell脚本if判断（数字条件，字符串条件，文件或文件夹是否存在） 整数比较 12345678910-eq 等于,如:if [ \"$a\" -eq \"$b\" ] -ne 不等于,如:if [ \"$a\" -ne \"$b\" ] -gt 大于,如:if [ \"$a\" -gt \"$b\" ] -ge 大于等于,如:if [ \"$a\" -ge \"$b\" ] -lt 小于,如:if [ \"$a\" -lt \"$b\" ] -le 小于等于,如:if [ \"$a\" -le \"$b\" ] &lt; 小于(需要双括号),如:((\"$a\" &lt; \"$b\")) &lt;= 小于等于(需要双括号),如:((\"$a\" &lt;= \"$b\")) &gt; 大于(需要双括号),如:((\"$a\" &gt; \"$b\")) &gt;= 大于等于(需要双括号),如:((\"$a\" &gt;= \"$b\")) 示例： 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bash i=6a=10 if [ $a -eq 10 ]then echo \"a = 10\"fi if [ $a -ne $i ]then echo \"a != $i\"fi if [ $a -gt $i ]then echo \"a &gt; i\"fi if [ $a -lt $i ]then echo \"a &lt; i\"else echo \"a &gt; i\"fi if((\"$a\" &gt; \"$i\"))then echo \"(())a&gt;i\"fi if(($a != $i))then echo \"(())a!=i\"fi 字符串比较 12= 等于,如:if [ \"$a\" = \"$b\" ] == 等于,如:if [ \"$a\" == \"$b\" ],与=等价 注意:==的功能在[[]]和[]中的行为是不同的,如下: [[ $a == z* ]] # 如果$a以&quot;z&quot;开头(模式匹配)那么将为true [[ $a == “z*” ]] # 如果$a等于z*(字符匹配),那么结果为true [ $a == z* ] # File globbing 和word splitting将会发生 [ “$a” == “z*” ] # 如果$a等于z*(字符匹配),那么结果为true 示例： 12345678910111213141516#!/bin/bash a=\"123\"b=\"1234\"c=\"123\" if [ \"$a\"x != \"$b\"x ]then echo \"a != b\"fi if [ \"$a\"x = \"$c\"x ]then echo \"a == c\"fi 判断字符串为空： 1234if [ -z \"$d\" ]then echo \"d is empty\"fi 文件或文件夹是否存在 1234567891011121314151617181920212223242526#!/bin/bash#判断文件存在，判断是否为文件夹等testPath=\"/home/hadoop/shell\"testFile=\"/home/hadoop/shell/test.txt\"#判断文件夹是否存在 -dif [[ ! -d \"$testPath\" ]]; then echo \"文件夹不存在\"else echo \"文件夹存在\"fi#判断文件夹是否存在，并且具有可执行权限if [[ ! -x \"$testFile\" ]]; then echo \"文件不存在并且没有可执行权限\"else echo \"文件存在并有可执行权限\"fi#判断文件是否存在if [[ ! -f \"$testFile\" ]]; then echo \"文件不存在\"else echo \"文件存在\"fi 文件比较符： 12345678910111213-e 判断对象是否存在-d 判断对象是否存在，并且为目录-f 判断对象是否存在，并且为常规文件-L 判断对象是否存在，并且为符号链接-h 判断对象是否存在，并且为软链接-s 判断对象是否存在，并且长度不为0-r 判断对象是否存在，并且可读-w 判断对象是否存在，并且可写-x 判断对象是否存在，并且可执行-O 判断对象是否存在，并且属于当前用户-G 判断对象是否存在，并且属于当前用户组-nt 判断file1是否比file2新 [ \"/data/file1\" -nt \"/data/file2\" ]-ot 判断file1是否比file2旧 [ \"/data/file1\" -ot \"/data/file2\" ] 备注 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253-e 文件存在-a 文件存在（已被弃用）-f 被测文件是一个regular文件（正常文件，非目录或设备）-s 文件长度不为0-d 被测对象是目录-b 被测对象是块设备-c 被测对象是字符设备-p 被测对象是管道-h 被测文件是符号连接-L 被测文件是符号连接-S(大写) 被测文件是一个socket-t 关联到一个终端设备的文件描述符。用来检测脚本的stdin[-t0]或[-t1]是一个终端-r 文件具有读权限，针对运行脚本的用户-w 文件具有写权限，针对运行脚本的用户-x 文件具有执行权限，针对运行脚本的用户-u set-user-id(suid)标志到文件，即普通用户可以使用的root权限文件，通过chmod +s file实现-k 设置粘贴位-O 运行脚本的用户是文件的所有者-G 文件的group-id和运行脚本的用户相同-N 从文件最后被阅读到现在，是否被修改f1 -nt f2 文件f1是否比f2新f1 -ot f2 文件f1是否比f2旧f1 -ef f2 文件f1和f2是否硬连接到同一个文件二元比较操作符，比较变量或比较数字整数比较：-eq 等于 if [ \"$a\" -eq \"$b\" ]-ne 不等于 if [ \"$a\" -ne \"$b\" ]-gt 大于 if [ \"$a\" -gt \"$b\" ]-ge 大于等于 if [ \"$a\" -ge \"$b\" ]-lt 小于 if [ \"$a\" -lt \"$b\" ]-le 小于等于 if [ \"$a\" -le \"$b\" ]&lt; 小于（需要双括号） (( \"$a\" &lt; \"$b\" ))&lt;= 小于等于(...) (( \"$a\" &lt;= \"$b\" ))&gt; 大于(...) (( \"$a\" &gt; \"$b\" ))&gt;= 大于等于(...) (( \"$a\" &gt;= \"$b\" ))字符串比较：= 等于 if [ \"$a\" = \"$b\" ]== 与=等价!= 不等于 if [ \"$a\" = \"$b\" ]&lt; 小于，在ASCII字母中的顺序：if [[ \"$a\" &lt; \"$b\" ]]if [ \"$a\" \\&lt; \"$b\" ] #需要对&lt;进行转义&gt; 大于-z 字符串为null，即长度为0-n 字符串不为null，即长度不为0","categories":[],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://zzuuriel.github.io/tags/Shell/"}]},{"title":"Shell脚本基础","date":"2019-06-12T12:12:36.000Z","path":"article/Shell脚本基础/","text":"Shell脚本基础 Shell入门 定义 123[root@hadoop000 shell]# vi wordcount.sh#!/bin/bashecho \"www.ruozedata.com\" 赋x权限 123456789101112[root@hadoop000 shell]# lltotal 4-rw-r--r--. 1 root root 40 Jun 12 21:11 wordcount.shchmod +x wordcount.sh[root@hadoop000 shell]# chmod 654 wordcount.sh[root@hadoop000 shell]# lltotal 4-rw-r-xr--. 1 root root 40 Jun 12 21:11 wordcount.sh[root@hadoop000 shell]# ./wordcount.sh www.ruozedata.com 调试与sh命令 12345678[root@hadoop000 shell]# vi wordcount.sh #!/bin/bash -xecho \"www.ruozedata.com\"[root@hadoop000 shell]# ./wordcount.sh + echo www.ruozedata.comwww.ruozedata.com 123[root@hadoop000 shell]# sh -x wordcount.sh+ echo www.ruozedata.comwww.ruozedata.com 总结: 1.脚本开头定义 #!/bin/bash 2.调试的方法有两种 直接执行sh -x xxx.sh 开头定义#!/bin/bash -x 执行./xxx.sh #!/bin/bash -x 这种方法会影响结果查看 变量定义及引用 12345678910111213141516171819202122232425262728293031[root@hadoop000 shell]# vi variable.sh#!/bin/bashrz=\"www.ruozedata.com\"date=`date`echo $rzecho $date[root@hadoop000 shell]# sh variable.shwww.ruozedata.comTue Jun 12 21:28:01 CST 2018[root@hadoop000 shell]# sh variable.shwww.ruozedata.comTue Jun 12 21:28:03 CST 2018[root@hadoop000 shell]# 静态: k=\"v\"动态: k=`v`=前后不能有空格引用: $k ==&gt; $rz $&#123;k&#125; ==&gt; $&#123;rz&#125;jepsonecho $rzjepsonecho $&#123;rz&#125;jepson 总结: 1.=前后不能有空格 2.字符串建议双引号 3.引用变量加上{} 传递参数 1234567891011121314151617181920212223242526272829[root@hadoop000 shell]# vi parameter.sh#!/bin/bashecho $1echo $2echo \"个数:$#\"echo \"传递参数作为1个字符串显示: $*\"echo \"PID: $$\"~~~~~\"parameter.sh\" 10L, 113C written[root@hadoop000 shell]# sh parameter.sh a b ab个数:2传递参数作为1个字符串显示: a bPID: 2647[root@hadoop000 shell]# [root@hadoop000 shell]# [root@hadoop000 shell]# sh parameter.sh \"a b\" a b个数:1传递参数作为1个字符串显示: a bPID: 2649[root@hadoop000 shell]# 数组 123456789101112131415[root@hadoop000 shell]# vi array.sh#!/bin/basharr=(rz jepson xingxing huhu qianxi)echo $&#123;arr[*]&#125;echo $&#123;arr[4]&#125;echo $&#123;#arr[*]&#125;[root@hadoop000 shell]# sh array.sh rz jepson xingxing huhu qianxiqianxi5[root@hadoop000 shell]# 这里的* 也可以用@表示 if判断 方法1： if [ $a == $b ];then 1234567891011121314151617181920212223[root@hadoop000 shell]# vi if.sh#!/bin/bash a=\"abc\"b=\"jepson\"if [ $a == $b ];then echo \"==\"else echo \"!=\"fi[root@hadoop000 shell]# sh if.sh !=[root@hadoop000 shell]# [root@hadoop000 shell]# sh -x if.sh + a=abc+ b=jepson+ '[' abc == jepson ']'+ echo '!='!=[root@hadoop000 shell] 方法2： if [ “$a” == “$b” ] then 1234567891011121314151617181920212223242526272829[root@hadoop000 shell]# vi if.shthen#!/bin/basha=\"ccc\"b=\"jepson\"if [ \"$a\" == \"$b\" ]then echo \"==\"elif [ \"$a\" == \"ccc\" ]then echo \"ccc\"else echo \"!=\"fi[root@hadoop000 shell]# [root@hadoop000 shell]# [root@hadoop000 shell]# sh -x if.sh + a=ccc+ b=jepson+ '[' ccc == jepson ']'+ '[' ccc == ccc ']'+ echo cccccc[root@hadoop000 shell]# 循环 123456789101112131415161718192021222324252627282930[root@hadoop000 shell]# vi forwhile.sh#!/bin/bash#方法1：for x in 1 2 3 4 5do echo $x let \"j++\"doneecho \" \"#方法2：for ((i=1;i&lt;10;i++))do echo $idoneecho \" \"#方法3：j=1while(($j&lt;10))do echo $j let \"j++\"done 推荐方法2 分割语法 12345678910111213141516171819202122232425[root@hadoop000 shell]# vi spilt.sh #!/bin/bash#方法1s=\"rz,jepson,xingxing,huhu,qianxi\"OLD_IFS=\"$IFS\"IFS=\",\"arr=($s)IFS=\"$OLD_IFS\"for x in $&#123;arr[*]&#125;do echo $xdoneecho \"-----------------------\"#方法2arr2=($&#123;s//,/ &#125;)for x in $&#123;arr2[*]&#125;do echo $xdone","categories":[],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://zzuuriel.github.io/tags/Shell/"}]},{"title":"Spark Cluster Mode Overview 官网翻译","date":"2019-06-11T12:12:36.000Z","path":"大数据技术/Spark Cluster Mode Overview 官网翻译/","text":"Spark Cluster Mode Overview 官网翻译 文档地址：http://spark.apache.org/docs/latest/cluster-overview.html Cluster Mode Overview(集群模式概述) This document gives a short overview of how Spark runs on clusters, to make it easier to understand the components involved. Read through the application submission guide to learn about launching applications on a cluster. 这篇文档介绍了Spark在集群上运行的大概情况，让我们更容易理解其各个组件是如何交互的。我们可以通读 application submission guide 来学习如何在集群上部署应用程序。 Components(组件) Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run. Spark应用程序在一个集群上运行着一组独立的进程，包括一个driver和多个executors，在你应用程序的main函数里通过SparkContext对象来协调组织，我们也把Spark applications称之为driver program。 具体来说，集群模式下，SparkContext能够连接不同类型的cluster managers，比如说Spark自己的standalone cluster manager， Mesos或者YARN，而这些cluster managers所扮演的角色是在各个应用程序application之间分配资源。一旦Spark连接上这些cluster managers，Spark就获得了分布在集群各个节点上的executors，这些executors其实是一系列的进程，这些进程执行我们的应用程序application中的计算并存储相关的数据。接着，SparkContext将我们的应用程序application代码发送给executors，这些应用程序application代码是由JAR或者Python文件所定义并且传给SparkContext。最后，SparkContext把tasks发送给executors去执行。 There are several useful things to note about this architecture: Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system. Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN). The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes. Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes. 关于这个架构，有以下几个有用的地方需要注意： 每个应用程序application都有属于它自己本身的executor进程，这些进程横跨这个application的整个生命周期并且以多线程的方式来执行内部的多个tasks。这有个好处，每个application之间无论是在调度层面scheduling side还是在执行层面 executor side都是相互隔离的。也就是说从调度层面来看，每个driver调度属于它自身的tasks，从执行层面上来看，属于不同applications的tasks运行在不同的JVM上。然而，这也意味着不同的Spark applications（也可以说是SparkContext的实例）是不能共享各自所属的数据，除非，你把数据写到外部存储系统，比如说Alluxio。 Spark不关心底层的cluster manager是哪种类型。只要Spark可以获取得到executor进程，并且这些executor进程能够互相通信，那么对于同样支持其他applications的cluster manager来说，比如Mesos/YARN，都是能去运行Spark程序的。 在driver program的整个生命周期中，它一直在监听并且接收来自属于它本身的executors的连接。可以查看 spark.driver.port in the network config section。因此，driver program必须跟各个worker nodes节点网络互通。 由于driver是在集群上调度各个任务的，按理来说，它应该运行在靠近worker nodes的节点上，最好是在同一个局域网里。如果你想发送请求给远端的集群，最佳的方式是你给driver开一个RPC，并且让driver在靠近worker nodes的节点上提交作业，而不是在远离worker nodes的节点上运行driver。 Cluster Manager Types(Cluster Manager类型) The system currently supports several cluster managers: Standalone – a simple cluster manager included with Spark that makes it easy to set up a cluster. Apache Mesos – a general cluster manager that can also run Hadoop MapReduce and service applications. Hadoop YARN – the resource manager in Hadoop 2. Kubernetes – an open-source system for automating deployment, scaling, and management of containerized applications. A third-party project (not supported by the Spark project) exists to add support for Nomad as a cluster manager. 如今Spark生态目前支持以下几个cluster managers: Standalone – 一个简单的cluster manager，它内置了Spark，使得我们能够快速的启动一个集群。 Apache Mesos – 一个通用的cluster manager，它能够运行Hadoop MapReduce以及service applications。 Hadoop YARN – Hadoop 2的resource manager。 Kubernetes – 一个开源的项目，用于自动部署，扩容以及容器内部应用的管理。 Submitting Applications（提交Applications） Applications can be submitted to a cluster of any type using the spark-submit script. The application submission guide describes how to do this. 通过spark-submit脚本，我们可以把应用Applications提交到任何类型的集群上。这篇 application submission guide 文章描述了具体的实现方式。 Monitoring（监控） Each driver program has a web UI, typically on port 4040, that displays information about running tasks, executors, and storage usage. Simply go to http://:4040 in a web browser to access this UI. The monitoring guide also describes other monitoring options. 每个driver program都有它自己的一套web UI界面，通常运行在4040端口，它详细展示了当前运行的tasks，executors，和存储使用情况等相关信息。我们可以通过浏览器访问http://:4040来浏览这个UI界面。这篇 monitoring guide 文章详细介绍了其他监控选项。 Job Scheduling(作业调度) Spark gives control over resource allocation both across applications (at the level of the cluster manager) and within applications (if multiple computations are happening on the same SparkContext). The job scheduling overview describes this in more detail. Spark不仅仅通过cluster manager在各个应用applications之间来控制资源的分配，而且在应用applications内部，当同一个SparkContext里面同时有多个计算同时运行的时候，Spark同样会去控制资源如何分配。详情请看 job scheduling overview。 Glossary(术语) The following table summarizes terms you’ll see used to refer to cluster concepts: 下面的表格总结了一些你可能会碰到的关于集群的概念和术语","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":" Spark RDD定义与特性 ","date":"2019-06-11T01:16:09.000Z","path":"大数据技术/4.spark RDD定义与特性/","text":"Spark RDD定义与特性 官网 http://spark.apache.org 什么是RDD RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性 RDD的特性 RDD在源码的介绍 12345- A list of partitions- A function for computing each split- A list of dependencies on other RDDs- Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)- Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) 一组分片（partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度，用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值，默认值就是程序所分配到的CPU core的数目。 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 RDD在源码的体现 打开源码RDD.scala 对应第二个特性 def compute(split: Partition, context: TaskContext): Iterator[T] 计算：其实是对RDD立面的每个分区做计算 传入的参数：split类型是Partition，context类型是TaskContext 对应第一个特性 protected def getPartitions: Array[Partition] 得到分区，返回的类型Array[Partition]，是一个数组或集合，数组或集合的类型是Partition 对应第三个特性 protected def getDependencies: Seq[Dependency[_]] = deps 得到一个Dependencies 对应第五个特性 protected def getPreferredLocations(split: Partition): Seq[String] = Nil 对应第四个特性 @transient val partitioner: Option[Partitioner] = None","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark中job、stage、task的划分","date":"2019-06-10T12:12:36.000Z","path":"大数据技术/Spark中job、stage、task的划分/","text":"Spark中job、stage、task的划分 参考博客： Spark_Spark 中 Stage, Job 划分依据 , Job, Stage, Task 相关概念 Spark的Job、Stage、Task是按何种规则产生的 （图中最小的方块代表一个partition，包裹partition的方块是RDD，忽略颜色） Spark的Stage是分割RDD执行的各种transformation而来。如上图，将这些转化步骤分为了3个Stage，分别为Stage1，Stage2和Stage3。这里最重要的是搞清楚分割Stage的规则，其实只有一个：从宽依赖处分割。 知道了这个分割规则，其实还是有一点疑惑，为什么这么分？ 其实道理蛮明显的，子RDD的partition会依赖父RDD中多个partition，这样就可能会有一些partition没有准备好，导致计算不能继续，所以就分开了，直到准备好了父RDD中所有partition，再继续进行将父RDD转换为子RDD的计算。而窄依赖完全不会有这个顾虑，窄依赖是父RDD一个partition对应子RDD一个partition，那么直接计算就可以了。 Job Spark的Job来源于用户执行action操作，就是从RDD中获取结果的操作，而不是将一个RDD转换成另一个RDD的transformation操作。 Stage 一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。 在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。比如： rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个； rdd.map(x =&gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。 Task task是stage下的一个任务执行单元，一般来说，一个rdd有多少个partition，就会有多少个task，因为每一个task只是处理一个partition上的数据。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Core之读写数据","date":"2019-06-06T12:12:36.000Z","path":"大数据技术/Spark Core之读写数据/","text":"Spark Core之读写数据 Spark支持多种数据源，从总体来分分为两大部分：文件系统和数据库。 文件系统 文件系统主要有本地文件系统、Amazon S3、HDFS等。 文件系统中存储的文件有多种存储格式。Spark支持的一些常见格式有： 格式名称 结构化 说明 文本文件 否 普通文本文件，每行一条记录 JSON 半结构化 常见的基于文本的半结构化数据 CSV 是 常见的基于文本的格式，在电子表格应用中使用 SequenceFiles 是 一种用于键值对数据的常见Hadoop文件格式 以文本文件为例： 1.读取 读取单个文件，参数为文件全路径，输入的每一行都会成为RDD的一个元素。 1val lines = sc.textFile(\"spark-core/data/cdnlog.txt\") 读取多个文件时，可以使用textFile将参数改为目录或以逗号文件的多个文件名。如果是小文件，也可以使用wholeTextFiles读取为一个Pair RDD（键是文件名，值是文件内容）。 1234567val input = sc.wholeTextFiles(\"spark-core/data/test.txt\")val result = input.mapValues&#123; y =&gt; &#123; val nums = y.split(\" \").map(x =&gt; x.toDouble) nums.sum / nums.size.toDouble &#125;&#125; 2.写入 输出文本文件时，可使用saveAsTextFile()方法接收一个目录，将RDD中的内容输出到目录中的多个文件中。 1result.saveAsTextFile(outputFile) 数据库 数据库主要分为关系型数据库（MySQL、PostgreSQL等）和非关系型数据库（HBase、ElasticSearch等）。Spark使用JDBC访问关系型数据库（MySQL、PostgreSQL等），只需要构建一个org.apache.spark.rdd.JdbcRDD即可。 以MySQL为例： 1.读取： 12345678910111213def readMySQL(sc: SparkContext) = &#123; val stat: JdbcRDD[(String, Int)] = new JdbcRDD(sc, () =&gt; &#123; DriverManager.getConnection(\"jdbc:mysql://hadoop000:3306/ruozedata_spark_online\", \"root\", \"123456\") &#125;, sql = \"select * from stat where cnt &gt;= ? and cnt &lt;= ?\" , lowerBound = 40, upperBound = 110, numPartitions = 1 resultSet =&gt; (resultSet.getString(1), resultSet.getInt(2)) ) println(\"count: \" + stat.count()) stat.foreach(println)&#125; 2.写入: 123456789101112131415161718192021def saveMySQL(sc: SparkContext) = &#123; val rdd = sc.parallelize(Array((\"BJ\",100),(\"SH\",10),(\"SZ\",30)),1) rdd.foreachPartition(partition =&gt; &#123; val connection = DriverManager.getConnection(\"jdbc:mysql://hadoop000:3306/ruozedata_spark_online\",\"root\",\"123456\") println(\"---------------------\") connection.setAutoCommit(false) //表名stat,列名province, cnt val pstmt = connection.prepareStatement(\"ins(province, cnt) ert into stat values (?,?)\") partition.foreach(x =&gt; &#123; pstmt.setString(1, x._1) pstmt.setInt(2, x._2) //批处理 pstmt.addBatch() &#125;) pstmt.executeBatch() connection.commit() connection.close() &#125;)&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Core基础-常用算子","date":"2019-06-04T12:12:36.000Z","path":"大数据技术/Spark Core基础-常用算子/","text":"Spark Core基础-常用算子 常用算子 map、mapPartition、mapPartitionWithIndex 1.map map对rdd中的每一个元素进行操作。 2.mapPartition mapPartitions是对rdd中的每个分区的迭代器进行操作。 mapPartitions的优点： 如果是普通的map，比如一个partition中有1万条数据。那么你的function要执行和计算1万次。使用mapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。SparkSql或DataFrame默认会对程序进行mapPartition的优化。 mapPartitions的缺点： 如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下， 比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。 所以说普通的map操作通常不会导致内存的OOM异常。 但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。 3.mapPartitionWithIndex mapPartitions则是对rdd中的每个分区的迭代器进行操作，并且可以将分区的编号取出来。 示例： 12345678910111213141516171819202122232425262728object mapPartitionsWithIndexApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val rdd1: RDD[String] = sc.parallelize(List( \"spark1\", \"spark2\", \"spark3\", \"spark4\", \"spark5\", \"spark6\", \"spark7\", \"spark8\", \"spark9\"), 3) val rdd2: RDD[String] = rdd1.mapPartitionsWithIndex &#123; (index, iter) =&gt; &#123; println() var result = List[String]() var subfix = \"\" if (index == 0) subfix = \"【北京区】\" else if (index == 1) subfix = \"【上海区】\" else subfix = \"【广州区】\" while (iter.hasNext) &#123; val str: String = iter.next() result = result :+ subfix + str &#125; result.iterator &#125; &#125; rdd2.foreach(println(_)) sc.stop() &#125;&#125; 运行结果： 12345678910111213【北京区】spark1【北京区】spark2【北京区】spark3【上海区】spark4【上海区】spark5【上海区】spark6【广州区】spark7【广州区】spark8【广州区】spark9Process finished with exit code 0 zip、zipWithIndex 1.zip zip把两个集合按元素顺序合并成元组： 12345678scala&gt; val rdd1 = sc.parallelize(List(\"pk\", \"J\", \"xingxing\"))rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(List(\"1\", \"2\", \"3\"))rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24scala&gt; rdd1.zip(rdd2).collectres4: Array[(String, String)] = Array((pk,1), (J,2), (xingxing,3)) 注意： 和scala不同,spark中必须保证: 元素数目相同 分区数相同 2.zipWithIndex 该函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对。 示例： 12345scala&gt; val rdd3 = sc.parallelize(List(\"hadoop\", \"spark\", \"flink\"))rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24scala&gt; rdd3.zipWithIndex().collectres6: Array[(String, Long)] = Array((hadoop,0), (spark,1), (flink,2)) union、distinct、intersection、substract、cartesian 创建两个rdd： 12345scala&gt; val rdd4 = sc.parallelize(List(\"a\", \"b\", \"b\",\"c\",\"c\"))rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24scala&gt; val rdd5 = sc.parallelize(List(\"c\", \"d\", \"e\"))rdd5: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[9] at parallelize at &lt;console&gt;:24 1.union 返回一个新的数据集，该数据集包含源数据集和参数中的元素的联合。（不去重） 12scala&gt; rdd4.union(rdd5).collectres7: Array[String] = Array(a, b, b, c, c, c, d, e) 2.distinct 返回包含源数据集去重后元素的新数据集。 源码： 123def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123; map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)&#125; 示例： 12scala&gt; rdd4.distinct().collectres8: Array[String] = Array(b, a, c) 3.intersection 交集 intersection(otherDataset) 返回一个新的RDD，它包含源数据集中元素和参数的交集。（去重） 12scala&gt; rdd4.intersection(rdd5).collectres9: Array[String] = Array(c) 4.substract 差集 rdd1.subtract (rdd2) 返回在rdd1中出现，但是不在rdd2中出现的元素。（不去重） 12scala&gt; rdd4.subtract(rdd5).collectres10: Array[String] = Array(b, b, a) 5.cartesian 笛卡尔 cartesian(otherDataset) 当调用类型T和U的数据集时，返回(T,U)对的数据集(所有对元素)。 12scala&gt; rdd4.cartesian(rdd5).collectres11: Array[(String, String)] = Array((a,c), (b,c), (a,d), (a,e), (b,d), (b,e), (b,c), (c,c), (c,c), (b,d), (b,e), (c,d), (c,e), (c,d), (c,e)) sortBy、sortByKey 在Spark中存在两种对RDD进行排序的函数，分别是 sortBy和sortByKey函数。sortBy是对标准的RDD进行排序，它是从Spark 0.9.0之后才引入的（可以参见SPARK-1063）。而sortByKey函数是对PairRDD进行排序，也就是有Key和Value的RDD。 1.sortBy sortBy函数函数的实现依赖于sortByKey函数，示例： 123456789101112//升序scala&gt; rdd6.sortBy(x =&gt; x).collect()res13: Array[Int] = Array(1, 3, 3, 6, 23, 46)//降序scala&gt; rdd6.sortBy(x =&gt; x, false).collect()res14: Array[Int] = Array(46, 23, 6, 3, 3, 1)//修改分区数，默认的分区个数是2，而我们对它进行了修改，所以最后变成了1。scala&gt; val result = rdd6.sortBy(x =&gt; x, false, 1)result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[41] at sortBy at &lt;console&gt;:25scala&gt; result.partitions.sizeres16: Int = 1 2.sortByKey sortByKey函数作用于Key-Value形式的RDD，并对Key进行排序。它是在org.apache.spark.rdd.OrderedRDDFunctions中实现的，实现如下： 12345678def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size) : RDD[(K, V)] =&#123; val part = new RangePartitioner(numPartitions, self, ascending) new ShuffledRDD[K, V, V](self, part) .setKeyOrdering(if (ascending) ordering else ordering.reverse)&#125; 该函数返回的RDD一定是ShuffledRDD类型的，因为对源RDD进行排序，必须进行Shuffle操作，而Shuffle操作的结果RDD就是ShuffledRDD。其实这个函数的实现很优雅，里面用到了RangePartitioner，它可以使得相应的范围Key数据分到同一个partition中，然后内部用到了mapPartitions对每个partition中的数据进行排序，而每个partition中数据的排序用到了标准的sort机制，避免了大量数据的shuffle。 groupByKey、reduceByKey 示例： 1234567891011121314151617181920object WCApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val words = Array(\"one\", \"two\", \"two\", \"three\", \"three\", \"three\") val wordsRDD = sc.parallelize(words).map(word =&gt; (word, 1)) val wordsCountWithReduce = wordsRDD .reduceByKey(_ + _) .foreach(println) val wordsCountWithGroup = wordsRDD .groupByKey() .map(x =&gt; (x._1, x._2.sum)) .foreach(println) sc.stop() &#125;&#125; 运行结果： 123456(two,2)(one,1)(three,3)(two,2)(one,1)(three,3) 虽然两个函数都能得出正确的结果， 但reduceByKey函数更适合使用在大数据集上。 这是因为它可以在每个分区移动数据之前将输出数据与一个共用的key结合（先本地聚合,再shuffle）。 groupByKey这个出来的是(key,Interator),需要对iteration的内容进行进一步的处理，当调用 groupByKey时，所有的键值对(key-value pair) 都会被移动,在网络上传输这些数据非常没必要，因此应避免使用 groupByKey。 数据量很大时，在使用 reduceByKey 和 groupByKey 时他们shuffle后的文件大小差别会被放大更多倍。 join、leftOuterJoin、rightOuterJoin、fullOuterJoin 1.join join函数输出两个RDD中相同Key的所有项(类似于SQL中的inner join)。 12345678910111213scala&gt; val a = sc.parallelize(Array((\"A\",\"a1\"),(\"B\",\"b1\"),(\"C\",\"c1\"),(\"D\",\"d1\"),(\"E\",\"e1\"),(\"F\",\"f1\")))a: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 scala&gt; val b = sc.parallelize(Array((\"A\",\"a2\"),(\"B\",\"b2\"),(\"C\",\"c1\"),(\"C\",\"c2\"),(\"C\",\"c3\"),(\"E\",\"e2\")))b: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24 scala&gt; a.join(b).collect // 这里的join是inner join，只返回左右都匹配上的内容 res1: Array[(String, (String, String))] = Array((B,(b1,b2)), (A,(a1,a2)), (C,(c1,c1)), (C,(c1,c2)), (C,(c1,c3)), (E,(e1,e2))) scala&gt; b.join(a).collect res2: Array[(String, (String, String))] = Array((B,(b2,b1)), (A,(a2,a1)), (C,(c1,c1)), (C,(c2,c1)), (C,(c3,c1)), (E,(e2,e1))) 2.leftOuterJoin 是以左边为基准，会保留对象的所有key 左边（a）的记录一定会存在，右边（b）的记录有的返回Some(x)，没有的补None。 12345scala&gt; a.leftOuterJoin(b).collectres3: Array[(String, (String, Option[String]))] = Array((B,(b1,Some(b2))), (F,(f1,None)), (D,(d1,None)), (A,(a1,Some(a2))), (C,(c1,Some(c1))), (C,(c1,Some(c2))), (C,(c1,Some(c3))), (E,(e1,Some(e2)))) scala&gt; b.leftOuterJoin(a).collectres5: Array[(String, (String, Option[String]))] = Array((B,(b2,Some(b1))), (A,(a2,Some(a1))), (C,(c1,Some(c1))), (C,(c2,Some(c1))), (C,(c3,Some(c1))), (E,(e2,Some(e1)))) 3.rightOuterJoin 是以右边为基准，保留参数对象的所有key 右边（b）的记录一定会存在，左边（a）的记录有的返回Some(x)，没有的补None。 12345scala&gt; a.rightOuterJoin(b).collectres4: Array[(String, (Option[String], String))] = Array((B,(Some(b1),b2)), (A,(Some(a1),a2)), (C,(Some(c1),c1)), (C,(Some(c1),c2)), (C,(Some(c1),c3)), (E,(Some(e1),e2))) scala&gt; b.rightOuterJoin(a).collectres6: Array[(String, (Option[String], String))] = Array((B,(Some(b2),b1)), (F,(None,f1)), (D,(None,d1)), (A,(Some(a2),a1)), (C,(Some(c1),c1)), (C,(Some(c2),c1)), (C,(Some(c3),c1)), (E,(Some(e2),e1))) 4.fullOuterJoin 保留两个RDD中的所有key 所有的值列都可能出现缺失的情况，所有所有值列都转换为Option对象 123456789101112scala&gt; val a = sc.parallelize(Array((\"A\",\"a1\"),(\"B\",\"b1\"),(\"C\",\"c1\"),(\"D\",\"d1\"),(\"E\",\"e1\"),(\"F\",\"f1\")))a: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[49] at parallelize at &lt;console&gt;:24 scala&gt; val b = sc.parallelize(Array((\"A\",\"a2\"),(\"B\",\"b2\"),(\"C\",\"c1\"),(\"C\",\"c2\"),(\"C\",\"c3\"),(\"E\",\"e2\")))b: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[50] at parallelize at &lt;console&gt;:24 scala&gt; a.fullOuterJoin(b).collectres15: Array[(String, (Option[String], Option[String]))] = Array((B,(Some(b1),Some(b2))), (F,(Some(f1),None)), (D,(Some(d1),None)), (A,(Some(a1),Some(a2))), (C,(Some(c1),Some(c1))), (C,(Some(c1),Some(c2))), (C,(Some(c1),Some(c3))), (E,(Some(e1),Some(e2)))) scala&gt; b.fullOuterJoin(a).collectres16: Array[(String, (Option[String], Option[String]))] = Array((B,(Some(b2),Some(b1))), (F,(None,Some(f1))), (D,(None,Some(d1))), (A,(Some(a2),Some(a1))), (C,(Some(c1),Some(c1))), (C,(Some(c2),Some(c1))), (C,(Some(c3),Some(c1))), (E,(Some(e2),Some(e1)))) coalesce、repartition repartition(numPartitions:Int):RDD[T]和coalesce(numPartitions:Int，shuffle:Boolean=false):RDD[T]， 他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区）： 如果N&lt;M，一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。 如果N&gt;M并且N和M相差不多，(假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false，在shuffl为false的情况下，如果M&gt;N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。 如果N&gt;M并且两者相差悬殊，这时如果将shuffle设置为false，父子RDD是窄依赖关系，他们同处在一个Stage中，就可能造成spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。 总之：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的分区数变多的。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Scala模式匹配","date":"2019-05-29T12:12:36.000Z","path":"Scala编程语言/Scala模式匹配/","text":"Scala模式匹配 Scala模式匹配 scala提供了一个非常强大的模式匹配机制,那什么是模式匹配呢?模式匹配是检查某个值（value）是否匹配某一个模式的机制，一个成功的匹配同时会将匹配值解构为其组成部分。它是Java中的switch语句的升级版，同样可以用于替代一系列的 if/else 语句。 字符串 123456789101112object CaseDemo01 &#123; def main(args: Array[String]): Unit = &#123; val arr = Array(\"hadoop\", \"zookeeper\", \"spark\", \"pk\") val name = arr(Random.nextInt(arr.length)) name match &#123; case \"hadoop\" =&gt; println(\" 大数据分布式存储和计算框架 ...\") case \"zookeeper\" =&gt; println(\" 大数据分布式协调服务框架 ...\") case \"spark\" =&gt; println(\" 大数据分布式内存计算框架 ...\") case _ =&gt; println(\" 我不认识你 ...\") &#125; &#125;&#125; 类型 123456789101112131415161718192021object CaseDemo02 extends MatchType &#123; def main(args: Array[String]): Unit = &#123; val arr = Array(\"xingxing\", 123, 4.23, true, CaseDemo01) val name = arr(Random.nextInt(arr.length)) println(\"name: \" + name) name match &#123; case str: String =&gt; println(s\"match String $str\") case int: Int =&gt; println(s\"match Int $int\") case double: Double =&gt; println(s\"match Double $double\") case boolean: Boolean =&gt; println(s\"match Boolean $boolean\") case matchType: MatchType =&gt; println(s\"match MatchType $matchType\") case _: Any =&gt; println(\"nothing\") &#125; &#125;&#125;class MatchType &#123;&#125; 数组、元组、集合 123456789101112131415161718192021222324252627282930313233343536373839object CaseDemo03 &#123; def main(args: Array[String]): Unit = &#123; /* * 匹配数组 */ val arr = Array(1, 2, 3, 4) arr match &#123; case Array(1, x, y, z) =&gt; println(s\"case: $x,$y,$z\") case Array(_, x, y) =&gt; println(s\"case: $x,$y\") case _ =&gt; println(\"nothing\") &#125; /* * 匹配元组 * 注意：元组匹配中的时候，元组内的个数是确定的 */ val tup = (1, 4, 6, 8) tup match &#123; case (1, x, y, z) =&gt; println(s\"case: $x,$y,$z\") case (4, w, s, t) =&gt; println(s\"case: $w,$s,$t\") case _ =&gt; println(\"nothing\") &#125; /* * 匹配集合 * 注意： :: 从右往左向队列的头部追加数据，创造新的列表。 * 用法为 x::list,其中x为加入到头部的元素； * Nil表示空的集合； * case1表示是一个空的list集合+0 就是只有一个0的list集合。 */ val list = List(0, 1, 3, 5, 6) list match &#123; case 0 :: Nil =&gt; println(\"case1:0\") case a :: b :: c :: d :: e :: Nil =&gt; println(s\"case2 : $a,$b,$c,$d,$e\") case 0 :: b :: Nil =&gt; println(s\"case3: $b\") case List(0, 1, 3, 5, 6) =&gt; println(\"这个也可以匹配到\") case _ =&gt; println(\"not found\") &#125; &#125;&#125; 样例类（case class） 123456789101112131415161718/** * 样例类 * 注意：在模式匹配case中，需要将参数设定好 */object CaseDemo04 &#123; def main(args: Array[String]): Unit = &#123; val arr = Array(HertBeat(2000L), SubmitTask(\"1000\", \"提交\"), CheckTimeOutTask) arr(Random.nextInt(arr.length)) match &#123; case HertBeat(time) =&gt; println(\"HertBeat\") case SubmitTask(id, task) =&gt; println(\"submitTask\") case CheckTimeOutTask =&gt; println(\"CheckTimeOutTask\") &#125; &#125;&#125;case class HertBeat(time: Long)case class SubmitTask(id: String, task: String)case class CheckTimeOutTask() Option 类型 在 scala 中 Option 类型用样例类来表示可能存在或者可能不存在的值(Option 的子类有Some 和 None)。Some 包装了某个值，None 表示没有值。 12345678910111213object CaseDemo05 &#123; def main(args: Array[String]) &#123; val map = Map(\"a\" -&gt; 1, \"b\" -&gt; 2) val v = map.get(\"b\") match &#123; case Some(i) =&gt; i case None =&gt; 0 &#125; println(v) //更好的方式 val v1 = map.getOrElse(\"c\", 0) println(v1) &#125;&#125; 偏函数 被包在花括号内没有 match 的一组 case 语句是一个偏函数，它是 PartialFunction[A, B]的一个实例，A 代表输入参数类型，B 代表返回结果类型，常用作输入模式匹配，偏函数最大的特点就是它只接受和处理其参数定义域的一个子集。 123456789101112131415161718object CaseDemo06 &#123; val func1: PartialFunction[String, Int] = &#123; case \"one\" =&gt; 1 case \"two\" =&gt; 2 case _ =&gt; -1 &#125; def func2(num: String): Int = num match &#123; case \"one\" =&gt; 1 case \"two\" =&gt; 2 case _ =&gt; -1 &#125; def main(args: Array[String]) &#123; println(func1(\"one\")) println(func2(\"one\")) &#125;&#125;","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala隐式转换、隐式参数与隐式类 ","date":"2019-05-27T02:15:18.000Z","path":"Scala编程语言/7.scala隐式转换/","text":"Scala隐式转换、隐式参数与隐式类 隐式转换 什么是隐式转换 隐式转换将某种类型的对象转换成其他类型的对象，其最核心的就是定义隐式转换函数，即implicit conversion function。定义的隐式转换函数，只要在编写的程序内引入，就会被Scala自动使用。Scala会根据隐式转换函数的签名，在程序中使用到隐式转换函数接收的参数类型定义的对象时，会自动将其传入隐式转换函数，转换为另外一种类型的对象并返回。这就是“隐式转换”。 隐式转换函数通常不会由用户手动调用，而是由Scala进行调用。因此通常建议将隐式转换函数的名称命名为“one2one”的形式。 举个例子：比如你去买票的时候，有正常的窗口和特殊人群窗口（学生，军人，残疾人…）,你定义了学生的这个类，创建学生的对象，但是没用啊！人家特殊窗口接收的参数只是特殊人群，你怎么证明你是特殊人群呢-----所以你就要拿着你学生证（名字），工作人员对照后发现你属于特殊人群，他就给你开了个证明，你就是特殊人群，这个时候你就可以去买票了 案例一：特殊售票窗口（只接受特殊人群，比如学生、老人等） 我们来看案例一：你会发现在隐式转换中先判断你的类是属于什么( if (obj.getClass == classOf[Student]))，然后证明（ val stu = obj.asInstanceOf[Student），最后把你列入特殊人群（new SpecialPerson(stu.name) ）。 123456789101112131415class SpecialPerson(val name: String)class Student(val name: String)class Older(val name: String)implicit def object2SpecialPerson (obj: Object): SpecialPerson = &#123; if (obj.getClass == classOf[Student]) &#123; val stu = obj.asInstanceOf[Student]; new SpecialPerson(stu.name) &#125; else if (obj.getClass == classOf[Older]) &#123; val older = obj.asInstanceOf[Older]; new SpecialPerson(older.name) &#125; else Nil&#125;var ticketNumber = 0def buySpecialTicket(p: SpecialPerson) = &#123; ticketNumber += 1 \"T-\" + ticketNumber&#125; 正常情况下，buySpecialTicket方法接受不了Student和Older类型的对象，如果想这样做，就要用隐式转换，将这两种类型的对象转换成SpecialPerson类型（内部创建SpecialPerson的对象） 案例二：让 File 类具备 RichFile 类中的 read 方法 123456789101112131415161718192021222324package cn.cheng.implic_demoimport java.io.Fileimport scala.io.Sourceobject Implic&#123;//定义隐式转换方法 implicit def file2RichFile(file: File)=new RichFile(file)&#125;class RichFile(val f:File) &#123; def read()=Source.fromFile(f).mkString&#125;//使用import导入隐式转换方法import Implic._object RichFile&#123; def main(args: Array[String]) &#123; val f=new File(\"E://text.txt\")//通过隐式转换，让File类具备了RichFile类中的方法 val content=f.read() println(content) &#125;&#125; 隐式转换作用域 Scala默认使用两种隐式转换，一种是源类型，或者目标类型的伴生对象内的隐式转换函数；一种是当前程序作用域内的可以用唯一标识符表示的隐式转换函数。 如果两种都没有还可以手动导入导入：使用import语法引入某个包下的隐式转换函数，比如import implic._ 隐转换发生时机 什么时候会进行隐式转换呢，分为三种情况 1、调用某个函数，但是给函数传入的参数的类型，与函数定义的接收参数类型不匹配（案例：特殊售票窗口） 2、使用某个类型的对象，调用某个方法，而这个方法并不存在于该类型时（案例：超人变身） 3、使用某个类型的对象，调用某个方法，虽然该类型有这个方法，但是给方法传入的参数类型，与方法定义的接收参数的类型不匹配 案例三：超人变身 123456789class Man(val name: String)class Superman(val name: String) &#123; def emitLaser = println(\"emit a laster!\")&#125;implicit def man2superman(man: Man): Superman = new Superman(man.name)val leo = new Man(\"leo\")leo.emitLaser 普通人没有办法用镭射眼（ emitLaser方法），但是进行隐式转换后，Man==&gt;Superman，就可以调用了 隐式参数 所谓的隐式参数，指的是在函数或者方法中，定义一个用implicit修饰的参数，此时Scala会尝试找到一个指定类型的，用implicit修饰的对象，即隐式值，并注入参数。 Scala会在两个范围内查找：一种是当前作用域内可见的val或var定义的隐式变量；一种是隐式参数类型的伴生对象内的隐式值 案例一：考试签到 考生到考场签到，所有人共用一支笔SignPen,这个时候就可以把&quot;笔&quot;变量，定义为隐式的 123456789101112131415object Text &#123; class SignPen&#123; def write(content:String) = println(content) &#125; implicit val pen = new SignPen def signForExam(name:String) (implicit pen: SignPen)&#123; pen.write(name + \" come to exam in time.\") &#125; def main(args: Array[String]): Unit = &#123; signForExam(\"pk\") &#125;&#125; 在调用的时候，你只需要==&gt;signForExam(“pk”)调用,scala就会自动去找隐式的SignPen对象，这样就可以实现调用了 案例二： 12345678910111213object ImplicitParameterDemo &#123; def testParam(implicit name: String) &#123; println(name) &#125; implicit val name = \"lujinhong\" def main(args: Array[String]) &#123; testParam(\"My name\") testParam &#125;&#125; 上述示例中将testParam中的参数name设为了implicit，因此当name未赋值时，它会寻找作用域内叫做name的隐式参数所定义的值。有几个注意事项： 函数及隐式参数都必须定义为implicit。 也可以显式定义函数，如本例中的”My name”，此时隐式参数不生效。 隐式类 案例一：作用于Int 12345678910object ImplicitClassDemo &#123; def main(args: Array[String]) &#123; println(2.addOne(1)) println(2.addOne(3)) &#125; implicit class Caculator(x: Int) &#123; def addOne(a: Int): Int = a + 1 &#125;&#125; 隐式类定义中的(x: Int) ，指定了当Int这种类型可以在这个隐式类中寻找方法，即这个类中定义的所有方法都会成为Int的隐式函数。 案例二：作用于String 12345678910object ImplicitClassDemo &#123; def main(args: Array[String]) &#123; println(\"d\".addOne(1)) println(\"kk\".addOne(3)) &#125; implicit class Caculator(x: String) &#123; def addOne(a: Int): Int = a + 1 &#125;&#125;","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala常用高阶函数(二）","date":"2019-05-22T01:15:46.000Z","path":"Scala编程语言/6.scala常用高阶函数（二）/","text":"Scala常用高阶函数（二） 柯里化 柯里化(Currying)指的是把原来接受多个参数的函数变换成接受一个参数的函数过程=&gt;返回值是一个函数，参数就是其余参数，而且第一个参数的值会累计在此函数中。 定义一个函数： 123456789101112//mutiplyBy这个函数的返回值是一个函数//该函数的输入是Doulbe，返回值也是Doublescala&gt; def multiplyBy(factor:Double)=(x:Double)=&gt;factor*xmultiplyBy: (factor: Double)Double =&gt; Double//返回的函数作为值函数赋值给变量xscala&gt; val x=multiplyBy(10)x: Double =&gt; Double = &lt;function1&gt; //变量x现在可以直接当函数使用scala&gt; x(50)res10: Double = 500.0 上述代码可以像这样使用： 12345scala&gt; def multiplyBy(factor:Double)=(x:Double)=&gt;factor*xmultiplyBy: (factor: Double)Double =&gt; Double //这是高阶函数调用的另外一种形式scala&gt; multiplyBy(10)(50) 那函数柯里化(curry）是怎么样的呢？其实就是将multiplyBy函数定义成如下形式 123scala&gt; def multiplyBy(factor:Double)(x:Double)=x*factormultiplyBy: (factor: Double)(x: Double)Double 即通过(factor:Double)(x:Double)定义函数参数，该函数的调用方式如下： 12345678910//柯里化的函数调用方式scala&gt; multiplyBy(10)(50)res11: Double = 500.0 //但此时它不能像def multiplyBy(factor:Double)=(x:Double)=&gt;factor*x函数一样，可以输入单个参数进行调用scala&gt; multiplyBy(10) &lt;console&gt;:10: error: missing arguments for method multiplyBy;follow this method with `_' if you want to treat it as a partially applied function multiplyBy(10) 错误提示函数multiplyBy缺少参数，如果要这么做的话，需要将其定义为偏函数 12scala&gt; multiplyBy(10)_res12: Double =&gt; Double = &lt;function1&gt; 那现在我们接着对偏函数进行介绍 偏函数 如果你想定义一个函数，而让它只接受和处理其参数定义域范围内的子集，对于这个参数范围外的参数则抛出异常，这样的函数就是偏函数（顾名思异就是这个函数只处理传入来的部分参数）。 需求： 给定List(1,2,3,4,“test”)，里面的每个Int元素+1 1234567891011121314val l = List(1,2,3,4,\"test\")//元素+1 ==&gt; 返回List(2,3,4,5)def f1(n:Any) = &#123; n.isInstanceOf[Int]&#125;def f2(n:Any)= &#123; n.asInstanceOf[Int]&#125;def f3(n:Int) = &#123; n + 1&#125;println(l.filter(f1).map(f2).map(f3)运行结果：List(2, 3, 4, 5) 使用偏函数实现： 123456789101112val addOne = new PartialFunction[Any,Int] &#123; override def isDefinedAt(x: Any): Boolean = if(x.isInstanceOf[Int])&#123; true &#125;else&#123; false &#125; override def apply(v1: Any): Int = v1.asInstanceOf[Int] + 1&#125;println(l.collect(addOne)运行结果：List(2, 3, 4, 5) 简化代码： 1234def f4:PartialFunction[Any,Int] = &#123;case i:Int =&gt; i + 1&#125; l.collect(f4) 直接调用： 1234567891011scala&gt; val l = List(1,2,3,4,\"test\")l: List[Any] = List(1, 2, 3, 4, test)scala&gt; l.collect(&#123; case i:Int =&gt; i + 1 &#125;)res13: List[Int] = List(2, 3, 4, 5)//最简化的版本：scala&gt; l.collect&#123; case i:Int =&gt; i + 1 &#125;res14: List[Int] = List(2, 3, 4, 5)","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala常用高阶函数 ","date":"2019-05-21T02:06:09.000Z","path":"Scala编程语言/5.scala常用高阶函数/","text":"Scala常用高阶函数 高阶函数介绍 MapReduce中仅仅只提供了两个函数map、reduce，而spark中提供非常多的函数供调用，更加人性化，开发效率更高。 map map：一一映射，对里面的每一个元素都作用一个function 123val l = List(1,2,3,4,5,6,7,8) println(l.map((x:Int) =&gt; x*2)) //每个元素都乘以2精简表示：println(l.map(_*2)) 1234scala&gt; val list = List(Array((\"hello\",3)),Array((\"world\",2)))list: List[Array[(String, Int)]] = List(Array((hello,3)), Array((world,2)))scala&gt; list.map(x =&gt; x.map(t =&gt; (t._1,t._2+2))) //每个tuple的第二个元素都加上2res1: List[Array[(String, Int)]] = List(Array((hello,5)), Array((world,4))) filter 123456scala&gt; val l = List(1,2,3,4,5,6,7,8)l: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8)scala&gt; l.filter(_ &gt; 6)res3: List[Int] = List(7, 8)scala&gt; l.map(_ * 2).filter(_ &gt; 8) //链式编程res4: List[Int] = List(10, 12, 14, 16) reduce 12345678//两两相加println(l.reduce((x:Int,y:Int) =&gt; x+y))简写如下：println(l.reduce( _ + _ ))//两两相减println(l.reduce((x:Int,y:Int) =&gt; x-y))简写如下:println(l.reduce(-)) reduceright 1234567891011121314151617//测试它的运行过程:val l = List(1,2,3,4,5,6,7,8)l.reduceRight((x:Int,y:Int) =&gt;&#123;println(x + “,” + y)x - y&#125;)结果：7,86,-15,74,-23,62,-31,5//运行结果scala&gt; l.reduceRight(_ - _)res2: Int = -4 fold 123456789101112131415161718//测试它的运行过程val l = List(1,2,3,4,5,6,7,8)l.fold(10)((x:Int,y:Int) =&gt;&#123;println(x + \":\" + y)x - y&#125;)结果：10:19:27:34:40:5-5:6-11:7-18:8运行结果：scala&gt; l.fold(10)(_ - _)res3: Int = -26 flatten flatten的作用是把输出拉平。 1234scala&gt; val a = List(List(1,2),List(3,4),List(5,6))a: List[List[Int]] = List(List(1, 2), List(3, 4), List(5, 6))scala&gt; a.flattenres4: List[Int] = List(1, 2, 3, 4, 5, 6) flatMap flatMap = map + flatten 123456789scala&gt; val a = List(List(1,2,3),List(3,4),List(4,5,6))a: List[List[Int]] = List(List(1, 2, 3), List(3, 4), List(4, 5, 6))scala&gt; val b = a.flattenb: List[Int] = List(1, 2, 3, 3, 4, 4, 5, 6)scala&gt; b.map(_*2)res5: List[Int] = List(2, 4, 6, 6, 8, 8, 10, 12)等价于==&gt;scala&gt; a.flatMap(_.map(_*2))res6: List[Int] = List(2, 4, 6, 6, 8, 8, 10, 12) groupBy 分组 123456scala&gt; val array = Array((\"a\",100),(\"b\",10),(\"a\",10),(\"d\",10))array: Array[(String, Int)] = Array((a,100), (b,10), (a,10), (d,10))scala&gt; array.groupBy(x =&gt; x._1)res7: scala.collection.immutable.Map[String,Array[(String, Int)]] = Map(b -&gt; Array((b,10)), d -&gt; Array((d,10)), a -&gt; Array((a,100), (a,10)))scala&gt; array.groupBy(x =&gt; x._2)res8: scala.collection.immutable.Map[Int,Array[(String, Int)]] = Map(100 -&gt; Array((a,100)), 10 -&gt; Array((b,10), (a,10), (d,10))) find 123456scala&gt; val array = Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; array.find(x =&gt; x &gt; 3)res9: Option[Int] = Some(4)scala&gt; array.find(x =&gt; x &gt; 30)res10: Option[Int] = None 词频统计实现 测试步骤： 1234567891011121314scala&gt; val list = List(\"hello world welcome\",\"hello world\")list: List[String] = List(hello world welcome, hello world)scala&gt; val words: List[String] = list.flatMap(_.split(\" \"))words: List[String] = List(hello, world, welcome, hello, world)scala&gt; val wordWithOne: List[(String, Int)] = words.map(x =&gt; (x, 1))wordWithOne: List[(String, Int)] = List((hello,1), (world,1), (welcome,1), (hello,1), (world,1))scala&gt; val groupByData: Map[String, List[(String, Int)]] = wordWithOne.groupBy(x =&gt; x._1)groupByData: Map[String,List[(String, Int)]] = Map(welcome -&gt; List((welcome,1)), world -&gt; List((world,1), (world,1)), hello -&gt; List((hello,1), (hello,1)))scala&gt; val result: Map[String, Int] = groupByData.mapValues(x =&gt; x.map(t =&gt; t._2).sum)result: Map[String,Int] = Map(welcome -&gt; 1, world -&gt; 2, hello -&gt; 2)scala&gt; result.toList.sortBy(_._2)res11: List[(String, Int)] = List((welcome,1), (world,2), (hello,2))scala&gt; result.toList.sortBy(-_._2)res12: List[(String, Int)] = List((world,2), (hello,2), (welcome,1)) 词频统计实现： 12345678910val list = List(\"hello world welcome\",\"hello world\")val words: List[String] = list.flatMap(_.split(\" \"))val wordWithOne: List[(String, Int)] = words.map(x =&gt; (x, 1))val groupByData: Map[String, List[(String, Int)]] = wordWithOne.groupBy(x =&gt; x._1)val result: Map[String, Int] = groupByData.mapValues(x =&gt; x.map(t =&gt; t._2).sum)println(result.toList.sortBy(_._2)) //map ==&gt;list,默认升序println(result.toList.sortBy(-_._2)) //加-改成降序 运行结果：List((welcome,1), (world,2), (hello,2))List((world,2), (hello,2), (welcome,1))","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala基础04","date":"2019-05-20T14:12:36.000Z","path":"Scala编程语言/Scala基础04/","text":"Scala基础04 集合 概述 scala提供了一套很好的集合实现，提供了一些集合类型的抽象。 scala 集合分为可变的和不可变的集合。可变集合可以在适当的地方被更新或扩展。这意味着你可以修改，添加，移除一个集合的元素。而不可变集合类永远不会改变。不过，你仍然可以模拟添加，移除或更新操作。但是这些操作将在每一种情况下都返回一个新的集合，同时使原来的集合不发生改变。 集合中基本结构： 根据scala集合体系图可以很清晰地看出scala中的集合类可以分为三大类： 1.Seq，是一组有序的元素。 2.Set，是一组没有重复元素的集合。 3.Map，是一组k-v对。 Seq Seq主要由两部分组成：IndexedSeq与LinearSeq。现在我们简单看下这两种类型。 首先看IndexedSeq，很容易看出来这种类型的主要访问方式是通过索引，默认的实现方式为vector。 12345678def test() = &#123; val x = IndexedSeq(1,2,3) println(x.getClass) println(x(0)) val y = Range(1, 5) println(y)&#125; 将以上函数运行起来以后，输出如下： 123class scala.collection.immutable.Vector1Range(1, 2, 3, 4) 而作为LinearSeq，主要的区别在于其被分为头与尾两部分。其中，头是容器内的第一个元素，尾是除了头元素以外剩余的其他所有元素。LinearSeq默认的实现是List。 12345678def test() = &#123; val x = collection.immutable.LinearSeq(\"a\", \"b\", \"c\") val head = x.head println(s\"head is: $head\") val y = x.tail println(s\"tail of y is: $y\")&#125; 将上面的代码运行起来以后，得到的结果如下： 12head is: atail of y is: List(b, c) Set 与其他任何一种编程语言一样，Scala中的Set集合类具有如下特点： 1.不存在有重复的元素。 2.集合中的元素是无序的。换句话说，不能以索引的方式访问集合中的元素。 3.判断某一个元素在集合中比Seq类型的集合要快。 Scala中的集合分为可变与不可变两种，对于Set类型自然也是如此。先来看看示例代码： 1234567891011121314def test() = &#123; def test() = &#123; val x = immutable.HashSet[String](\"a\",\"c\",\"b\") //x.add(\"d\")无法使用，因为是不可变集合，没有add方法。 val y = x + \"d\" + \"f\" // 增加新的元素，生成一个新的集合 val z = y - \"a\" // 删除一个元素，生成一个新的集合 val a = Set(1,2,3) val b = Set(1,4,5) val c = a ++ b // 生成一个新的集合，增加集合 val d = a -- b // 生成一个新的集合，去除集合 val e = a &amp; b // 与操作 val f = a | b // 或操作 &#125; 因为上面代码里的集合类型都是不可变类型，所以所有语句结果其实都是生成一个新的集合。 12345678910111213def test() = &#123; val x = new mutable.HashSet[String]() x += \"a\" // 添加一个新的元素。注意此时没有生成一个新的集合 x.add(\"d\") //因为是可变集合，所以有add方法 x ++= Set(\"b\", \"c\") // 添加一个新的集合 x.foreach(each =&gt; println(each)) x -= \"b\" // 删除一个元素 println() x.foreach(each =&gt; println(each)) println() val flag = x.contains(\"a\") // 是否包含元素 println(flag)&#125; 将上面这段代码运行起来以后，得到的结果如下： 12345678910cdab cda true Map Map这种数据结构是日常开发中使用非常频繁的一种数据结构。Map作为一个存储键值对的容器（key－value），其中key值必须是唯一的。 默认情况下，我们可以通过Map直接创建一个不可变的Map容器对象，这时候容器中的内容是不能改变的。示例代码如下。 12345678910111213def test() = &#123; val peoples = Map(\"john\" -&gt; 19, \"Tracy\" -&gt; 18, \"Lily\" -&gt; 20) //不可变 // people.put(\"lucy\",15) 会出错，因为是不可变集合。 //遍历方式1 for(p &lt;- peoples) &#123; print(p + \" \") // (john,19) (Tracy,18) (Lily,20) &#125; //遍历方式2 peoples.foreach(x =&gt; &#123;val (k, v) = x; print(k + \":\" + v + \" \")&#125;) //john:19 Tracy:18 Lily:20 //遍历方式3 peoples.foreach (&#123; case(k, v) =&gt; print(s\"key: $k, value: $v \")&#125;) //key: john, value: 19 key: Tracy, value: 18 key: Lily, value: 20&#125; 上面代码中的hashMap是不可变类型。 如果要使用可变类型的map，可以使用mutable包中的map相关类。 123456789101112131415def test() = &#123; val map = new mutable.HashMap[String, Int]() map.put(\"john\", 19) // 因为是可变集合，所以可以put map.put(\"Tracy\", 18) map.contains(\"Lily\") //false val res = getSome(map.get(\"john\")) println(res) //Some(19)&#125; def getSome(x:Option[Int]) : Any = &#123; x match &#123; case Some(s) =&gt; s case None =&gt; \"None\" &#125;&#125; 泛型 泛型 1.概述 泛型用于指定方法或类可以接受任意类型参数，参数在实际使用时才被确定，泛型可以有效地增强程序的适用性，使用泛型可以使得类或方法具有更强的通用性。泛型的典型应用场景是集合及集合中的方法参数，可以说同java一样，scala中泛型无处不在，具体可以查看scala的api。 2.泛型类、泛型方法 scala中的泛型主要包括泛型类和泛型方法 泛型类：指定类可以接受任意类型参数。 泛型方法：指定方法可以接受任意类型参数。 3.示例： 1）定义泛型类 1234567891011121314151617181920212223242526/** * 类上定义的泛型，只要是Comparable就可以传递 */class GenericTest1[T &lt;: Comparable[T]] &#123; def choose(one:T,two:T): T =&#123; //定义一个选择的方法 if(one.compareTo(two) &gt; 0) one else two &#125; &#125; class Boy(val name:String,var age:Int) extends Comparable[Boy]&#123; override def compareTo(o: Boy): Int = &#123; this.age - o.age &#125;&#125; object GenericTestOne&#123; def main(args: Array[String]): Unit = &#123; val gt = new GenericTest1[Boy] val pk = new Boy(\"pk\",60) val xingxing= new Boy(\"xingxing\",66) val boy = gt.choose(pk,xingxing) println(boy.name) &#125;&#125; 运行结果： 123xingxingProcess finished with exit code 0 2）定义泛型方法 123456789101112131415161718192021222324class GenericTest2&#123; //在方法上定义泛型 def choose[T &lt;: Comparable[T]](one:T,two:T): T =&#123; if(one.compareTo(two) &gt; 0) one else two &#125;&#125; class Boy(val name:String,var age:Int) extends Comparable[Boy]&#123; override def compareTo(o: Boy): Int = &#123; this.age - o.age &#125;&#125; object GenericTestTwo&#123; def main(args: Array[String]): Unit = &#123; val gt = new GenericTest2 val pk = new Boy(\"pk\",60) val xingxing = new Boy(\"xingxing\",66) val boy = gt.choose(pk,xingxing) println(boy) &#125;&#125; 运行结果: 123xingxingProcess finished with exit code 0 上界和下界 1.概述 在指定泛型类型时，有时需要界定泛型类型的范围，而不是接收任意类型。比如，要求某个泛型类型，必须是某个类的子类，这样在程序中就可以放心的调用父类的方法，程序才能正常的使用与运行。此时，就可以使用上下边界Bounds的特性。 Scala的上下边界特性允许泛型类型是某个类的子类，或者是某个类的父类。 1）S &lt;: T 这是类型上界的定义，也就是S必须是类型T的子类（或本身，自己也可以认为是自己的子类)。 2）U &gt;: T 这是类型下界的定义，也就是U必须是类型T的父类(或本身，自己也可以认为是自己的父类)。 2.示例 1）上界示例 参考上面的泛型方法 2）下界示例 123456789101112131415161718class GranderFatherclass Father extends GranderFatherclass Son extends Fatherclass Tongxue object Card&#123; def getIDCard[T &gt;: Son](person:T): Unit =&#123; println(\"OK,交给你了\") &#125; def main(args: Array[String]): Unit = &#123; getIDCard[GranderFather](new Father) getIDCard[GranderFather](new GranderFather) getIDCard[GranderFather](new Son) //getIDCard[GranderFather](new Tongxue) //Tongxue不是Son的父类，运行报错，注释掉 &#125;&#125; 运行结果： 12345OK,交给你了OK,交给你了OK,交给你了Process finished with exit code 0 协变和逆变 Scala中协变和逆变主要作用是用来解决参数化类型的泛化问题。由于参数化类型的参数（参数类型）是可变的，当两个参数化类型的参数是继承关系（可泛化），那被参数化的类型是否也可以泛化呢？在Java中这种情况下是不可泛化的，然而Scala提供了三个选择，即协变、逆变和非变，解决了参数化类型的泛化问题。 对于一个带类型参数的类型，比如 List[T]： 如果对A及其子类型B，满足 List[B]也符合 List[A]的子类型，那么就称为covariance(协变)； 如果 List[A]是 List[B]的子类型，即与原来的父子关系正相反，则称为contravariance(逆变)。 协变 12345678910____ _____________ | | | || A | | List[ A ] ||_____| |_____________| ^ ^ | | _____ _____________ | | | || B | | List[ B ] ||_____| |_____________| 逆变 12345678910___ _____________ | | | || A | | List[ B ] ||_____| |_____________| ^ ^ | | _____ _____________ | | | || B | | List[ A ] ||_____| |_____________| 协变和逆变使用“+”,“-”差异标记。当我们定义一个协变类型List[+A]时，List[Child]可以是List[Parent]的子类型，当我们定义一个逆变类型List[-A]时，List[Child]可以是List[Parent]的父类型 假设有参数化特质List，那么可以有三种定义。如下所示： (1) trait List [T]{} 非变。这种情况下，当类型S是类型A的子类型，则List [S]不可以认为是List [A]的子类型或父类型，这种情况和Java是一样的。 (2) trait List [+T]{} 协变。如果S extends A (S为子类型，A为父类型)，则List [S]为子类型，List [A]为父类型S &lt;: A =&gt; List [S] &lt;: List [A]。 (3) trait List [-T]{} 逆变。如果S extends A (S为子类型，A为父类型)，则List [S]为父类型，List [A]为子类型，和协变互逆S &lt;: A =&gt; Queue[S] &gt;: Queue[A]。 那么，在Scala中如何定义协变逆变类呢，举例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960object CovariantAndContravariantDemo &#123; def main(args: Array[String]): Unit = &#123; //不变 def inv1: Invariant[Tiger] = new Invariant[Tiger]() //不可以赋值，编译器编译不通过 def inv2: Invariant[Cat] = inv1 //协变 def cov1: Covariant[Tiger] = new Covariant[Tiger]() //可以直接赋值 def cov2: Covariant[Cat] = cov1 //逆变 def cont1: Contravariant[Cat] = new Contravariant[Cat]() //可以直接赋值 def cont2: Contravariant[Tiger] = cont1 &#125; /** * 定义一个不可变的类 */ class Invariant[T] &#123;&#125; /** * 定义一个协变的类 */ class Covariant[+T] &#123;&#125; /** * 定义一个逆变的类 */ class Contravariant[-T] &#123;&#125; /** * 定义一个动物的类，具有行为（吃） */ class Animal &#123; def eat() &#123; println(\"Animal likeeat botany.\") &#125; &#125; /** * 定义动物（猫）的类，具有行为（吃） */ class Cat extends Animal &#123; override def eat() &#123; println(\"Cat eatfish.\") &#125; &#125; /** * 定义动物（老虎）的类，具有行为（吃） */ class Tiger extends Cat &#123; override def eat() &#123; println(\"Tiger eatmeat.\") &#125; &#125;&#125;","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala基础03","date":"2019-05-18T14:25:36.000Z","path":"Scala编程语言/Scala基础03/","text":"Scala基础03 伴生类和伴生对象 Object 示例： 1234567object Timer&#123; var index = 0 def current()&#123; index += 1 index &#125;&#125; 调用： 123456789object TestApp &#123; def main(args: Array[String]): Unit = &#123; for(i &lt;- 1 to 3) &#123; Timer.current() println(Timer.index) &#125; &#125;&#125; 结果： 12345123Process finished with exit code 0 Object中的属性和方法，可以通过Object名称(Timer)直接调用。 伴生类和伴生对象 1.定义： 在一个源文件中，如果出现object与class名称相同的情况，那么就可以将该object成为该class的伴生对象，该class则可以成为该object的伴生类。 当一个源文件中，只有object而没有对应的伴生类时，object被称之为Standalone Object-独立对象。 2.使用伴生类或者伴生对象的apply方法： 示例： 1234567891011121314151617object ApplyDemo &#123; def main(args: Array[String]) &#123; ApplyTest() //调用object中的apply方法 val a = new ApplyT a() //调用class中的apply方法 &#125;&#125;class ApplyTest &#123; def apply() &#123; println(\"enter class apply\") &#125;&#125;object ApplyTest &#123; def apply() &#123; println(\"enter object apply\") &#125;&#125; 用类名()，会主动调用object中的apply方法（这是默认的，必须命名为apply），我们知道object关键字在scala中是用来表示单例对象的，也就是说object ApplyTest我们不需要new出来就可以直接使用，但是class与java中的class关键字是一致的，我们想要使用通过new关键字获取一个对象使用，调用伴生类中的apply方法，直接用对象()即可。 枚举 示例： 1234567891011121314151617object EnumApp &#123; def main(args: Array[String]): Unit = &#123; println(Season(1)) println(Season.withName(\"冬天\")) for(ele &lt;- Season.values)&#123; println(ele) &#125; &#125; object Season extends Enumeration &#123; val SPRING = Value(1,\"春天\") val SUMMER = Value(2,\"夏天\") val AUTUMN = Value(3,\"秋天\") val WINTER = Value(4,\"冬天\") &#125;&#125; 运行结果： 12345678春天冬天春天夏天秋天冬天Process finished with exit code 0 case class 在Scala中存在case class，它其实就是一个普通的class。但是它又和普通的class略有区别，如下： 1.初始化的时候可以不用new，当然你也可以加上，普通类一定需要加new。 12345678scala&gt; case class Iteblog(name:String)defined class Iteblogscala&gt; val iteblog = Iteblog(\"iteblog_hadoop\")iteblog: Iteblog = Iteblog(iteblog_hadoop)scala&gt; val iteblog = new Iteblog(\"iteblog_hadoop\")iteblog: Iteblog = Iteblog(iteblog_hadoop) 2.toString的实现更漂亮。 12scala&gt; iteblogres5: Iteblog = Iteblog(iteblog_hadoop) 3.默认实现了equals 和hashCode。 12345678scala&gt; val iteblog2 = Iteblog(\"iteblog_hadoop\")iteblog2: Iteblog = Iteblog(iteblog_hadoop)scala&gt; iteblog == iteblog2res6: Boolean = truescala&gt; iteblog.hashCoderes7: Int = 57880342 4.默认是可以序列化的，也就是实现了Serializable。 12345678910111213141516171819scala&gt; class Adefined class Ascala&gt; import java.io._import java.io._scala&gt; val bos = new ByteArrayOutputStreambos: java.io.ByteArrayOutputStream =scala&gt; val oos = new ObjectOutputStream(bos)oos: java.io.ObjectOutputStream = java.io.ObjectOutputStream@4c257aefscala&gt; oos.writeObject(iteblog)scala&gt; val a = new Aa: A = $iwC$$iwC$A@71687b10scala&gt; oos.writeObject(a)java.io.NotSerializableException: $iwC$$iwC$A 5.自动从scala.Product中继承一些函数。 6.case class构造函数的参数是public级别的，我们可以直接访问。 12scala&gt; iteblog.nameres11: String = iteblog_hadoop 7.支持模式匹配。 其实感觉case class最重要的特性应该就是支持模式匹配。这也是我们定义case class的唯一理由，难怪Scala官方也说：It makes only sense to define case classes if pattern matching is used to decompose data structures. 。来看看下面的例子： 1234567891011121314object caseTest &#123; def main(args: Array[String]): Unit = &#123; val p:Person = Student(\"Tom\",22,80) p match&#123; case Student(name,age,sno) =&gt; println(\"this is a student!\") case Teacher(name,age,tno) =&gt; println(\"this is a teacher!\") case Nobody(name) =&gt; println(\"unknown:\"+name) &#125; &#125; &#125; abstract class Person case class Student(name:String,age:Int,sno:Int) extends Person case class Teacher(name:String,age:Int,tno:Int) extends Person case class Nobody(name:String) extends Person 接口trait 示例： 1234567891011121314151617181920//声明一个 traittrait UserService &#123; def addUser()&#125;trait Logger &#123; def log()&#125;//对于多重继承，使用 with 关键字class UserServiceImpl extends UserService with Logger &#123;//重写父类的方法 override def addUser(): Unit = &#123; println(\"UserServiceImpl.addUser invoked...\") &#125; override def log(): Unit = &#123; println(\"UserServiceImpl.log invoked...\") &#125;&#125; 调用： 12345678object TraitApp &#123; def main(args: Array[String]): Unit = &#123; val userService = new UserServiceImpl userService.addUser() userService.log() &#125;&#125; 运行结果： 1234UserServiceImpl.addUser invoked...UserServiceImpl.log invoked...Process finished with exit code 0","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala基础02 ","date":"2019-05-16T13:25:36.000Z","path":"Scala编程语言/Scala基础02/","text":"Scala基础02 循环 to、Range、until 1.to 1 to 10 等同于1.to(10)，区间左闭右闭，包括10 12scala&gt; 1 to 10res0: scala.collection.immutable.Range.Inclusive = Range 1 to 10 2.Range 区间左闭右开，不包括10 12scala&gt; Range(1,10)res1: scala.collection.immutable.Range = Range 1 until 10 3.until 1 until 10 等同于1.until(10)，与Range相同，区间左闭右开，不包括10 12scala&gt; 1 until 10res2: scala.collection.immutable.Range = Range 1 until 10 4.by by是步长，用法如下： 1234scala&gt; 1 to 10 by 2res3: scala.collection.immutable.Range = inexact Range 1 to 10 by 2scala&gt; 1.to(10).by(2)res4: scala.collection.immutable.Range = inexact Range 1 to 10 by 2 遍历 1.foreach 1234scala&gt; Array(\"Hadoop\",\"Hive\",\"Spark\").foreach(x =&gt; println(x))HadoopHiveSpark 2.for 不需要通过下标来访问 1234val array = Array(\"Hadoop\",\"Hive\",\"Spark\")for(x &lt;- array ) &#123; println(x) &#125; 需要通过下标来访问 1234val array = Array(\"Hadoop\",\"Hive\",\"Spark\")for(i &lt;- 0 to array.length - 1) &#123; println(array(i))&#125; 带条件 123for (i &lt;- 1 to 10 if i%2 ==0)&#123; println(i)&#125; 3.while 求1到100的和 123456var(num, sum) = (100, 0)while (num &gt; 0)&#123; sum = sum + num num = num - 1&#125;println(sum) 终止循环 通过标志位flag终止循环 12345678var index = truevar flag = 1while(index) &#123; if(flag &gt; 10) &#123; index = false &#125; flag += 1&#125; 参数 默认参数 Scala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。 123456789object Test &#123; def main(args: Array[String]) &#123; println( \"返回值 : \" + addInt() ); &#125; def addInt( a:Int=5, b:Int=7 ) : Int = &#123; var sum:Int = 0 sum = a + b &#125;&#125; 由于addInt未传入参数，函数调用它的默认参数，运行结果为12。 可变长参数 Scala 允许向函数传入可变长度参数列表。想要标注可变长度参数，在参数的类型之后加一个星号。 123456789101112object Test &#123; def main(args: Array[String]) &#123; println(sum(1 to 10 : _*) ) &#125; def sum( nums:Int*) : Int = &#123; var result = 0 for (num &lt;- nums) &#123; result += num &#125; result &#125;&#125; :_ *是类型归属的一个特殊实例，它告诉编译器将一个序列类型的单个参数当作可变参数序列。 最后运行结果为55。 命名参数 当调用方法时，可以使用参数名称标记参数。借此机制，可以乱序传参。 123def num(a:String, b:String, c:String, d:String) = println(s\"$a $b $c $d\")num(\"aaa\", \"bbb\", \"ccc\", \"ddd\")num(b=\"bbb\", a=\"aaa\", d=\"ddd\", c=\"ccc\") 当部分参数被命名，部分参数没有被命名时，未命名参数应当放在命名参数之前，并且传参的结果将会遵照方法签名中的参数的顺序。 1num(\"aaa\", d=\"ddd\",b=\"bbb\", c= \"ccc\") 注意，命名参数不能用于调用Java方法。命名参数了解即可。 面向对象 定义类 1.类 scala中定义类的方式：class 类名，例如我们定义个Person类 12class Person&#123;&#125; 2.属性/方法 在类中 1）添加属性的方式：val/var 名称[:类型]=值 2）添加方法的方式：def 方法名(参数列表):返回值类型={方法体} 12345678910111213141516class Person&#123; // 类型可以省略 var age = 18 // _为占位符，那么name的默认值为null val name:String = _ // 属性前面加上private表示私有的，外面就不可以直接调用 private var gender = \"M\" // 定义方法 def playbasketball(team:String)=&#123; println(name + \" is playing basketball for \" + team) &#125; //定义方法 def drinking() = &#123; println(this.name + \" is drinking...\") &#125;&#125; 调用上面的类： 1234567891011object SimpleClassApp &#123; def main(args: Array[String]): Unit = &#123; //创建对象 val person = new Person //给对象的属性赋值 person.name=\"zhang san\" //调用对象方法 person.playBasketball(\"china\") person.drink() &#125;&#125; 运行结果如下： 1234zhangsan is playing basketball for chinazhangsan is drinkingProcess finished with exit code 0 构造函数/继承 构造函数的方式： 1.类名（参数列表…）主构造器 2.def this (参数列表…) 附属构造器 1234567891011121314class Person2(val name: String, val age: Int) &#123; println(\"进入Person2构造器\") val job = \"解说员\" var term = \"\" /** * 1.附属构造起的名称this * 2.每个附属构造起的第一行要么调用主构造器，要么调用其他附属构造起 */ def this(name: String, age: Int, team: String) &#123; this(name, age) this.term = team &#125; println(\"执行完Person2构造器\")&#125; 写一个student 继承Person2，继承的子类注意点已经在注释中写清楚了 1234567891011121314151617181920/** * 继承： * 1.子类方法/构造器执行之前，会先执行父类的构造器 * 2.对于父类没有的字段，需要加上val/var，否则访问不到 * * @param name * @param age * @param stuId */class Student(name: String, age: Int, val stuId: Int) extends Person2(name, age) &#123; println(\"进入Student构造器\") //如果要重写父类的属性或方法，那么前面要加上override override val job: String = \"学生\" //重写toString override def toString: String = &#123; this.name + \"\\t\" + this.age + \"\\t\" + this.stuId &#125; println(\"执行完Student构造器\")&#125; 调用上面的类： 12345678object ConstructApp &#123; def main(args: Array[String]): Unit = &#123; val zhangsan = new Student(\"zhangsan\", 20, 1001) println(\"name:\" + zhangsan.name + \",age:\" + zhangsan.age + \",stuId:\" + zhangsan.stuId) println(zhangsan) &#125;&#125; 运行结果如下： 12345678进入Person2构造器执行完Person2构造器进入Student构造器执行完Student构造器name:zhangsan,age:20,stuId:1001zhangsan 20 1001Process finished with exit code 0 抽象类 Scala继承一个基类跟Java很相似，注意点： 1.类中有一个或者多个方法/属性，没有实现，那么为抽象方法 2.子类必须要实现父类的抽象方法或者属性 3.抽象类不能new 4.子类重写的抽象方法或者属性，不一定要加上override 5.重写一个非抽象方法必须使用override修饰符 12345678910111213141516171819202122232425262728abstract class Person3 &#123; //抽象方法 def speak //抽象属性 val name: String //非抽象方法 def sayHello(): Unit = &#123; println(\"abstract sayHello...\") &#125; //非抽象方法 def walk(): Unit = &#123; println(\"abstract walk...\") &#125;&#125;class Student3 extends Person3 &#123; //重写抽象方法 def speak: Unit = &#123; println(\"override speak...\") &#125; val name: String = \"zhangsan\" //重写非抽象方法 override def sayHello(): Unit = &#123; println(\"override sayHello...\") &#125;&#125; 调用上面的类： 123456789101112object AbstractApp &#123; def main(args: Array[String]): Unit = &#123; val student = new Student3 println(student.name) student.speak student.sayHello() //调用继承过来的方法 student.walk() &#125;&#125; 运行结果如下： 123456zhangsanoverride speak...override sayHello...abstract walk...Process finished with exit code 0","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala基础01 ","date":"2019-05-15T12:25:36.000Z","path":"Scala编程语言/Scala基础01/","text":"Scala基础01 编译 1.演示代码如下： 12345678object HelloWorld &#123; def main(args: Array[String]): Unit = &#123; println(\"Hello World...FROM IDEA....\") &#125;&#125; 2.编译 1$ scalac HelloWorld.scala 然后我们会看到多了一些.class文件： 123HelloWorld.scalaHelloWorld.classHelloWorld$.class 3.运行 1$ scala HelloWorld 运行结果： 1Hello World...FROM IDEA.... 4.对比Java 与Java不同，Scala无需编译就可直接执行： 1scala HelloWorld.scala 执行结果： 1Hello World...FROM IDEA.... 数据类型 下表列出了 Scala 支持的数据类型： 1）Scala 与 Java有着相同的数据类型，在Scala中数据类型都是对象，也就是说scala没有java中的原生类型。 2）Scala数据类型分为两大类 AnyVal(值类型) 和 AnyRef(引用类型)， 注意：不管是AnyVal还是AnyRef 都是对象。 数据类型转换 1.将Int类型转成Double类型 12345678scala&gt; val a = 10a: Int = 10方法一：scala&gt; 10.asInstanceOf[Double]res0: Double = 10.0方法二：scala&gt; 10.toDoubleres1: Double = 10.0 2.判断类型 12345scala&gt; 10.isInstanceOf[Int]res2: Boolean = truescala&gt; 10.isInstanceOf[Double]res3: Boolean = false 声明 1、val, 常量声明 12val name:String = \"uriel\"val name = \"uriel\" 2.var, 变量声明 12var name:String = \"uriel\"var name = \"uriel\" 3.def，方法声明 123def add(a:Int,b:Int) = &#123; a+b&#125; 方法&amp;&amp;函数的定义 定义方法 1.一个完整的方法如下： 123def add(a:Int, b:Int):Int = &#123; return a+b //方法体最后一行作为返回值，大部分场景不要return&#125; 2.方法的最后一句为返回值的话，可以将return 去掉, 如果一个方法体只有一句代码，大括号可以去掉。 1def add(a:Int, b:Int):Int = a + b 3.如果一个方法没有返回值，其返回类型为Unit , 并且“=”号可以去掉 1234//无参数传入，调用是可不带括号，sayHellodef sayHello()&#123; println(\"你好\")&#125; 4.方法可以带返回值，也可以不带返回值，带递归时必须带返回值 1234567def milit（i：Int）：Int = &#123; if（i &lt;= 1）&#123; 1 &#125;else&#123; i * milit(i-1) &#125; &#125; 5.总结 123def 方法名称（[参数名：参数类型]，.....）[:返回值类型 = ] &#123; 方法体&#125; []内的可有可不有 定义函数 1.函数的定义：val/var 函数名称=(函数的参数列表)=&gt;函数体 1234567891011//定义函数，有一个Int类型的参数val fun01 = (a: Int) =&gt; a + 10 //定义函数，有两个Int类型的参数val fun02 = (a: Int, b: Int) =&gt; &#123; if (a &gt; 10 || b &gt; 10)&#123; a + b &#125;else&#123; a - b &#125;&#125; 2.在函数使用过程中，还有另一个常见的操作，就是将函数作为另一个函数的参数来使用，即传名调用（区别于传值调用） 12345678910111213var money =100def calMoneyLeft():Int = &#123; println(\"函数参数调用中\") money&#125; def cost(x: =&gt; Int)&#123; money -=50 println(\"函数参数调用之前\") println(x)&#125; cost(calMoneyLeft()) 运行结果： 123函数参数调用之前函数参数调用中50 方法和函数 1.区别： 方法和函数的定义语法不同 方法一般定义在类，特质，或者object中 方法可以共享所在类，特质，或者object中的属性 可以调用函数，也可以存放到一个变量中，作为参数传递给其他的方法或者函数，也可以作为返回值 2.联系： 1）可以把函数作为参数传递给一个方法： 12345678scala&gt; def method(f:(Int,Int)=&gt;Int) = f(5,1)m: (f: (Int, Int) =&gt; Int)Intscala&gt; val function = (x:Int,y:Int)=&gt;x-yf: (Int, Int) =&gt; Int = $$Lambda$1125/809735384@615784a8scala&gt; method(function)res11: Int = 4 2）方法可以转换成函数 a. 把一个方法作为参数传递给其他的方法或者函数 b. 使用(方法名 _ )把一个方法显式的转换一个函数,注意方法名后面有个空格 12345678910111213141516scala&gt; def m(f:(Int,Int)=&gt;Int) = f(5,1)m: (f: (Int, Int) =&gt; Int)Intscala&gt; def m2(x:Int,y:Int) = x-ym2: (x: Int, y: Int)Int//方法可以转换成函数使用scala&gt; m(m2)res13: Int = 4//把一个方法显式的转换一个函数scala&gt; m2 _res15: (Int, Int) =&gt; Int = $$Lambda$1133/550115845@20d9f6b8scala&gt; m(m2 _)res14: Int = 4","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":"第13章 哈希表","date":"2019-05-07T02:06:09.000Z","path":"数据结构与算法/第13章 哈希表/","text":"第13章 哈希表 13-1 哈希表基础 LeetCode 387. 字符串中的第一个唯一字符 给定一个字符串，找到它的第一个不重复的字符，并返回它的索引。如果不存在，则返回 -1。 示例： s = &quot;leetcode&quot; 返回 0 s = &quot;loveleetcode&quot; 返回 2 提示：你可以假定该字符串只包含小写字母。 1234567891011121314class Solution &#123; public int firstUniqChar(String s) &#123; int[] freq = new int[26]; for(int i = 0 ; i &lt; s.length() ; i ++) freq[s.charAt(i) - 'a'] ++; for(int i = 0 ; i &lt; s.length() ; i ++) if(freq[s.charAt(i) - 'a'] == 1) return i; return -1; &#125;&#125; 需要解决的问题： 哈希函数的设计 哈希冲突的解决 13-2 哈希表的设计 1.大整数 2.浮点型 3.字符串 4.复合类型 设计原则 13-3 Java中的hashCode方法 1.实现一个student类，重写hashCode和equals方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Student &#123; int grade; int cls; String firstName; String lastName; Student(int grade, int cls, String firstName, String lastName)&#123; this.grade = grade; this.cls = cls; this.firstName = firstName; this.lastName = lastName; &#125; @Override public int hashCode()&#123; int B = 31; int hash = 0; hash = hash * B + ((Integer)grade).hashCode(); hash = hash * B + ((Integer)cls).hashCode(); hash = hash * B + firstName.toLowerCase().hashCode(); hash = hash * B + lastName.toLowerCase().hashCode(); return hash; &#125; @Override public boolean equals(Object o)&#123; if(this == o) return true; if(o == null) return false; if(getClass() != o.getClass()) return false; Student another = (Student)o; return this.grade == another.grade &amp;&amp; this.cls == another.cls &amp;&amp; this.firstName.toLowerCase().equals(another.firstName.toLowerCase()) &amp;&amp; this.lastName.toLowerCase().equals(another.lastName.toLowerCase()); &#125;&#125; 2.测试 1234567891011121314151617181920212223242526272829303132public class Main &#123; public static void main(String[] args) &#123; int a = 42; System.out.println(((Integer)a).hashCode()); int b = -42; System.out.println(((Integer)b).hashCode()); double c = 3.1415926; System.out.println(((Double)c).hashCode()); String d = \"uriel\"; System.out.println(d.hashCode()); System.out.println(Integer.MAX_VALUE + 1); System.out.println(); Student student = new Student(3, 2, \"uriel\", \"D\"); System.out.println(student.hashCode()); HashSet&lt;Student&gt; set = new HashSet&lt;&gt;(); set.add(student); HashMap&lt;Student, Integer&gt; scores = new HashMap&lt;&gt;(); scores.put(student, 100); Student student2 = new Student(3, 2, \"uriel\", \"D\"); System.out.println(student2.hashCode()); &#125;&#125; 运行结果： 1234567891042-42219937201111552275-2147483648-836755376-836755376Process finished with exit code 0 13-4 链地址法 Separate Chaining 13-5 实现属于我们自己的哈希表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.TreeMap;public class HashTable&lt;K, V&gt; &#123; private TreeMap&lt;K, V&gt;[] hashtable; private int size; private int M; public HashTable(int M)&#123; this.M = M; size = 0; hashtable = new TreeMap[M]; for(int i = 0 ; i &lt; M ; i ++) hashtable[i] = new TreeMap&lt;&gt;(); &#125; public HashTable()&#123; this(97); &#125; private int hash(K key)&#123; return (key.hashCode() &amp; 0x7fffffff) % M; &#125; public int getSize()&#123; return size; &#125; public void add(K key, V value)&#123; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(map.containsKey(key)) map.put(key, value); else&#123; map.put(key, value); size ++; &#125; &#125; public V remove(K key)&#123; V ret = null; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(map.containsKey(key))&#123; ret = map.remove(key); size --; &#125; return ret; &#125; public void set(K key, V value)&#123; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(!map.containsKey(key)) throw new IllegalArgumentException(key + \" doesn't exist!\"); map.put(key, value); &#125; public boolean contains(K key)&#123; return hashtable[hash(key)].containsKey(key); &#125; public V get(K key)&#123; return hashtable[hash(key)].get(key); &#125;&#125;© 2020 GitHub, Inc. 13-6 哈希表的动态空间处理和复杂度分析 哈希表时间复杂度分析： 哈希表动态空间： 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import java.util.Map;import java.util.TreeMap;public class HashTable&lt;K, V&gt; &#123; private static final int upperTol = 10; private static final int lowerTol = 2; private static final int initCapacity = 7; private TreeMap&lt;K, V&gt;[] hashtable; private int size; private int M; public HashTable(int M)&#123; this.M = M; size = 0; hashtable = new TreeMap[M]; for(int i = 0 ; i &lt; M ; i ++) hashtable[i] = new TreeMap&lt;&gt;(); &#125; public HashTable()&#123; this(initCapacity); &#125; private int hash(K key)&#123; return (key.hashCode() &amp; 0x7fffffff) % M; &#125; public int getSize()&#123; return size; &#125; public void add(K key, V value)&#123; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(map.containsKey(key)) map.put(key, value); else&#123; map.put(key, value); size ++; if(size &gt;= upperTol * M) resize(2 * M); &#125; &#125; public V remove(K key)&#123; V ret = null; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(map.containsKey(key))&#123; ret = map.remove(key); size --; if(size &lt; lowerTol * M &amp;&amp; M / 2 &gt;= initCapacity) resize(M / 2); &#125; return ret; &#125; public void set(K key, V value)&#123; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(!map.containsKey(key)) throw new IllegalArgumentException(key + \" doesn't exist!\"); map.put(key, value); &#125; public boolean contains(K key)&#123; return hashtable[hash(key)].containsKey(key); &#125; public V get(K key)&#123; return hashtable[hash(key)].get(key); &#125; private void resize(int newM)&#123; TreeMap&lt;K, V&gt;[] newHashTable = new TreeMap[newM]; for(int i = 0 ; i &lt; newM ; i ++) newHashTable[i] = new TreeMap&lt;&gt;(); int oldM = M; this.M = newM; for(int i = 0 ; i &lt; oldM ; i ++)&#123; TreeMap&lt;K, V&gt; map = hashtable[i]; for(K key: map.keySet()) newHashTable[hash(key)].put(key, map.get(key)); &#125; this.hashtable = newHashTable; &#125;&#125; 13-7 哈希表更复杂的动态空间处理方法 之前实现的哈希表扩容后M不是素数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495import java.util.TreeMap;public class HashTable&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private final int[] capacity = &#123;53, 97, 193, 389, 769, 1543, 3079, 6151, 12289, 24593, 49157, 98317, 196613, 393241, 786433, 1572869, 3145739, 6291469, 12582917, 25165843, 50331653, 100663319, 201326611, 402653189, 805306457, 1610612741&#125;; private static final int upperTol = 10; private static final int lowerTol = 2; private int capacityIndex = 0; private TreeMap&lt;K, V&gt;[] hashtable; private int size; private int M; public HashTable()&#123; this.M = capacity[capacityIndex]; size = 0; hashtable = new TreeMap[M]; for(int i = 0 ; i &lt; M ; i ++) hashtable[i] = new TreeMap&lt;&gt;(); &#125; private int hash(K key)&#123; return (key.hashCode() &amp; 0x7fffffff) % M; &#125; public int getSize()&#123; return size; &#125; public void add(K key, V value)&#123; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(map.containsKey(key)) map.put(key, value); else&#123; map.put(key, value); size ++; if(size &gt;= upperTol * M &amp;&amp; capacityIndex + 1 &lt; capacity.length)&#123; capacityIndex ++; resize(capacity[capacityIndex]); &#125; &#125; &#125; public V remove(K key)&#123; V ret = null; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(map.containsKey(key))&#123; ret = map.remove(key); size --; if(size &lt; lowerTol * M &amp;&amp; capacityIndex - 1 &gt;= 0)&#123; capacityIndex --; resize(capacity[capacityIndex]); &#125; &#125; return ret; &#125; public void set(K key, V value)&#123; TreeMap&lt;K, V&gt; map = hashtable[hash(key)]; if(!map.containsKey(key)) throw new IllegalArgumentException(key + \" doesn't exist!\"); map.put(key, value); &#125; public boolean contains(K key)&#123; return hashtable[hash(key)].containsKey(key); &#125; public V get(K key)&#123; return hashtable[hash(key)].get(key); &#125; private void resize(int newM)&#123; TreeMap&lt;K, V&gt;[] newHashTable = new TreeMap[newM]; for(int i = 0 ; i &lt; newM ; i ++) newHashTable[i] = new TreeMap&lt;&gt;(); int oldM = M; this.M = newM; for(int i = 0 ; i &lt; oldM ; i ++)&#123; TreeMap&lt;K, V&gt; map = hashtable[i]; for(K key: map.keySet()) newHashTable[hash(key)].put(key, map.get(key)); &#125; this.hashtable = newHashTable; &#125;&#125; 哈希表是无序的： 存在的问题 13-8 更多哈希冲突的处理方法 开放地址法 再哈希法 Coalesced Hashing","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第12章 红黑树","date":"2019-05-05T02:06:09.000Z","path":"数据结构与算法/第12章 红黑树/","text":"第12章 红黑树 12-1 红黑树和2-3树 java.util的TreeSet和TreeMap是基于红黑树实现的 算法导论中关于红黑树的介绍 学习红黑树，首先了解一下2-3树，2-3树是一颗绝对平衡的树 12-2 2-3树的绝对平衡性 2-3树有两种节点： 插入元素到2节点 插入元素到3节点 1）插入的是根节点 2）插入的不是根节点，父节点是2节点 3）插入的不是根节点，父节点是3节点 12-3 红黑树与2-3树的等价性 红黑树和2-3树的等价性 示例 12-4 红黑树的基本性质和复杂度分析 12-5 保持根节点为黑色和左旋转 基于左倾红黑树，需要左旋转的情况 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import java.util.ArrayList;public class RBTree&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private static final boolean RED = true; private static final boolean BLACK = false; private class Node&#123; public K key; public V value; public Node left, right; public boolean color; public Node(K key, V value)&#123; this.key = key; this.value = value; left = null; right = null; color = RED; &#125; &#125; private Node root; private int size; public RBTree()&#123; root = null; size = 0; &#125; public int getSize()&#123; return size; &#125; public boolean isEmpty()&#123; return size == 0; &#125; // 判断节点node的颜色 private boolean isRed(Node node)&#123; if(node == null) return BLACK; return node.color; &#125; // node x // / \\ 左旋转 / \\ // T1 x ---------&gt; node T3 // / \\ / \\ // T2 T3 T1 T2 private Node leftRotate(Node node)&#123; Node x = node.right; // 左旋转 node.right = x.left; x.left = node; x.color = node.color; node.color = RED; return x; &#125; // 向红黑树中添加新的元素(key, value) public void add(K key, V value)&#123; root = add(root, key, value); root.color = BLACK; // 最终根节点为黑色节点 &#125; // 向以node为根的红黑树中插入元素(key, value)，递归算法 // 返回插入新节点后红黑树的根 private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); // 默认插入红色节点 &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; return node; &#125; 12-6 颜色翻转和右旋转 1.颜色翻转 1234567// 颜色翻转private void flipColors(Node node)&#123; node.color = RED; node.left.color = BLACK; node.right.color = BLACK;&#125; 2.右旋转 最后进行颜色翻转 123456789101112131415161718// node x// / \\ 右旋转 / \\// x T2 -------&gt; y node// / \\ / \\// y T1 T1 T2private Node rightRotate(Node node)&#123; Node x = node.left; // 右旋转 node.left = x.right; x.right = node; x.color = node.color; node.color = RED; return x;&#125; 12-7 红黑树中添加新元素 向红黑树中（等价2节点）添加新元素有两种情况 一种新添加元素比较小，另一种情况新添加元素比较大（右孩子是红节点），做左旋转 向红黑树中（等价3节点）添加新元素有三种情况 情况1：颜色翻转 情况2：右旋转 情况3： 这三种情况可以用一个逻辑链条串起来 123456789101112131415161718192021222324252627282930313233// 向红黑树中添加新的元素(key, value) public void add(K key, V value)&#123; root = add(root, key, value); root.color = BLACK; // 最终根节点为黑色节点 &#125; // 向以node为根的红黑树中插入元素(key, value)，递归算法 // 返回插入新节点后红黑树的根 private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); // 默认插入红色节点 &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; if (isRed(node.right) &amp;&amp; !isRed(node.left)) node = leftRotate(node); if (isRed(node.left) &amp;&amp; isRed(node.left.left)) node = rightRotate(node); if (isRed(node.left) &amp;&amp; isRed(node.right)) flipColors(node); return node; &#125; 12-8 红黑树的性能测试 红黑树在添加或删除元素的性能比AVL树更有优势，下面是只添加元素的情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.util.ArrayList;import java.util.Random;public class Main2 &#123; public static void main(String[] args) &#123; // int n = 20000000; int n = 20000000; Random random = new Random(n); ArrayList&lt;Integer&gt; testData = new ArrayList&lt;&gt;(n); for(int i = 0 ; i &lt; n ; i ++) testData.add(random.nextInt(Integer.MAX_VALUE)); // Test BST long startTime = System.nanoTime(); BST&lt;Integer, Integer&gt; bst = new BST&lt;&gt;(); for (Integer x: testData) bst.add(x, null); long endTime = System.nanoTime(); double time = (endTime - startTime) / 1000000000.0; System.out.println(\"BST: \" + time + \" s\"); // Test AVL startTime = System.nanoTime(); AVLTree&lt;Integer, Integer&gt; avl = new AVLTree&lt;&gt;(); for (Integer x: testData) avl.add(x, null); endTime = System.nanoTime(); time = (endTime - startTime) / 1000000000.0; System.out.println(\"AVL: \" + time + \" s\"); // Test RBTree startTime = System.nanoTime(); RBTree&lt;Integer, Integer&gt; rbt = new RBTree&lt;&gt;(); for (Integer x: testData) rbt.add(x, null); endTime = System.nanoTime(); time = (endTime - startTime) / 1000000000.0; System.out.println(\"RBTree: \" + time + \" s\"); &#125;&#125; 总结：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第11章 AVL","date":"2019-05-01T02:06:09.000Z","path":"数据结构与算法/第11章 AVL/","text":"第11章 AVL 11-1 平衡树和AVL AVL树是最先发明的自平衡二叉查找树。在AVL树中任何节点的两个儿子子树的高度最大差别为一，所以它也被称为高度平衡树。 平衡二叉树判定条件： 11-2 计算节点的高度和平衡因子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190import java.util.ArrayList;public class AVLTree&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private class Node&#123; public K key; public V value; public Node left, right; public int height; public Node(K key, V value)&#123; this.key = key; this.value = value; left = null; right = null; height = 1; &#125; &#125; private Node root; private int size; public AVLTree()&#123; root = null; size = 0; &#125; public int getSize()&#123; return size; &#125; public boolean isEmpty()&#123; return size == 0; &#125; // 获得节点node的高度 private int getHeight(Node node)&#123; if(node == null) return 0; return node.height; &#125; // 获得节点node的平衡因子 private int getBalanceFactor(Node node)&#123; if(node == null) return 0; return getHeight(node.left) - getHeight(node.right); &#125; // 向二分搜索树中添加新的元素(key, value) public void add(K key, V value)&#123; root = add(root, key, value); &#125; // 向以node为根的二分搜索树中插入元素(key, value)，递归算法 // 返回插入新节点后二分搜索树的根 private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; // 更新height node.height = 1 + Math.max(getHeight(node.left), getHeight(node.right)); // 计算平衡因子 int balanceFactor = getBalanceFactor(node); if(Math.abs(balanceFactor) &gt; 1) System.out.println(\"unbalanced : \" + balanceFactor); return node; &#125; // 返回以node为根节点的二分搜索树中，key所在的节点 private Node getNode(Node node, K key)&#123; if(node == null) return null; if(key.equals(node.key)) return node; else if(key.compareTo(node.key) &lt; 0) return getNode(node.left, key); else // if(key.compareTo(node.key) &gt; 0) return getNode(node.right, key); &#125; public boolean contains(K key)&#123; return getNode(root, key) != null; &#125; public V get(K key)&#123; Node node = getNode(root, key); return node == null ? null : node.value; &#125; public void set(K key, V newValue)&#123; Node node = getNode(root, key); if(node == null) throw new IllegalArgumentException(key + \" doesn't exist!\"); node.value = newValue; &#125; // 返回以node为根的二分搜索树的最小值所在的节点 private Node minimum(Node node)&#123; if(node.left == null) return node; return minimum(node.left); &#125; // 删除掉以node为根的二分搜索树中的最小节点 // 返回删除节点后新的二分搜索树的根 private Node removeMin(Node node)&#123; if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; node.left = removeMin(node.left); return node; &#125; // 从二分搜索树中删除键为key的节点 public V remove(K key)&#123; Node node = getNode(root, key); if(node != null)&#123; root = remove(root, key); return node.value; &#125; return null; &#125; private Node remove(Node node, K key)&#123; if( node == null ) return null; if( key.compareTo(node.key) &lt; 0 )&#123; node.left = remove(node.left , key); return node; &#125; else if(key.compareTo(node.key) &gt; 0 )&#123; node.right = remove(node.right, key); return node; &#125; else&#123; // key.compareTo(node.key) == 0 // 待删除节点左子树为空的情况 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; // 待删除节点右子树为空的情况 if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; // 待删除节点左右子树均不为空的情况 // 找到比待删除节点大的最小节点, 即待删除节点右子树的最小节点 // 用这个节点顶替待删除节点的位置 Node successor = minimum(node.right); successor.right = removeMin(node.right); successor.left = node.left; node.left = node.right = null; return successor; &#125; &#125; 11-3 检查二分搜索树性质和平衡性 1.判断该二叉树是否是一棵二分搜索树 1234567891011121314151617181920// 判断该二叉树是否是一棵二分搜索树public boolean isBST()&#123; ArrayList&lt;K&gt; keys = new ArrayList&lt;&gt;(); inOrder(root, keys); for(int i = 1 ; i &lt; keys.size() ; i ++) if(keys.get(i - 1).compareTo(keys.get(i)) &gt; 0) return false; return true;&#125;private void inOrder(Node node, ArrayList&lt;K&gt; keys)&#123; if(node == null) return; inOrder(node.left, keys); keys.add(node.key); inOrder(node.right, keys);&#125; 2.判断该二叉树是否是一棵平衡二叉树 12345678910111213141516// 判断该二叉树是否是一棵平衡二叉树public boolean isBalanced()&#123; return isBalanced(root);&#125;// 判断以Node为根的二叉树是否是一棵平衡二叉树，递归算法private boolean isBalanced(Node node)&#123; if(node == null) return true; int balanceFactor = getBalanceFactor(node); if(Math.abs(balanceFactor) &gt; 1) return false; return isBalanced(node.left) &amp;&amp; isBalanced(node.right);&#125; 11-4 旋转操作的基本原理 右旋转 当AVL树中某一节点的左子树与右子树的高度差大于1时（左高右低），进行右旋转操作。 if (balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &gt;= 0) 11-5 左旋转和右旋转的实现 左旋转 代码实现左、右旋转 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 对节点y进行向右旋转操作，返回旋转后新的根节点x// y x// / \\ / \\// x T4 向右旋转 (y) z y// / \\ - - - - - - - -&gt; / \\ / \\// z T3 T1 T2 T3 T4// / \\// T1 T2private Node rightRotate(Node y) &#123; Node x = y.left; Node T3 = x.right; // 向右旋转过程 x.right = y; y.left = T3; // 更新height y.height = Math.max(getHeight(y.left), getHeight(y.right)) + 1; x.height = Math.max(getHeight(x.left), getHeight(x.right)) + 1; return x;&#125;// 对节点y进行向左旋转操作，返回旋转后新的根节点x// y x// / \\ / \\// T1 x 向左旋转 (y) y z// / \\ - - - - - - - -&gt; / \\ / \\// T2 z T1 T2 T3 T4// / \\// T3 T4private Node leftRotate(Node y) &#123; Node x = y.right; Node T2 = x.left; // 向左旋转过程 x.left = y; y.right = T2; // 更新height y.height = Math.max(getHeight(y.left), getHeight(y.right)) + 1; x.height = Math.max(getHeight(x.left), getHeight(x.right)) + 1; return x;&#125; 平衡维护 1234567891011121314151617181920212223242526272829303132333435363738394041// 向以node为根的二分搜索树中插入元素(key, value)，递归算法// 返回插入新节点后二分搜索树的根private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; // 更新height node.height = 1 + Math.max(getHeight(node.left), getHeight(node.right)); // 计算平衡因子 int balanceFactor = getBalanceFactor(node); // 平衡维护 if (balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &gt;= 0) return rightRotate(node); if (balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(node.right) &lt;= 0) return leftRotate(node); if (balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &lt; 0) &#123; node.left = leftRotate(node.left); return rightRotate(node); &#125; if (balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(node.right) &gt; 0) &#123; node.right = rightRotate(node.right); return leftRotate(node); &#125; return node;&#125; 11-6 LR和RL 左旋转和右旋转适用于LL和RR的情况 LL：新插入的节点在根节点的左侧的左侧 RR：新插入的节点在根节点右侧的右侧 LR旋转前： LR旋转后： RL旋转前： RL旋转后： 插入新元素后的平衡维护： 123456789101112131415161718192021222324252627282930313233343536373839404142 // 向以node为根的二分搜索树中插入元素(key, value)，递归算法 // 返回插入新节点后二分搜索树的根 private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; // 更新height node.height = 1 + Math.max(getHeight(node.left), getHeight(node.right)); // 计算平衡因子 int balanceFactor = getBalanceFactor(node); // 平衡维护//LL if (balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &gt;= 0) return rightRotate(node); //RR if (balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(node.right) &lt;= 0) return leftRotate(node); //LR if (balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(node.left) &lt; 0) &#123; node.left = leftRotate(node.left); return rightRotate(node); &#125; //RL if (balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(node.right) &gt; 0) &#123; node.right = rightRotate(node.right); return leftRotate(node); &#125; return node; &#125; 11-7 从AVL树中删除元素 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495// 从二分搜索树中删除键为key的节点public V remove(K key)&#123; Node node = getNode(root, key); if(node != null)&#123; root = remove(root, key); return node.value; &#125; return null;&#125;private Node remove(Node node, K key)&#123; if( node == null ) return null; Node retNode; if( key.compareTo(node.key) &lt; 0 )&#123; node.left = remove(node.left , key); // return node; retNode = node; &#125; else if(key.compareTo(node.key) &gt; 0 )&#123; node.right = remove(node.right, key); // return node; retNode = node; &#125; else&#123; // key.compareTo(node.key) == 0 // 待删除节点左子树为空的情况 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; // return rightNode; retNode = rightNode; &#125; // 待删除节点右子树为空的情况 else if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; // return leftNode; retNode = leftNode; &#125; // 待删除节点左右子树均不为空的情况 else&#123; // 找到比待删除节点大的最小节点, 即待删除节点右子树的最小节点 // 用这个节点顶替待删除节点的位置 Node successor = minimum(node.right); //successor.right = removeMin(node.right); successor.right = remove(node.right, successor.key); successor.left = node.left; node.left = node.right = null; // return successor; retNode = successor; &#125; &#125; if(retNode == null) return null; // 更新height retNode.height = 1 + Math.max(getHeight(retNode.left), getHeight(retNode.right)); // 计算平衡因子 int balanceFactor = getBalanceFactor(retNode); // 平衡维护 // LL if (balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(retNode.left) &gt;= 0) return rightRotate(retNode); // RR if (balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(retNode.right) &lt;= 0) return leftRotate(retNode); // LR if (balanceFactor &gt; 1 &amp;&amp; getBalanceFactor(retNode.left) &lt; 0) &#123; retNode.left = leftRotate(retNode.left); return rightRotate(retNode); &#125; // RL if (balanceFactor &lt; -1 &amp;&amp; getBalanceFactor(retNode.right) &gt; 0) &#123; retNode.right = rightRotate(retNode.right); return leftRotate(retNode); &#125; return retNode;&#125; 11-8 基于AVL树的集合和映射 1.基于AVL树的集合 123456789101112131415161718192021222324252627282930313233public class AVLSet&lt;E extends Comparable&lt;E&gt;&gt; implements Set&lt;E&gt; &#123; private AVLTree&lt;E, Object&gt; avl; public AVLSet()&#123; avl = new AVLTree&lt;&gt;(); &#125; @Override public int getSize()&#123; return avl.getSize(); &#125; @Override public boolean isEmpty()&#123; return avl.isEmpty(); &#125; @Override public void add(E e)&#123; avl.add(e, null); &#125; @Override public boolean contains(E e)&#123; return avl.contains(e); &#125; @Override public void remove(E e)&#123; avl.remove(e); &#125;&#125; 2.基于AVL树的映射 123456789101112131415161718192021222324252627282930313233public class AVLSet&lt;E extends Comparable&lt;E&gt;&gt; implements Set&lt;E&gt; &#123; private AVLTree&lt;E, Object&gt; avl; public AVLSet()&#123; avl = new AVLTree&lt;&gt;(); &#125; @Override public int getSize()&#123; return avl.getSize(); &#125; @Override public boolean isEmpty()&#123; return avl.isEmpty(); &#125; @Override public void add(E e)&#123; avl.add(e, null); &#125; @Override public boolean contains(E e)&#123; return avl.contains(e); &#125; @Override public void remove(E e)&#123; avl.remove(e); &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第10章 并查集","date":"2019-04-29T02:06:09.000Z","path":"数据结构与算法/第10章 并查集/","text":"第10章 并查集 10-1 什么是并查集 并查集，在一些有N个元素的集合应用问题中，我们通常是在开始时让每个元素构成一个单元素的集合，然后按一定顺序将属于同一组的元素所在的集合合并，其间要反复查找一个元素在哪个集合中。 并查集是一种树型的数据结构，用于处理一些不相交集合(Disjoint Sets)的合并及查询问题。常常在使用中以森林来表示。 10-2 Quick Find 初步实现，用数组模拟并查集的实现过程 1.定义并查集的接口 12345public interface UF &#123; int getSize(); boolean isConnected(int p, int q); void unionElements(int p, int q);&#125; 2.实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 我们的第一版Union-Findpublic class UnionFind1 implements UF &#123; private int[] id; // 我们的第一版Union-Find本质就是一个数组 public UnionFind1(int size) &#123; id = new int[size]; // 初始化, 每一个id[i]指向自己, 没有合并的元素 for (int i = 0; i &lt; size; i++) id[i] = i; &#125; @Override public int getSize()&#123; return id.length; &#125; // 查找元素p所对应的集合编号 // O(1)复杂度 private int find(int p) &#123; if(p &lt; 0 || p &gt;= id.length) throw new IllegalArgumentException(\"p is out of bound.\"); return id[p]; &#125; // 查看元素p和元素q是否所属一个集合 // O(1)复杂度 @Override public boolean isConnected(int p, int q) &#123; return find(p) == find(q); &#125; // 合并元素p和元素q所属的集合 // O(n) 复杂度 @Override public void unionElements(int p, int q) &#123; int pID = find(p); int qID = find(q); if (pID == qID) return; // 合并过程需要遍历一遍所有元素, 将两个元素的所属集合编号合并 for (int i = 0; i &lt; id.length; i++) if (id[i] == pID) id[i] = qID; &#125;&#125; 3.时间复杂度分析 10-3 Quick Union 第二版Union-Find，优化Union，时间复杂度为O(h),h为树的深度 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class UnionFind2 implements UF &#123; // 我们的第二版Union-Find, 使用一个数组构建一棵指向父节点的树 // parent[i]表示第一个元素所指向的父节点 private int[] parent; // 构造函数 public UnionFind2(int size)&#123; parent = new int[size]; // 初始化, 每一个parent[i]指向自己, 表示每一个元素自己自成一个集合 for( int i = 0 ; i &lt; size ; i ++ ) parent[i] = i; &#125; @Override public int getSize()&#123; return parent.length; &#125; // 查找过程, 查找元素p所对应的集合编号 // O(h)复杂度, h为树的高度 private int find(int p)&#123; if(p &lt; 0 || p &gt;= parent.length) throw new IllegalArgumentException(\"p is out of bound.\"); // 不断去查询自己的父亲节点, 直到到达根节点 // 根节点的特点: parent[p] == p while(p != parent[p]) p = parent[p]; return p; &#125; // 查看元素p和元素q是否所属一个集合 // O(h)复杂度, h为树的高度 @Override public boolean isConnected( int p , int q )&#123; return find(p) == find(q); &#125; // 合并元素p和元素q所属的集合 // O(h)复杂度, h为树的高度 @Override public void unionElements(int p, int q)&#123; int pRoot = find(p); int qRoot = find(q); if( pRoot == qRoot ) return; parent[pRoot] = qRoot; &#125;&#125; 10-4 基于size的优化 第三版Union-Find：基于size的优化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class UnionFind3 implements UF&#123; private int[] parent; // parent[i]表示第一个元素所指向的父节点 private int[] sz; // sz[i]表示以i为根的集合中元素个数 // 构造函数 public UnionFind3(int size)&#123; parent = new int[size]; sz = new int[size]; // 初始化, 每一个parent[i]指向自己, 表示每一个元素自己自成一个集合 for(int i = 0 ; i &lt; size ; i ++)&#123; parent[i] = i; sz[i] = 1; &#125; &#125; @Override public int getSize()&#123; return parent.length; &#125; // 查找过程, 查找元素p所对应的集合编号 // O(h)复杂度, h为树的高度 private int find(int p)&#123; if(p &lt; 0 || p &gt;= parent.length) throw new IllegalArgumentException(\"p is out of bound.\"); // 不断去查询自己的父亲节点, 直到到达根节点 // 根节点的特点: parent[p] == p while( p != parent[p] ) p = parent[p]; return p; &#125; // 查看元素p和元素q是否所属一个集合 // O(h)复杂度, h为树的高度 @Override public boolean isConnected( int p , int q )&#123; return find(p) == find(q); &#125; // 合并元素p和元素q所属的集合 // O(h)复杂度, h为树的高度 @Override public void unionElements(int p, int q)&#123; int pRoot = find(p); int qRoot = find(q); if(pRoot == qRoot) return; // 根据两个元素所在树的元素个数不同判断合并方向 // 将元素个数少的集合合并到元素个数多的集合上 if(sz[pRoot] &lt; sz[qRoot])&#123; parent[pRoot] = qRoot; sz[qRoot] += sz[pRoot]; &#125; else&#123; // sz[qRoot] &lt;= sz[pRoot] parent[qRoot] = pRoot; sz[pRoot] += sz[qRoot]; &#125; &#125;&#125; 10-5 基于rank的优化 第四版Union-Find， 基于rank的优化（不完全是深度） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class UnionFind4 implements UF &#123; private int[] rank; // rank[i]表示以i为根的集合所表示的树的层数 private int[] parent; // parent[i]表示第i个元素所指向的父节点 // 构造函数 public UnionFind4(int size)&#123; rank = new int[size]; parent = new int[size]; // 初始化, 每一个parent[i]指向自己, 表示每一个元素自己自成一个集合 for( int i = 0 ; i &lt; size ; i ++ )&#123; parent[i] = i; rank[i] = 1; &#125; &#125; @Override public int getSize()&#123; return parent.length; &#125; // 查找过程, 查找元素p所对应的集合编号 // O(h)复杂度, h为树的高度 private int find(int p)&#123; if(p &lt; 0 || p &gt;= parent.length) throw new IllegalArgumentException(\"p is out of bound.\"); // 不断去查询自己的父亲节点, 直到到达根节点 // 根节点的特点: parent[p] == p while(p != parent[p]) p = parent[p]; return p; &#125; // 查看元素p和元素q是否所属一个集合 // O(h)复杂度, h为树的高度 @Override public boolean isConnected( int p , int q )&#123; return find(p) == find(q); &#125; // 合并元素p和元素q所属的集合 // O(h)复杂度, h为树的高度 @Override public void unionElements(int p, int q)&#123; int pRoot = find(p); int qRoot = find(q); if( pRoot == qRoot ) return; // 根据两个元素所在树的rank不同判断合并方向 // 将rank低的集合合并到rank高的集合上 if(rank[pRoot] &lt; rank[qRoot]) parent[pRoot] = qRoot; else if(rank[qRoot] &lt; rank[pRoot]) parent[qRoot] = pRoot; else&#123; // rank[pRoot] == rank[qRoot] parent[pRoot] = qRoot; rank[qRoot] += 1; // 此时, 我维护rank的值 &#125; &#125;&#125; 10-6 路径压缩 第五版Union-Find：路径压缩 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class UnionFind5 implements UF &#123; // rank[i]表示以i为根的集合所表示的树的层数 // 在后续的代码中, 我们并不会维护rank的语意, 也就是rank的值在路径压缩的过程中, 有可能不在是树的层数值 // 这也是我们的rank不叫height或者depth的原因, 他只是作为比较的一个标准 private int[] rank; private int[] parent; // parent[i]表示第i个元素所指向的父节点 // 构造函数 public UnionFind5(int size)&#123; rank = new int[size]; parent = new int[size]; // 初始化, 每一个parent[i]指向自己, 表示每一个元素自己自成一个集合 for( int i = 0 ; i &lt; size ; i ++ )&#123; parent[i] = i; rank[i] = 1; &#125; &#125; @Override public int getSize()&#123; return parent.length; &#125; // 查找过程, 查找元素p所对应的集合编号 // O(h)复杂度, h为树的高度 private int find(int p)&#123; if(p &lt; 0 || p &gt;= parent.length) throw new IllegalArgumentException(\"p is out of bound.\"); while( p != parent[p] )&#123; parent[p] = parent[parent[p]]; p = parent[p]; &#125; return p; &#125; // 查看元素p和元素q是否所属一个集合 // O(h)复杂度, h为树的高度 @Override public boolean isConnected( int p , int q )&#123; return find(p) == find(q); &#125; // 合并元素p和元素q所属的集合 // O(h)复杂度, h为树的高度 @Override public void unionElements(int p, int q)&#123; int pRoot = find(p); int qRoot = find(q); if( pRoot == qRoot ) return; // 根据两个元素所在树的rank不同判断合并方向 // 将rank低的集合合并到rank高的集合上 if( rank[pRoot] &lt; rank[qRoot] ) parent[pRoot] = qRoot; else if( rank[qRoot] &lt; rank[pRoot]) parent[qRoot] = pRoot; else&#123; // rank[pRoot] == rank[qRoot] parent[pRoot] = qRoot; rank[qRoot] += 1; // 此时, 我维护rank的值 &#125; &#125;&#125; 10-7 更多和并查集相关的话题 使用递归完成路径压缩，每个节点都指向根节点，压缩的更彻底，但因为使用了递归方法，性能比非递归方式差些。 1234567891011// 查找过程, 查找元素p所对应的集合编号// O(h)复杂度, h为树的高度private int find(int p)&#123; if(p &lt; 0 || p &gt;= parent.length) throw new IllegalArgumentException(\"p is out of bound.\"); // path compression 2, 递归算法 if(p != parent[p]) parent[p] = find(parent[p]); return parent[p];&#125; 经以上优化后并查集的时间复杂度分析：","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第9章 字典树Trie","date":"2019-04-26T02:06:09.000Z","path":"数据结构与算法/第9章 字典树Trie/","text":"第9章 字典树Trie 9-1 什么是Trie字典树 Trie，又称前缀树或字典树，是一种有序树状的数据结构，用于保存关联数组，其中的键通常是字符串。 Trie与二叉查找树不同，键不是直接保存在节点中，而是由节点在树中的位置决定。一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符串，而根节点对应空字符串。一般情况下，不是所有的节点都有对应的值，只有叶子节点和部分内部节点所对应的键才有相关的值。 9-2 Trie字典树基础 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.TreeMap;public class Trie &#123; private class Node&#123; public boolean isWord; public TreeMap&lt;Character, Node&gt; next; public Node(boolean isWord)&#123; this.isWord = isWord; next = new TreeMap&lt;&gt;(); &#125; public Node()&#123; this(false); &#125; &#125; private Node root; private int size; public Trie()&#123; root = new Node(); size = 0; &#125; // 获得Trie中存储的单词数量 public int getSize()&#123; return size; &#125; // 向Trie中添加一个新的单词word public void add(String word)&#123; Node cur = root; for(int i = 0 ; i &lt; word.length() ; i ++)&#123; char c = word.charAt(i); if(cur.next.get(c) == null) cur.next.put(c, new Node()); cur = cur.next.get(c); &#125; if(!cur.isWord)&#123; cur.isWord = true; size ++; &#125; &#125;&#125; 9-3 Trie字典树的查询 查询单词word是否在Trie中 1234567891011public boolean contains(String word)&#123; Node cur = root; for(int i = 0 ; i &lt; word.length() ; i ++)&#123; char c = word.charAt(i); if(cur.next.get(c) == null) return false; cur = cur.next.get(c); &#125; return cur.isWord;&#125; 9-4 Trie字典树的前缀查询 查询是否在Trie中有单词以prefix为前缀 123456789101112public boolean isPrefix(String prefix)&#123; Node cur = root; for(int i = 0 ; i &lt; prefix.length() ; i ++)&#123; char c = prefix.charAt(i); if(cur.next.get(c) == null) return false; cur = cur.next.get(c); &#125; return true;&#125; 9-5 Trie字典树和简单的模式匹配 LeetCode211题 添加与搜索单词 - 数据结构设计 设计一个支持以下两种操作的数据结构： void addWord(word) bool search(word) search(word) 可以搜索文字或正则表达式字符串，字符串只包含字母 . 或 a-z 。 . 可以表示任何一个字母。 示例: addWord(“bad”) addWord(“dad”) addWord(“mad”) search(“pad”) -&gt; false search(“bad”) -&gt; true search(&quot;.ad&quot;) -&gt; true search(“b…”) -&gt; true 说明:你可以假设所有单词都是由小写字母 a-z 组成的。 分析： 实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import java.util.TreeMap;public class WordDictionary &#123; private class Node&#123; public boolean isWord; public TreeMap&lt;Character, Node&gt; next; public Node(boolean isWord)&#123; this.isWord = isWord; next = new TreeMap&lt;&gt;(); &#125; public Node()&#123; this(false); &#125; &#125; private Node root; /** Initialize your data structure here. */ public WordDictionary() &#123; root = new Node(); &#125; /** Adds a word into the data structure. */ public void addWord(String word) &#123; Node cur = root; for(int i = 0 ; i &lt; word.length() ; i ++)&#123; char c = word.charAt(i); if(cur.next.get(c) == null) cur.next.put(c, new Node()); cur = cur.next.get(c); &#125; cur.isWord = true; &#125; /** Returns if the word is in the data structure. A word could contain the dot character '.' to represent any one letter. */ public boolean search(String word) &#123; return match(root, word, 0); &#125; private boolean match(Node node, String word, int index)&#123; if(index == word.length()) return node.isWord; char c = word.charAt(index); if(c != '.')&#123; if(node.next.get(c) == null) return false; return match(node.next.get(c), word, index + 1); &#125; else&#123; for(char nextChar: node.next.keySet()) if(match(node.next.get(nextChar), word, index + 1)) return true; return false; &#125; &#125;&#125; 9-6 Trie字典树和字符串映射 LeetCode677题：键值映射 实现一个 MapSum 类里的两个方法，insert 和 sum。 对于方法insert，你将得到一对（字符串，整数）的键值对。字符串表示键，整数表示值。如果键已经存在，那么原来的键值对将被替代成新的键值对。 对于方法 sum，你将得到一个表示前缀的字符串，你需要返回所有以该前缀开头的键的值的总和。 示例 1: 输入: insert(“apple”, 3), 输出: Null 输入: sum(“ap”), 输出: 3 输入: insert(“app”, 2), 输出: Null 输入: sum(“ap”), 输出: 5 实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import java.util.TreeMap;public class MapSum &#123; private class Node&#123; public int value; public TreeMap&lt;Character, Node&gt; next; public Node(int value)&#123; this.value = value; next = new TreeMap&lt;&gt;(); &#125; public Node()&#123; this(0); &#125; &#125; private Node root; /** Initialize your data structure here. */ public MapSum() &#123; root = new Node(); &#125; public void insert(String key, int val) &#123; Node cur = root; for(int i = 0 ; i &lt; key.length() ; i ++)&#123; char c = key.charAt(i); if(cur.next.get(c) == null) cur.next.put(c, new Node()); cur = cur.next.get(c); &#125; cur.value = val; &#125; public int sum(String prefix) &#123; Node cur = root; for(int i = 0 ; i &lt; prefix.length() ; i ++)&#123; char c = prefix.charAt(i); if(cur.next.get(c) == null) return 0; cur = cur.next.get(c); &#125; return sum(cur); &#125; private int sum(Node node)&#123; int res = node.value; for(char c: node.next.keySet()) res += sum(node.next.get(c)); return res; &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第8章 线段树","date":"2019-04-23T02:06:09.000Z","path":"数据结构与算法/第8章 线段树/","text":"第8章 线段树 8-1 什么是线段树 线段树是一种二叉搜索树，与区间树相似，它将一个区间划分成一些单元区间，每个单元区间对应线段树中的一个叶结点。 使用线段树可以快速的查找某一个节点在若干条线段中出现的次数，时间复杂度为O(logN)。而未优化的空间复杂度为2N，实际应用时一般还要开4N的数组以免越界，因此有时需要离散化让空间压缩。 线段树应用： 线段树示例： 8-2 线段树基础表示 线段树数组需要开辟的空间： 8-3 创建线段树 1.定义一个Merger接口，使用户在构建线段树的时候传递业务‘ 123public interface Merger&lt;E&gt; &#123; E merge(E a, E b);&#125; 2.创建线段树 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374public class SegmentTree&lt;E&gt; &#123; private E[] tree; private E[] data; private Merger&lt;E&gt; merger; public SegmentTree(E[] arr, Merger&lt;E&gt; merger)&#123; this.merger = merger; data = (E[])new Object[arr.length]; for(int i = 0 ; i &lt; arr.length ; i ++) data[i] = arr[i]; tree = (E[])new Object[4 * arr.length]; buildSegmentTree(0, 0, arr.length - 1); &#125; // 在treeIndex的位置创建表示区间[l...r]的线段树 private void buildSegmentTree(int treeIndex, int l, int r)&#123; if(l == r)&#123; tree[treeIndex] = data[l]; return; &#125; int leftTreeIndex = leftChild(treeIndex); int rightTreeIndex = rightChild(treeIndex); // int mid = (l + r) / 2; int mid = l + (r - l) / 2; buildSegmentTree(leftTreeIndex, l, mid); buildSegmentTree(rightTreeIndex, mid + 1, r); tree[treeIndex] = merger.merge(tree[leftTreeIndex], tree[rightTreeIndex]); &#125; public int getSize()&#123; return data.length; &#125; public E get(int index)&#123; if(index &lt; 0 || index &gt;= data.length) throw new IllegalArgumentException(\"Index is illegal.\"); return data[index]; &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的左孩子节点的索引 private int leftChild(int index)&#123; return 2*index + 1; &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的右孩子节点的索引 private int rightChild(int index)&#123; return 2*index + 2; &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append('['); for(int i = 0 ; i &lt; tree.length ; i ++)&#123; if(tree[i] != null) res.append(tree[i]); else res.append(\"null\"); if(i != tree.length - 1) res.append(\", \"); &#125; res.append(']'); return res.toString(); &#125;&#125; 3.测试 123456789101112131415161718public class Main &#123; public static void main(String[] args) &#123; Integer[] nums = &#123;-2, 0, 3, -5, 2, -1&#125;;// SegmentTree&lt;Integer&gt; segTree = new SegmentTree&lt;&gt;(nums,// new Merger&lt;Integer&gt;() &#123;// @Override// public Integer merge(Integer a, Integer b) &#123;// return a + b;// &#125;// &#125;); SegmentTree&lt;Integer&gt; segTree = new SegmentTree&lt;&gt;(nums, (a, b) -&gt; a + b); System.out.println(segTree); &#125;&#125; 8-4 线段树中的区间查询 1234567891011121314151617181920// 在以treeIndex为根的线段树中[l...r]的范围里，搜索区间[queryL...queryR]的值 private E query(int treeIndex, int l, int r, int queryL, int queryR)&#123; if(l == queryL &amp;&amp; r == queryR) return tree[treeIndex]; int mid = l + (r - l) / 2; // treeIndex的节点分为[l...mid]和[mid+1...r]两部分 int leftTreeIndex = leftChild(treeIndex); int rightTreeIndex = rightChild(treeIndex); if(queryL &gt;= mid + 1) return query(rightTreeIndex, mid + 1, r, queryL, queryR); else if(queryR &lt;= mid) return query(leftTreeIndex, l, mid, queryL, queryR); E leftResult = query(leftTreeIndex, l, mid, queryL, mid); E rightResult = query(rightTreeIndex, mid + 1, r, mid + 1, queryR); return merger.merge(leftResult, rightResult); &#125; 8.5 LeetCode上线段树的问题 303.区域和检索-不可变 给定一个整数数组nums，求出数组从索引i到j(i≤j)范围内元素的总和，包含i,j两点。 示例： 给定 nums = [-2, 0, 3, -5, 2, -1]，求和函数为 sumRange() sumRange(0, 2) -&gt; 1 sumRange(2, 5) -&gt; -1 sumRange(0, 5) -&gt; -3 说明: 你可以假设数组不可变。 会多次调用sumRange方法。 方法1：使用线段树 12345678910111213141516171819202122class NumArray &#123; private SegmentTree&lt;Integer&gt; segmentTree; public NumArray(int[] nums) &#123; if(nums.length &gt; 0)&#123; Integer[] data = new Integer[nums.length]; for (int i = 0; i &lt; nums.length; i++) data[i] = nums[i]; segmentTree = new SegmentTree&lt;&gt;(data, (a, b) -&gt; a + b); &#125; &#125; public int sumRange(int i, int j) &#123; if(segmentTree == null) throw new IllegalArgumentException(\"Segment Tree is null\"); return segmentTree.query(i, j); &#125;&#125; 方法2：不使用线段树 1234567891011121314151617public class NumArray2 &#123; private int[] sum; // sum[i]存储前i个元素和, sum[0] = 0 // 即sum[i]存储nums[0...i-1]的和 // sum(i, j) = sum[j + 1] - sum[i] public NumArray2(int[] nums) &#123; sum = new int[nums.length + 1]; sum[0] = 0; for(int i = 1 ; i &lt; sum.length ; i ++) sum[i] = sum[i - 1] + nums[i - 1]; &#125; public int sumRange(int i, int j) &#123; return sum[j + 1] - sum[i]; &#125;&#125; 当不涉及update时，方法2更简便，下面是涉及update的情况 307.区域和检索-不可变 给定一个整数数组nums，求出数组从索引i到j(i≤j)范围内元素的总和，包含i, j两点。 update(i,val) 函数可以通过将下标为i的数值更新为val，从而对数列进行修改。 示例: Given nums = [1, 3, 5] sumRange(0, 2) -&gt; 9 update(1, 2) sumRange(0, 2) -&gt; 8 说明: 数组仅可以在update函数下进行修改。 你可以假设update函数与sumRange函数的调用次数是均匀分布的。 不使用线段树实现 1234567891011121314151617181920212223242526class NumArray3 &#123; private int[] data; private int[] sum; public NumArray3(int[] nums) &#123; data = new int[nums.length]; for(int i = 0 ; i &lt; nums.length ; i ++) data[i] = nums[i]; sum = new int[nums.length + 1]; sum[0] = 0; for(int i = 1 ; i &lt;= nums.length ; i ++) sum[i] = sum[i - 1] + nums[i - 1]; &#125; public int sumRange(int i, int j) &#123; return sum[j + 1] - sum[i]; &#125; public void update(int index, int val) &#123; data[index] = val; for(int i = index + 1 ; i &lt; sum.length ; i ++) sum[i] = sum[i - 1] + data[i - 1]; &#125;&#125; 提交到LeetCode会报超出时间限制，update操作时间复杂度是O（n）级别的 8.6 线段树中的更新操作 在SegmentTree类中添加更新方法： 123456789101112131415161718192021222324252627282930// 将index位置的值，更新为e public void set(int index, E e)&#123; if(index &lt; 0 || index &gt;= data.length) throw new IllegalArgumentException(\"Index is illegal\"); data[index] = e; set(0, 0, data.length - 1, index, e); &#125; // 在以treeIndex为根的线段树中更新index的值为e private void set(int treeIndex, int l, int r, int index, E e)&#123; if(l == r)&#123; tree[treeIndex] = e; return; &#125; int mid = l + (r - l) / 2; // treeIndex的节点分为[l...mid]和[mid+1...r]两部分 int leftTreeIndex = leftChild(treeIndex); int rightTreeIndex = rightChild(treeIndex); if(index &gt;= mid + 1) set(rightTreeIndex, mid + 1, r, index, e); else // index &lt;= mid set(leftTreeIndex, l, mid, index, e); tree[treeIndex] = merger.merge(tree[leftTreeIndex], tree[rightTreeIndex]); &#125; 实现： 12345678910111213141516171819202122232425class NumArray &#123; private SegmentTree&lt;Integer&gt; segTree; public NumArray(int[] nums) &#123; if(nums.length != 0)&#123; Integer[] data = new Integer[nums.length]; for(int i = 0 ; i &lt; nums.length ; i ++) data[i] = nums[i]; segTree = new SegmentTree&lt;&gt;(data, (a, b) -&gt; a + b); &#125; &#125; public void update(int i, int val) &#123; if(segTree == null) throw new IllegalArgumentException(\"Error\"); segTree.set(i, val); &#125; public int sumRange(int i, int j) &#123; if(segTree == null) throw new IllegalArgumentException(\"Error\"); return segTree.query(i, j); &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第7章 优先队列和堆","date":"2019-04-22T02:06:09.000Z","path":"数据结构与算法/第7章 优先队列和堆/","text":"第7章 优先队列和堆 7-1 什么是优先队列 普通的队列是一种先进先出的数据结构，元素在队列尾追加，而从队列头删除。在优先队列中，元素被赋予优先级。当访问元素时，具有最高优先级的元素最先删除。优先队列具有最高级先出 (first in, largest out)的行为特征。通常采用堆数据结构来实现。 优先队列底层实现： 7-2 堆的基础表示 堆数据结构是一种数组对象，它可以被视为完全二叉树结构。它的特点是父节点的值大于（小于）两个子节点的值（分别称为大顶堆和小顶堆）。它常用于管理算法执行过程中的信息，应用场景包括堆排序，优先队列等。 最大堆 基于动态数组实现最大堆： 123456789101112131415161718192021222324252627282930313233343536373839public class MaxHeap&lt;E extends Comparable&lt;E&gt;&gt; &#123; private Array&lt;E&gt; data; public MaxHeap(int capacity)&#123; data = new Array&lt;&gt;(capacity); &#125; public MaxHeap()&#123; data = new Array&lt;&gt;(); &#125; // 返回堆中的元素个数 public int size()&#123; return data.getSize(); &#125; // 返回一个布尔值, 表示堆中是否为空 public boolean isEmpty()&#123; return data.isEmpty(); &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的父亲节点的索引 private int parent(int index)&#123; if(index == 0) throw new IllegalArgumentException(\"index-0 doesn't have parent.\"); return (index - 1) / 2; &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的左孩子节点的索引 private int leftChild(int index)&#123; return index * 2 + 1; &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的右孩子节点的索引 private int rightChild(int index)&#123; return index * 2 + 2; &#125;&#125; 7-3 向堆中添加元素和Sift Up 在动态数组中添加swap方法，当添加的元素大于父节点时，与父节点调换位置 123456789public void swap(int i, int j)&#123; if(i &lt; 0 || i &gt;= size || j &lt; 0 || j &gt;= size) throw new IllegalArgumentException(\"Index is illegal.\"); E t = data[i]; data[i] = data[j]; data[j] = t;&#125; 向堆中添加元素： 12345678910111213// 向堆中添加元素 public void add(E e)&#123; data.addLast(e); siftUp(data.getSize() - 1); &#125; private void siftUp(int k)&#123; while(k &gt; 0 &amp;&amp; data.get(parent(k)).compareTo(data.get(k)) &lt; 0 )&#123; data.swap(k, parent(k)); k = parent(k); &#125; &#125; 7-4 从堆中取出元素和Sift Down 12345678910111213141516171819202122232425262728// 取出堆中最大元素 public E extractMax()&#123; E ret = findMax(); data.swap(0, data.getSize() - 1); data.removeLast(); siftDown(0); return ret; &#125; private void siftDown(int k)&#123; while(leftChild(k) &lt; data.getSize())&#123; int j = leftChild(k); // 在此轮循环中,data[k]和data[j]交换位置 if( j + 1 &lt; data.getSize() &amp;&amp; data.get(j + 1).compareTo(data.get(j)) &gt; 0 ) j ++; // data[j] 是 leftChild 和 rightChild 中的最大值 if(data.get(k).compareTo(data.get(j)) &gt;= 0 ) break; data.swap(k, j); k = j; &#125; &#125; 7-5 Heapify 和 Replace Replace：取出堆中的最大元素，并且替换成元素e 1234567public E replace(E e)&#123; E ret = findMax(); data.set(0, e); siftDown(0); return ret;&#125; Heapify：将任意数组整理成堆的形状 将n个元素逐个插入到一个空堆中，算法复杂度是O(nlogn) heapify的过程，算法复杂度为O(n) 在动态数组类中添加方法： 123456public Array(E[] arr)&#123; data = (E[])new Object[arr.length]; for(int i = 0 ; i &lt; arr.length ; i ++) data[i] = arr[i]; size = arr.length;&#125; MaxHeap类中添加构造方法： 1234567public MaxHeap(E[] arr)&#123; data = new Array&lt;&gt;(arr); if(arr.length != 1)&#123; for(int i = parent(arr.length - 1) ; i &gt;= 0 ; i --) siftDown(i); &#125;&#125; 测试比较将n个元素逐个插入到一个空堆中和将任意数组变成堆的时间复杂度 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Main &#123; private static double testHeap(Integer[] testData, boolean isHeapify)&#123; long startTime = System.nanoTime(); MaxHeap&lt;Integer&gt; maxHeap; if(isHeapify) maxHeap = new MaxHeap&lt;&gt;(testData); else&#123; maxHeap = new MaxHeap&lt;&gt;(testData.length); for(int num: testData) maxHeap.add(num); &#125; int[] arr = new int[testData.length]; for(int i = 0 ; i &lt; testData.length ; i ++) arr[i] = maxHeap.extractMax(); for(int i = 1 ; i &lt; testData.length ; i ++) if(arr[i-1] &lt; arr[i]) throw new IllegalArgumentException(\"Error\"); System.out.println(\"Test MaxHeap completed.\"); long endTime = System.nanoTime(); return (endTime - startTime) / 1000000000.0; &#125; public static void main(String[] args) &#123; int n = 1000000; Random random = new Random(); Integer[] testData1 = new Integer[n]; for(int i = 0 ; i &lt; n ; i ++) testData1[i] = random.nextInt(Integer.MAX_VALUE); Integer[] testData2 = Arrays.copyOf(testData1, n); double time1 = testHeap(testData1, false); System.out.println(\"Without heapify: \" + time1 + \" s\"); double time2 = testHeap(testData2, true); System.out.println(\"With heapify: \" + time2 + \" s\"); &#125;&#125; 7-6 基于堆的优先队列 自定义队列的接口 1234567public interface Queue&lt;E&gt; &#123;int getSize();boolean isEmpty();void enqueue(E e);E dequeue();E getFront();&#125; 基于堆实现优先队列 123456789101112131415161718192021222324252627282930313233public class PriorityQueue&lt;E extends Comparable&lt;E&gt;&gt; implements Queue&lt;E&gt; &#123; private MaxHeap&lt;E&gt; maxHeap; public PriorityQueue()&#123; maxHeap = new MaxHeap&lt;&gt;(); &#125; @Override public int getSize()&#123; return maxHeap.size(); &#125; @Override public boolean isEmpty()&#123; return maxHeap.isEmpty(); &#125; @Override public E getFront()&#123; return maxHeap.findMax(); &#125; @Override public void enqueue(E e)&#123; maxHeap.add(e); &#125; @Override public E dequeue()&#123; return maxHeap.extractMax(); &#125;&#125; 7-7 Leetcode上优先队列相关问题 347.前K个高频元素 给定一个非空的整数数组，返回其中出现频率前k高的元素。 示例 1: 输入: nums = [1,1,1,2,2,3], k = 2 输出: [1,2] 示例 2: 输入: nums = [1], k = 1 输出: [1] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378import java.util.LinkedList;import java.util.List;import java.util.TreeMap;class Solution &#123; private class Array&lt;E&gt; &#123; private E[] data; private int size; // 构造函数，传入数组的容量capacity构造Array public Array(int capacity)&#123; data = (E[])new Object[capacity]; size = 0; &#125; // 无参数的构造函数，默认数组的容量capacity=10 public Array()&#123; this(10); &#125; public Array(E[] arr)&#123; data = (E[])new Object[arr.length]; for(int i = 0 ; i &lt; arr.length ; i ++) data[i] = arr[i]; size = arr.length; &#125; // 获取数组的容量 public int getCapacity()&#123; return data.length; &#125; // 获取数组中的元素个数 public int getSize()&#123; return size; &#125; // 返回数组是否为空 public boolean isEmpty()&#123; return size == 0; &#125; // 在index索引的位置插入一个新元素e public void add(int index, E e)&#123; if(index &lt; 0 || index &gt; size) throw new IllegalArgumentException(\"Add failed. Require index &gt;= 0 and index &lt;= size.\"); if(size == data.length) resize(2 * data.length); for(int i = size - 1; i &gt;= index ; i --) data[i + 1] = data[i]; data[index] = e; size ++; &#125; // 向所有元素后添加一个新元素 public void addLast(E e)&#123; add(size, e); &#125; // 在所有元素前添加一个新元素 public void addFirst(E e)&#123; add(0, e); &#125; // 获取index索引位置的元素 public E get(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Get failed. Index is illegal.\"); return data[index]; &#125; // 修改index索引位置的元素为e public void set(int index, E e)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Set failed. Index is illegal.\"); data[index] = e; &#125; // 查找数组中是否有元素e public boolean contains(E e)&#123; for(int i = 0 ; i &lt; size ; i ++)&#123; if(data[i].equals(e)) return true; &#125; return false; &#125; // 查找数组中元素e所在的索引，如果不存在元素e，则返回-1 public int find(E e)&#123; for(int i = 0 ; i &lt; size ; i ++)&#123; if(data[i].equals(e)) return i; &#125; return -1; &#125; // 从数组中删除index位置的元素, 返回删除的元素 public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Remove failed. Index is illegal.\"); E ret = data[index]; for(int i = index + 1 ; i &lt; size ; i ++) data[i - 1] = data[i]; size --; data[size] = null; // loitering objects != memory leak if(size == data.length / 4 &amp;&amp; data.length / 2 != 0) resize(data.length / 2); return ret; &#125; // 从数组中删除第一个元素, 返回删除的元素 public E removeFirst()&#123; return remove(0); &#125; // 从数组中删除最后一个元素, 返回删除的元素 public E removeLast()&#123; return remove(size - 1); &#125; // 从数组中删除元素e public void removeElement(E e)&#123; int index = find(e); if(index != -1) remove(index); &#125; public void swap(int i, int j)&#123; if(i &lt; 0 || i &gt;= size || j &lt; 0 || j &gt;= size) throw new IllegalArgumentException(\"Index is illegal.\"); E t = data[i]; data[i] = data[j]; data[j] = t; &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format(\"Array: size = %d , capacity = %d\\n\", size, data.length)); res.append('['); for(int i = 0 ; i &lt; size ; i ++)&#123; res.append(data[i]); if(i != size - 1) res.append(\", \"); &#125; res.append(']'); return res.toString(); &#125; // 将数组空间的容量变成newCapacity大小 private void resize(int newCapacity)&#123; E[] newData = (E[])new Object[newCapacity]; for(int i = 0 ; i &lt; size ; i ++) newData[i] = data[i]; data = newData; &#125; &#125; private class MaxHeap&lt;E extends Comparable&lt;E&gt;&gt; &#123; private Array&lt;E&gt; data; public MaxHeap(int capacity)&#123; data = new Array&lt;&gt;(capacity); &#125; public MaxHeap()&#123; data = new Array&lt;&gt;(); &#125; public MaxHeap(E[] arr)&#123; data = new Array&lt;&gt;(arr); for(int i = parent(arr.length - 1) ; i &gt;= 0 ; i --) siftDown(i); &#125; // 返回堆中的元素个数 public int size()&#123; return data.getSize(); &#125; // 返回一个布尔值, 表示堆中是否为空 public boolean isEmpty()&#123; return data.isEmpty(); &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的父亲节点的索引 private int parent(int index)&#123; if(index == 0) throw new IllegalArgumentException(\"index-0 doesn't have parent.\"); return (index - 1) / 2; &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的左孩子节点的索引 private int leftChild(int index)&#123; return index * 2 + 1; &#125; // 返回完全二叉树的数组表示中，一个索引所表示的元素的右孩子节点的索引 private int rightChild(int index)&#123; return index * 2 + 2; &#125; // 向堆中添加元素 public void add(E e)&#123; data.addLast(e); siftUp(data.getSize() - 1); &#125; private void siftUp(int k)&#123; while(k &gt; 0 &amp;&amp; data.get(parent(k)).compareTo(data.get(k)) &lt; 0 )&#123; data.swap(k, parent(k)); k = parent(k); &#125; &#125; // 看堆中的最大元素 public E findMax()&#123; if(data.getSize() == 0) throw new IllegalArgumentException(\"Can not findMax when heap is empty.\"); return data.get(0); &#125; // 取出堆中最大元素 public E extractMax()&#123; E ret = findMax(); data.swap(0, data.getSize() - 1); data.removeLast(); siftDown(0); return ret; &#125; private void siftDown(int k)&#123; while(leftChild(k) &lt; data.getSize())&#123; int j = leftChild(k); // 在此轮循环中,data[k]和data[j]交换位置 if( j + 1 &lt; data.getSize() &amp;&amp; data.get(j + 1).compareTo(data.get(j)) &gt; 0 ) j ++; // data[j] 是 leftChild 和 rightChild 中的最大值 if(data.get(k).compareTo(data.get(j)) &gt;= 0 ) break; data.swap(k, j); k = j; &#125; &#125; // 取出堆中的最大元素，并且替换成元素e public E replace(E e)&#123; E ret = findMax(); data.set(0, e); siftDown(0); return ret; &#125; &#125; private interface Queue&lt;E&gt; &#123; int getSize(); boolean isEmpty(); void enqueue(E e); E dequeue(); E getFront(); &#125; private class PriorityQueue&lt;E extends Comparable&lt;E&gt;&gt; implements Queue&lt;E&gt; &#123; private MaxHeap&lt;E&gt; maxHeap; public PriorityQueue()&#123; maxHeap = new MaxHeap&lt;&gt;(); &#125; @Override public int getSize()&#123; return maxHeap.size(); &#125; @Override public boolean isEmpty()&#123; return maxHeap.isEmpty(); &#125; @Override public E getFront()&#123; return maxHeap.findMax(); &#125; @Override public void enqueue(E e)&#123; maxHeap.add(e); &#125; @Override public E dequeue()&#123; return maxHeap.extractMax(); &#125; &#125; private class Freq implements Comparable&lt;Freq&gt;&#123; public int e, freq; public Freq(int e, int freq)&#123; this.e = e; this.freq = freq; &#125; @Override public int compareTo(Freq another)&#123; if(this.freq &lt; another.freq) return 1; else if(this.freq &gt; another.freq) return -1; else return 0; &#125; &#125; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; TreeMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;(); for(int num: nums)&#123; if(map.containsKey(num)) map.put(num, map.get(num) + 1); else map.put(num, 1); &#125; PriorityQueue&lt;Freq&gt; pq = new PriorityQueue&lt;&gt;(); for(int key: map.keySet())&#123; if(pq.getSize() &lt; k) pq.enqueue(new Freq(key, map.get(key))); else if(map.get(key) &gt; pq.getFront().freq)&#123; pq.dequeue(); pq.enqueue(new Freq(key, map.get(key))); &#125; &#125; LinkedList&lt;Integer&gt; res = new LinkedList&lt;&gt;(); while(!pq.isEmpty()) res.add(pq.dequeue().e); return res; &#125; private static void printList(List&lt;Integer&gt; nums)&#123; for(Integer num: nums) System.out.print(num + \" \"); System.out.println(); &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 1, 1, 2, 2, 3&#125;; int k = 2; printList((new Solution()).topKFrequent(nums, k)); &#125;&#125; 7-8 Java中的PriorityQueue 方法1： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import java.util.LinkedList;import java.util.List;import java.util.PriorityQueue;import java.util.TreeMap;public class Solution &#123; private class Freq implements Comparable&lt;Freq&gt;&#123; public int e, freq; public Freq(int e, int freq)&#123; this.e = e; this.freq = freq; &#125; public int compareTo(Freq another)&#123; if(this.freq &lt; another.freq) return -1; else if(this.freq &gt; another.freq) return 1; else return 0; &#125; &#125; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; TreeMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;(); for(int num: nums)&#123; if(map.containsKey(num)) map.put(num, map.get(num) + 1); else map.put(num, 1); &#125; PriorityQueue&lt;Freq&gt; pq = new PriorityQueue&lt;&gt;(); for(int key: map.keySet())&#123; if(pq.size() &lt; k) pq.add(new Freq(key, map.get(key))); else if(map.get(key) &gt; pq.peek().freq)&#123; pq.remove(); pq.add(new Freq(key, map.get(key))); &#125; &#125; LinkedList&lt;Integer&gt; res = new LinkedList&lt;&gt;(); while(!pq.isEmpty()) res.add(pq.remove().e); return res; &#125; private static void printList(List&lt;Integer&gt; nums)&#123; for(Integer num: nums) System.out.print(num + \" \"); System.out.println(); &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 1, 1, 2, 2, 3&#125;; int k = 2; printList((new Solution()).topKFrequent(nums, k)); &#125;&#125; 方法2： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.*;public class Solution2 &#123; private class Freq&#123; public int e, freq; public Freq(int e, int freq)&#123; this.e = e; this.freq = freq; &#125; &#125; private class FreqComparator implements Comparator&lt;Freq&gt;&#123; @Override public int compare(Freq a, Freq b)&#123; return a.freq - b.freq; &#125; &#125; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; TreeMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;(); for(int num: nums)&#123; if(map.containsKey(num)) map.put(num, map.get(num) + 1); else map.put(num, 1); &#125; PriorityQueue&lt;Freq&gt; pq = new PriorityQueue&lt;&gt;(new FreqComparator()); for(int key: map.keySet())&#123; if(pq.size() &lt; k) pq.add(new Freq(key, map.get(key))); else if(map.get(key) &gt; pq.peek().freq)&#123; pq.remove(); pq.add(new Freq(key, map.get(key))); &#125; &#125; LinkedList&lt;Integer&gt; res = new LinkedList&lt;&gt;(); while(!pq.isEmpty()) res.add(pq.remove().e); return res; &#125; private static void printList(List&lt;Integer&gt; nums)&#123; for(Integer num: nums) System.out.print(num + \" \"); System.out.println(); &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 1, 1, 2, 2, 3&#125;; int k = 2; printList((new Solution()).topKFrequent(nums, k)); &#125;&#125; 方法3： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import java.util.*;public class Solution3 &#123; private class Freq&#123; public int e, freq; public Freq(int e, int freq)&#123; this.e = e; this.freq = freq; &#125; &#125; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; TreeMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;(); for(int num: nums)&#123; if(map.containsKey(num)) map.put(num, map.get(num) + 1); else map.put(num, 1); &#125; PriorityQueue&lt;Freq&gt; pq = new PriorityQueue&lt;&gt;(new Comparator&lt;Freq&gt;() &#123; @Override public int compare(Freq a, Freq b) &#123; return a.freq - b.freq; &#125; &#125;); for(int key: map.keySet())&#123; if(pq.size() &lt; k) pq.add(new Freq(key, map.get(key))); else if(map.get(key) &gt; pq.peek().freq)&#123; pq.remove(); pq.add(new Freq(key, map.get(key))); &#125; &#125; LinkedList&lt;Integer&gt; res = new LinkedList&lt;&gt;(); while(!pq.isEmpty()) res.add(pq.remove().e); return res; &#125; private static void printList(List&lt;Integer&gt; nums)&#123; for(Integer num: nums) System.out.print(num + \" \"); System.out.println(); &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 1, 1, 2, 2, 3&#125;; int k = 2; printList((new Solution()).topKFrequent(nums, k)); &#125;&#125; 方法4： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.util.*;public class Solution4 &#123; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; TreeMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;(); for(int num: nums)&#123; if(map.containsKey(num)) map.put(num, map.get(num) + 1); else map.put(num, 1); &#125; PriorityQueue&lt;Integer&gt; pq = new PriorityQueue&lt;&gt;(new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer a, Integer b) &#123; return map.get(a) - map.get(b); &#125; &#125;); for(int key: map.keySet())&#123; if(pq.size() &lt; k) pq.add(key); else if(map.get(key) &gt; map.get(pq.peek()))&#123; pq.remove(); pq.add(key); &#125; &#125; LinkedList&lt;Integer&gt; res = new LinkedList&lt;&gt;(); while(!pq.isEmpty()) res.add(pq.remove()); return res; &#125; private static void printList(List&lt;Integer&gt; nums)&#123; for(Integer num: nums) System.out.print(num + \" \"); System.out.println(); &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 1, 1, 2, 2, 3&#125;; int k = 2; printList((new Solution()).topKFrequent(nums, k)); &#125;&#125; 方法5： 123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.util.*;public class Solution5 &#123; public List&lt;Integer&gt; topKFrequent(int[] nums, int k) &#123; TreeMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;(); for(int num: nums)&#123; if(map.containsKey(num)) map.put(num, map.get(num) + 1); else map.put(num, 1); &#125; PriorityQueue&lt;Integer&gt; pq = new PriorityQueue&lt;&gt;( (a, b) -&gt; map.get(a) - map.get(b) ); for(int key: map.keySet())&#123; if(pq.size() &lt; k) pq.add(key); else if(map.get(key) &gt; map.get(pq.peek()))&#123; pq.remove(); pq.add(key); &#125; &#125; LinkedList&lt;Integer&gt; res = new LinkedList&lt;&gt;(); while(!pq.isEmpty()) res.add(pq.remove()); return res; &#125; private static void printList(List&lt;Integer&gt; nums)&#123; for(Integer num: nums) System.out.print(num + \" \"); System.out.println(); &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 1, 1, 2, 2, 3&#125;; int k = 2; printList((new Solution()).topKFrequent(nums, k)); &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第6章 集合和映射","date":"2019-04-20T02:06:09.000Z","path":"数据结构与算法/第6章 集合和映射/","text":"第6章 集合和映射 6-1 集合基础和基于二分搜索树的集合实现 1.创建集合的接口 12345678public interface Set&lt;E&gt; &#123; void add(E e); boolean contains(E e); void remove(E e); int getSize(); boolean isEmpty();&#125; 2.基于二分搜索树实现集合 123456789101112131415161718192021222324252627282930313233public class BSTSet&lt;E extends Comparable&lt;E&gt;&gt; implements Set&lt;E&gt; &#123; private BST&lt;E&gt; bst; public BSTSet()&#123; bst = new BST&lt;&gt;(); &#125; @Override public int getSize()&#123; return bst.size(); &#125; @Override public boolean isEmpty()&#123; return bst.isEmpty(); &#125; @Override public void add(E e)&#123; bst.add(e); &#125; @Override public boolean contains(E e)&#123; return bst.contains(e); &#125; @Override public void remove(E e)&#123; bst.remove(e); &#125;&#125; 6-2 基于链表的集合实现 基于之前实现的LinkedList类 12345678910111213141516171819202122232425262728293031323334353637import java.util.ArrayList;public class LinkedListSet&lt;E&gt; implements Set&lt;E&gt; &#123; private LinkedList&lt;E&gt; list; public LinkedListSet()&#123; list = new LinkedList&lt;&gt;(); &#125; @Override public int getSize()&#123; return list.getSize(); &#125; @Override public boolean isEmpty()&#123; return list.isEmpty(); &#125; @Override public void add(E e)&#123; if(!list.contains(e)) list.addFirst(e); &#125; @Override public boolean contains(E e)&#123; return list.contains(e); &#125; @Override public void remove(E e)&#123; list.removeElement(e); &#125; &#125; 6-3 集合类的复杂度分析 h是二分搜索树的深度（层数），n是集合元素的个数 6-4 映射基础 映射是存储（键，值）数据对的数据结构（Key，Value） 根据键（Key），寻找值（Value） 非常容易使用链表或二分搜索树实现 有序映射一般用二分搜索树实现，无序映射一般用哈希表实现 定义映射的接口 12345678910public interface Map&lt;K, V&gt; &#123; void add(K key, V value); V remove(K key); boolean contains(K key); V get(K key); void set(K key, V newValue); int getSize(); boolean isEmpty();&#125; 6-5 基于链表的映射实现 基于映射实现词频统计，调用了FileOperation类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131public class LinkedListMap&lt;K, V&gt; implements Map&lt;K, V&gt; &#123; private class Node&#123; public K key; public V value; public Node next; public Node(K key, V value, Node next)&#123; this.key = key; this.value = value; this.next = next; &#125; public Node(K key, V value)&#123; this(key, value, null); &#125; public Node()&#123; this(null, null, null); &#125; @Override public String toString()&#123; return key.toString() + \" : \" + value.toString(); &#125; &#125; private Node dummyHead; private int size; public LinkedListMap()&#123; dummyHead = new Node(); size = 0; &#125; @Override public int getSize()&#123; return size; &#125; @Override public boolean isEmpty()&#123; return size == 0; &#125; private Node getNode(K key)&#123; Node cur = dummyHead.next; while(cur != null)&#123; if(cur.key.equals(key)) return cur; cur = cur.next; &#125; return null; &#125; @Override public boolean contains(K key)&#123; return getNode(key) != null; &#125; @Override public V get(K key)&#123; Node node = getNode(key); return node == null ? null : node.value; &#125; @Override public void add(K key, V value)&#123; Node node = getNode(key); if(node == null)&#123; dummyHead.next = new Node(key, value, dummyHead.next); size ++; &#125; else node.value = value; &#125; @Override public void set(K key, V newValue)&#123; Node node = getNode(key); if(node == null) throw new IllegalArgumentException(key + \" doesn't exist!\"); node.value = newValue; &#125; @Override public V remove(K key)&#123; Node prev = dummyHead; while(prev.next != null)&#123; if(prev.next.key.equals(key)) break; prev = prev.next; &#125; if(prev.next != null)&#123; Node delNode = prev.next; prev.next = delNode.next; delNode.next = null; size --; return delNode.value; &#125; return null; &#125; public static void main(String[] args)&#123; System.out.println(\"Pride and Prejudice\"); ArrayList&lt;String&gt; words = new ArrayList&lt;&gt;(); if(FileOperation.readFile(\"pride-and-prejudice.txt\", words)) &#123; System.out.println(\"Total words: \" + words.size()); LinkedListMap&lt;String, Integer&gt; map = new LinkedListMap&lt;&gt;(); for (String word : words) &#123; if (map.contains(word)) map.set(word, map.get(word) + 1); else map.add(word, 1); &#125; System.out.println(\"Total different words: \" + map.getSize()); System.out.println(\"Frequency of PRIDE: \" + map.get(\"pride\")); System.out.println(\"Frequency of PREJUDICE: \" + map.get(\"prejudice\")); &#125; System.out.println(); &#125;&#125; FileOperation类，输入文件名，返回包含文件中所有单词的链表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// 文件相关操作public class FileOperation &#123; // 读取文件名称为filename中的内容，并将其中包含的所有词语放进words中 public static boolean readFile(String filename, ArrayList&lt;String&gt; words)&#123; if (filename == null || words == null)&#123; System.out.println(\"filename is null or words is null\"); return false; &#125; // 文件读取 Scanner scanner; try &#123; File file = new File(filename); if(file.exists())&#123; FileInputStream fis = new FileInputStream(file); scanner = new Scanner(new BufferedInputStream(fis), \"UTF-8\"); scanner.useLocale(Locale.ENGLISH); &#125; else return false; &#125; catch(IOException ioe)&#123; System.out.println(\"Cannot open \" + filename); return false; &#125; // 简单分词 // 这个分词方式相对简陋, 没有考虑很多文本处理中的特殊问题 // 在这里只做demo展示用 if (scanner.hasNextLine()) &#123; String contents = scanner.useDelimiter(\"\\\\A\").next(); int start = firstCharacterIndex(contents, 0); for (int i = start + 1; i &lt;= contents.length(); ) if (i == contents.length() || !Character.isLetter(contents.charAt(i))) &#123; String word = contents.substring(start, i).toLowerCase(); words.add(word); start = firstCharacterIndex(contents, i); i = start + 1; &#125; else i++; &#125; return true; &#125; // 寻找字符串s中，从start的位置开始的第一个字母字符的位置 private static int firstCharacterIndex(String s, int start)&#123; for( int i = start ; i &lt; s.length() ; i ++ ) if( Character.isLetter(s.charAt(i)) ) return i; return s.length(); &#125;&#125; 6-6 基于二分搜索树的映射实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196public class BSTMap&lt;K extends Comparable&lt;K&gt;, V&gt; implements Map&lt;K, V&gt; &#123; private class Node&#123; public K key; public V value; public Node left, right; public Node(K key, V value)&#123; this.key = key; this.value = value; left = null; right = null; &#125; &#125; private Node root; private int size; public BSTMap()&#123; root = null; size = 0; &#125; @Override public int getSize()&#123; return size; &#125; @Override public boolean isEmpty()&#123; return size == 0; &#125; // 向二分搜索树中添加新的元素(key, value) @Override public void add(K key, V value)&#123; root = add(root, key, value); &#125; // 向以node为根的二分搜索树中插入元素(key, value)，递归算法 // 返回插入新节点后二分搜索树的根 private Node add(Node node, K key, V value)&#123; if(node == null)&#123; size ++; return new Node(key, value); &#125; if(key.compareTo(node.key) &lt; 0) node.left = add(node.left, key, value); else if(key.compareTo(node.key) &gt; 0) node.right = add(node.right, key, value); else // key.compareTo(node.key) == 0 node.value = value; return node; &#125; // 返回以node为根节点的二分搜索树中，key所在的节点 private Node getNode(Node node, K key)&#123; if(node == null) return null; if(key.equals(node.key)) return node; else if(key.compareTo(node.key) &lt; 0) return getNode(node.left, key); else // if(key.compareTo(node.key) &gt; 0) return getNode(node.right, key); &#125; @Override public boolean contains(K key)&#123; return getNode(root, key) != null; &#125; @Override public V get(K key)&#123; Node node = getNode(root, key); return node == null ? null : node.value; &#125; @Override public void set(K key, V newValue)&#123; Node node = getNode(root, key); if(node == null) throw new IllegalArgumentException(key + \" doesn't exist!\"); node.value = newValue; &#125; // 返回以node为根的二分搜索树的最小值所在的节点 private Node minimum(Node node)&#123; if(node.left == null) return node; return minimum(node.left); &#125; // 删除掉以node为根的二分搜索树中的最小节点 // 返回删除节点后新的二分搜索树的根 private Node removeMin(Node node)&#123; if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; node.left = removeMin(node.left); return node; &#125; // 从二分搜索树中删除键为key的节点 @Override public V remove(K key)&#123; Node node = getNode(root, key); if(node != null)&#123; root = remove(root, key); return node.value; &#125; return null; &#125; private Node remove(Node node, K key)&#123; if( node == null ) return null; if( key.compareTo(node.key) &lt; 0 )&#123; node.left = remove(node.left , key); return node; &#125; else if(key.compareTo(node.key) &gt; 0 )&#123; node.right = remove(node.right, key); return node; &#125; else&#123; // key.compareTo(node.key) == 0 // 待删除节点左子树为空的情况 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; // 待删除节点右子树为空的情况 if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; // 待删除节点左右子树均不为空的情况 // 找到比待删除节点大的最小节点, 即待删除节点右子树的最小节点 // 用这个节点顶替待删除节点的位置 Node successor = minimum(node.right); successor.right = removeMin(node.right); successor.left = node.left; node.left = node.right = null; return successor; &#125; &#125; public static void main(String[] args)&#123; System.out.println(\"Pride and Prejudice\"); ArrayList&lt;String&gt; words = new ArrayList&lt;&gt;(); if(FileOperation.readFile(\"pride-and-prejudice.txt\", words)) &#123; System.out.println(\"Total words: \" + words.size()); BSTMap&lt;String, Integer&gt; map = new BSTMap&lt;&gt;(); for (String word : words) &#123; if (map.contains(word)) map.set(word, map.get(word) + 1); else map.add(word, 1); &#125; System.out.println(\"Total different words: \" + map.getSize()); System.out.println(\"Frequency of PRIDE: \" + map.get(\"pride\")); System.out.println(\"Frequency of PREJUDICE: \" + map.get(\"prejudice\")); &#125; System.out.println(); &#125;&#125; 6-7 映射的时间复杂度分析 基于BSTMap实现的映射效率远远高于基于LinkedListMap实现的映射 1234567891011121314151617181920212223242526272829303132333435363738394041424344enter code herepublic class Main &#123; private static double testMap(Map&lt;String, Integer&gt; map, String filename)&#123; long startTime = System.nanoTime(); System.out.println(filename); ArrayList&lt;String&gt; words = new ArrayList&lt;&gt;(); if(FileOperation.readFile(filename, words)) &#123; System.out.println(\"Total words: \" + words.size()); for (String word : words)&#123; if(map.contains(word)) map.set(word, map.get(word) + 1); else map.add(word, 1); &#125; System.out.println(\"Total different words: \" + map.getSize()); System.out.println(\"Frequency of PRIDE: \" + map.get(\"pride\")); System.out.println(\"Frequency of PREJUDICE: \" + map.get(\"prejudice\")); &#125; long endTime = System.nanoTime(); return (endTime - startTime) / 1000000000.0; &#125; public static void main(String[] args) &#123; String filename = \"pride-and-prejudice.txt\"; BSTMap&lt;String, Integer&gt; bstMap = new BSTMap&lt;&gt;(); double time1 = testMap(bstMap, filename); System.out.println(\"BST Map: \" + time1 + \" s\"); System.out.println(); LinkedListMap&lt;String, Integer&gt; linkedListMap = new LinkedListMap&lt;&gt;(); double time2 = testMap(linkedListMap, filename); System.out.println(\"Linked List Map: \" + time2 + \" s\"); &#125;&#125; 6-8 LeetCode更多关于集合和映射的问题 349题，用集合求交集 123456789101112131415161718192021222324import java.util.ArrayList;import java.util.TreeSet;class Solution349 &#123; public int[] intersection(int[] nums1, int[] nums2) &#123; TreeSet&lt;Integer&gt; set = new TreeSet&lt;&gt;(); for(int num: nums1) set.add(num); ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for(int num: nums2)&#123; if(set.contains(num))&#123; list.add(num); set.remove(num); &#125; &#125; int[] res = new int[list.size()]; for(int i = 0 ; i &lt; list.size() ; i ++) res[i] = list.get(i); return res; &#125;&#125; 350题，用映射求交集 1234567891011121314151617181920212223242526272829303132import java.util.ArrayList;import java.util.TreeMap;public class Solution350 &#123; public int[] intersect(int[] nums1, int[] nums2) &#123; TreeMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;(); for(int num: nums1)&#123; if(!map.containsKey(num)) map.put(num, 1); else map.put(num, map.get(num) + 1); &#125; ArrayList&lt;Integer&gt; res = new ArrayList&lt;&gt;(); for(int num: nums2)&#123; if(map.containsKey(num))&#123; res.add(num); map.put(num, map.get(num) - 1); if(map.get(num) == 0) map.remove(num); &#125; &#125; int[] ret = new int[res.size()]; for(int i = 0 ; i &lt; res.size() ; i ++) ret[i] = res.get(i); return ret; &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第5章 二分搜索树","date":"2019-04-15T02:06:09.000Z","path":"数据结构与算法/第5章 二分搜索树/","text":"第5章 二分搜索树 5-1 为什么要研究树结构 树结构是一种天然的组织结构，比如电脑文件目录结构 将数据使用树结构存储后，出奇的高效 5-2 二分搜索树基础 1.最常用的树结构二叉树： 二叉树具有天然的递归结构，每个节点的左、右子树也是二叉树 二叉树不一定是“满”的，一个节点也是二叉树，空（null）也是二叉树 2.二分搜索树 5-3 二分搜索树添加新元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class BST&lt;E extends Comparable&lt;E&gt;&gt; &#123; private class Node &#123; public E e; public Node left, right; public Node(E e) &#123; this.e = e; left = null; right = null; &#125; &#125; private Node root; private int size; public BST()&#123; root = null; size = 0; &#125; public int size()&#123; return size; &#125; public boolean isEmpty()&#123; return size == 0; &#125; // 向二分搜索树中添加新的元素e public void add(E e)&#123; if(root == null)&#123; root = new Node(e); size ++; &#125; else add(root, e); &#125; // 向以node为根的二分搜索树中插入元素e，递归算法 private void add(Node node, E e)&#123; if(e.equals(node.e)) return; else if(e.compareTo(node.e) &lt; 0 &amp;&amp; node.left == null)&#123; node.left = new Node(e); size ++; return; &#125; else if(e.compareTo(node.e) &gt; 0 &amp;&amp; node.right == null)&#123; node.right = new Node(e); size ++; return; &#125; if(e.compareTo(node.e) &lt; 0) add(node.left, e); else //e.compareTo(node.e) &gt; 0 add(node.right, e); &#125;&#125; 5-4 改进添加操作：深入理解递归终止条件 1234567891011121314151617181920// 向二分搜索树中添加新的元素epublic void add(E e)&#123; root = add(root, e);&#125;// 向以node为根的二分搜索树中插入元素e，递归算法// 返回插入新节点后二分搜索树的根private Node add(Node node, E e)&#123; if(node == null)&#123; size ++; return new Node(e); &#125; if(e.compareTo(node.e) &lt; 0) node.left = add(node.left, e); else if(e.compareTo(node.e) &gt; 0) node.right = add(node.right, e); return node;&#125; 5-5 二分搜索树的查询操作 123456789101112131415161718// 看二分搜索树中是否包含元素epublic boolean contains(E e)&#123; return contains(root, e);&#125;// 看以node为根的二分搜索树中是否包含元素e, 递归算法private boolean contains(Node node, E e)&#123; if(node == null) return false; if(e.compareTo(node.e) == 0) return true; else if(e.compareTo(node.e) &lt; 0) return contains(node.left, e); else // e.compareTo(node.e) &gt; 0 return contains(node.right, e);&#125; 5-6 二分搜索树的遍历 二分搜索树的前序遍历，先访问节点，再访问左、右子树： 1234567891011121314151617181920212223242526272829303132333435363738394041// 二分搜索树的前序遍历public void preOrder()&#123; preOrder(root);&#125;// 前序遍历以node为根的二分搜索树, 递归算法private void preOrder(Node node)&#123; if(node == null) return; System.out.println(node.e); preOrder(node.left); preOrder(node.right);&#125;@Overridepublic String toString()&#123; StringBuilder res = new StringBuilder(); generateBSTString(root, 0, res); return res.toString();&#125;// 生成以node为根节点，深度为depth的描述二叉树的字符串private void generateBSTString(Node node, int depth, StringBuilder res)&#123; if(node == null)&#123; res.append(generateDepthString(depth) + \"null\\n\"); return; &#125; res.append(generateDepthString(depth) + node.e + \"\\n\"); generateBSTString(node.left, depth + 1, res); generateBSTString(node.right, depth + 1, res);&#125;private String generateDepthString(int depth)&#123; StringBuilder res = new StringBuilder(); for(int i = 0 ; i &lt; depth ; i ++) res.append(\"--\"); return res.toString();&#125; 测试： 12345678910111213141516171819202122public class Main &#123; public static void main(String[] args) &#123; BST&lt;Integer&gt; bst = new BST&lt;&gt;(); int[] nums = &#123;5, 3, 6, 8, 4, 2&#125;; for(int num: nums) bst.add(num); ///////////////// // 5 // // / \\ // // 3 6 // // / \\ \\ // // 2 4 8 // ///////////////// bst.preOrder(); System.out.println(); System.out.println(bst); &#125;&#125; 运行结果： 12345678910111213141516171819202122235324685--3----2------null------null----4------null------null--6----null----8------null------nullProcess finished with exit code 0 5-7 二分搜索树的中序遍历和后序遍历 1.二分搜索树的中序遍历，访问节点在访问左、右子树中间 1234567891011121314// 二分搜索树的中序遍历 public void inOrder()&#123; inOrder(root); &#125; // 中序遍历以node为根的二分搜索树, 递归算法 private void inOrder(Node node)&#123; if(node == null) return; inOrder(node.left); System.out.println(node.e); inOrder(node.right); &#125; 2.二分搜索树的后序遍历，访问节点在访问左、右子树后面 1234567891011121314// 二分搜索树的后序遍历public void postOrder()&#123; postOrder(root);&#125;// 后序遍历以node为根的二分搜索树, 递归算法private void postOrder(Node node)&#123; if(node == null) return; postOrder(node.left); postOrder(node.right); System.out.println(node.e);&#125; 3.测试： 12345678910111213141516171819202122public class Main &#123; public static void main(String[] args) &#123; BST&lt;Integer&gt; bst = new BST&lt;&gt;(); int[] nums = &#123;5, 3, 6, 8, 4, 2&#125;; for(int num: nums) bst.add(num); ///////////////// // 5 // // / \\ // // 3 6 // // / \\ \\ // // 2 4 8 // ///////////////// bst.inOrder(); System.out.println(); bst.postOrder(); &#125;&#125; 中序遍历运行结果是顺序排列的，后续遍历应用，内存释放 123456789101112131415234568243865Process finished with exit code 0 5-8 二分搜索树前序遍历的非递归实现 利用栈实现二分搜索树前序遍历 123456789101112131415161718// 二分搜索树的非递归前序遍历public void preOrderNR()&#123; if(root == null) return; Stack&lt;Node&gt; stack = new Stack&lt;&gt;(); stack.push(root); while(!stack.isEmpty())&#123; Node cur = stack.pop(); System.out.println(cur.e); if(cur.right != null) stack.push(cur.right); if(cur.left != null) stack.push(cur.left); &#125;&#125; 5-9 二分搜索树的层序遍历 利用队列实现二分搜索树层序遍历，层序遍历即按深度一层一层的遍历 123456789101112131415161718// 二分搜索树的层序遍历public void levelOrder()&#123; if(root == null) return; Queue&lt;Node&gt; q = new LinkedList&lt;&gt;(); q.add(root); while(!q.isEmpty())&#123; Node cur = q.remove(); System.out.println(cur.e); if(cur.left != null) q.add(cur.left); if(cur.right != null) q.add(cur.right); &#125;&#125; 层序遍历（广度优先遍历）的优点： 更快的找到问题的解 常用于算法设计-最短路径 5-10 删除二分搜索树的最大元素和最小元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576// 寻找二分搜索树的最小元素public E minimum()&#123; if(size == 0) throw new IllegalArgumentException(\"BST is empty\"); Node minNode = minimum(root); return minNode.e;&#125;// 返回以node为根的二分搜索树的最小值所在的节点private Node minimum(Node node)&#123; if( node.left == null ) return node; return minimum(node.left);&#125;// 寻找二分搜索树的最大元素public E maximum()&#123; if(size == 0) throw new IllegalArgumentException(\"BST is empty\"); return maximum(root).e;&#125;// 返回以node为根的二分搜索树的最大值所在的节点private Node maximum(Node node)&#123; if( node.right == null ) return node; return maximum(node.right);&#125;// 从二分搜索树中删除最小值所在节点, 返回最小值public E removeMin()&#123; E ret = minimum(); root = removeMin(root); return ret;&#125;// 删除掉以node为根的二分搜索树中的最小节点// 返回删除节点后新的二分搜索树的根private Node removeMin(Node node)&#123; if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; node.left = removeMin(node.left); return node;&#125;// 从二分搜索树中删除最大值所在节点public E removeMax()&#123; E ret = maximum(); root = removeMax(root); return ret;&#125;// 删除掉以node为根的二分搜索树中的最大节点// 返回删除节点后新的二分搜索树的根private Node removeMax(Node node)&#123; if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; node.right = removeMax(node.right); return node;&#125; 5-11 删除二分搜索树的任意元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 从二分搜索树中删除元素为e的节点public void remove(E e)&#123; root = remove(root, e);&#125;// 删除掉以node为根的二分搜索树中值为e的节点, 递归算法// 返回删除节点后新的二分搜索树的根private Node remove(Node node, E e)&#123; if( node == null ) return null; if( e.compareTo(node.e) &lt; 0 )&#123; node.left = remove(node.left , e); return node; &#125; else if(e.compareTo(node.e) &gt; 0 )&#123; node.right = remove(node.right, e); return node; &#125; else&#123; // e.compareTo(node.e) == 0 // 待删除节点左子树为空的情况 if(node.left == null)&#123; Node rightNode = node.right; node.right = null; size --; return rightNode; &#125; // 待删除节点右子树为空的情况 if(node.right == null)&#123; Node leftNode = node.left; node.left = null; size --; return leftNode; &#125; // 待删除节点左右子树均不为空的情况 // 找到比待删除节点大的最小节点, 即待删除节点右子树的最小节点 // 用这个节点顶替待删除节点的位置 Node successor = minimum(node.right); successor.right = removeMin(node.right); successor.left = node.left; node.left = node.right = null; return successor; &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第4章 链表和递归","date":"2019-04-10T02:06:09.000Z","path":"数据结构与算法/第4章 链表和递归/","text":"第4章 链表和递归 4-1 Leetcode中和链表相关的问题 Leetcode203题，删除链表中等于给定值 val 的所有节点。 示例: 输入: 1-&gt;2-&gt;6-&gt;3-&gt;4-&gt;5-&gt;6, val = 6 输出: 1-&gt;2-&gt;3-&gt;4-&gt;5 12345public class ListNode &#123; int val; ListNode next; ListNode(int x) &#123; val = x; &#125;&#125; 方法1：不使用虚拟头节点 123456789101112131415161718192021222324252627class Solution &#123; public ListNode removeElements(ListNode head, int val) &#123; while(head != null &amp;&amp; head.val == val)&#123; ListNode delNode = head; head = head.next; delNode.next = null; &#125; if(head == null) return head; ListNode prev = head; while(prev.next != null)&#123; if(prev.next.val == val) &#123; ListNode delNode = prev.next; prev.next = delNode.next; delNode.next = null; &#125; else prev = prev.next; &#125; return head; &#125;&#125; 方法1简化版 123456789101112131415161718192021class Solution2 &#123; public ListNode removeElements(ListNode head, int val) &#123; while(head != null &amp;&amp; head.val == val) head = head.next; if(head == null) return head; ListNode prev = head; while(prev.next != null)&#123; if(prev.next.val == val) prev.next = prev.next.next; else prev = prev.next; &#125; return head; &#125;&#125; 方法2：使用虚拟头节点 123456789101112131415161718class Solution3 &#123; public ListNode removeElements(ListNode head, int val) &#123; ListNode dummyHead = new ListNode(-1); dummyHead.next = head; ListNode prev = dummyHead; while(prev.next != null)&#123; if(prev.next.val == val) prev.next = prev.next.next; else prev = prev.next; &#125; return dummyHead.next; &#125;&#125; 4-2 测试自己的Leetcode链表代码 1.修改ListNode 1234567891011121314151617181920212223242526272829303132333435363738public class ListNode &#123; public int val; public ListNode next; public ListNode(int x) &#123; val = x; &#125; // 链表节点的构造函数 // 使用arr为参数，创建一个链表，当前的ListNode为链表头结点 public ListNode(int[] arr)&#123; if(arr == null || arr.length == 0) throw new IllegalArgumentException(\"arr can not be empty\"); this.val = arr[0]; ListNode cur = this; for(int i = 1 ; i &lt; arr.length ; i ++)&#123; cur.next = new ListNode(arr[i]); cur = cur.next; &#125; &#125; // 以当前节点为头结点的链表信息字符串 @Override public String toString()&#123; StringBuilder s = new StringBuilder(); ListNode cur = this; while(cur != null)&#123; s.append(cur.val + \"-&gt;\"); cur = cur.next; &#125; s.append(\"NULL\"); return s.toString(); &#125;&#125; 2.测试 12345678910111213141516171819202122232425262728class Solution &#123; public ListNode removeElements(ListNode head, int val) &#123; ListNode dummyHead = new ListNode(-1); dummyHead.next = head; ListNode prev = dummyHead; while(prev.next != null)&#123; if(prev.next.val == val) prev.next = prev.next.next; else prev = prev.next; &#125; return dummyHead.next; &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 2, 6, 3, 4, 5, 6&#125;; ListNode head = new ListNode(nums); System.out.println(head); ListNode res = (new Solution3()).removeElements(head, 6); System.out.println(res); &#125;&#125; 4-3 递归基础与递归的宏观语意 递归 本质上，将原来的问题，转化为更小的同一问题 举例：数组求和 12345678910111213141516171819public class Sum &#123; public static int sum(int[] arr)&#123; return sum(arr, 0); &#125; // 计算arr[l...n)这个区间内所有数字的和 private static int sum(int[] arr, int l)&#123; if(l == arr.length) return 0; return arr[l] + sum(arr, l + 1); &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 2, 3, 4, 5, 6, 7, 8&#125;; System.out.println(sum(nums)); &#125;&#125; 分析： 4-4 链表的天然递归结构性质 使用递归的方法解决LeetCode203题 方法1： 1234567891011121314151617181920212223242526272829class Solution &#123; public ListNode removeElements(ListNode head, int val) &#123; if(head == null) return head; //除头结点外，删除特定元素的链表res ListNode res = removeElements(head.next, val); //删除头结点，返回res if(head.val == val) return res; //不删除头结点，接上res else&#123; head.next = res; return head; &#125; &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 2, 6, 3, 4, 5, 6&#125;; ListNode head = new ListNode(nums); System.out.println(head); ListNode res = (new Solution4()).removeElements(head, 6); System.out.println(res); &#125;&#125; 方法2： 123456789101112131415161718192021class Solution &#123; public ListNode removeElements(ListNode head, int val) &#123; if(head == null) return head; head.next = removeElements(head.next, val); return head.val == val ? head.next : head; &#125; public static void main(String[] args) &#123; int[] nums = &#123;1, 2, 6, 3, 4, 5, 6&#125;; ListNode head = new ListNode(nums); System.out.println(head); ListNode res = (new Solution5()).removeElements(head, 6); System.out.println(res); &#125;&#125; 4-5 递归运行的机制：递归的微观解读 1.递归求和微观解读 2.递归删除微观解读","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第3章 最基础的动态数据结构：链表","date":"2019-04-06T02:06:09.000Z","path":"数据结构与算法/第3章 最基础的动态数据结构：链表/","text":"第3章 最基础的动态数据结构：链表 3-1 什么是链表 为什么链表很重要？ 最简单的动态数据结构 更深入的理解引用(或者指针) 更深入的理解递归 辅助组成其它数据结构 什么是链表？ 优点：真正的动态，不需要处理固定容量的问题 缺点：丧失了随机访问的能力（根据索引拿元素） 数组和链表的对比 数组最好用于索引有语意的情况，例如：scores[2] 最大的优点，支持快速查询 链表不适合用于索引有语意的情况 最大的优点：动态 3.2 在链表中添加元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class LinkedList&lt;E&gt; &#123; private class Node&#123; public E e; public Node next; public Node(E e, Node next)&#123; this.e = e; this.next = next; &#125; public Node(E e)&#123; this(e, null); &#125; public Node()&#123; this(null, null); &#125; @Override public String toString()&#123; return e.toString(); &#125; &#125; private Node head; private int size; public LinkedList()&#123; head = null; size = 0; &#125; // 获取链表中的元素个数 public int getSize()&#123; return size; &#125; // 返回链表是否为空 public boolean isEmpty()&#123; return size == 0; &#125; // 在链表头添加新的元素e public void addFirst(E e)&#123;// Node node = new Node(e);// node.next = head;// head = node; head = new Node(e, head); size ++; &#125; // 在链表的index(0-based)位置添加新的元素e // 在链表中不是一个常用的操作，练习用：） public void add(int index, E e)&#123; if(index &lt; 0 || index &gt; size) throw new IllegalArgumentException(\"Add failed. Illegal index.\"); if(index == 0) addFirst(e); else&#123; Node prev = head; for(int i = 0 ; i &lt; index - 1 ; i ++) prev = prev.next;// Node node = new Node(e);// node.next = prev.next;// prev.next = node; prev.next = new Node(e, prev.next); size ++; &#125; &#125; // 在链表末尾添加新的元素e public void addLast(E e)&#123; add(size, e); &#125;&#125; 代码实现并不优雅，因为在添加头结点的时候做了特殊处理，在下一小节解决这个问题 3.3 使用链表的虚拟头结点 使用虚拟头节点实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class LinkedList&lt;E&gt; &#123; private class Node&#123; public E e; public Node next; public Node(E e, Node next)&#123; this.e = e; this.next = next; &#125; public Node(E e)&#123; this(e, null); &#125; public Node()&#123; this(null, null); &#125; @Override public String toString()&#123; return e.toString(); &#125; &#125; private Node dummyHead; private int size; //使用虚拟头节点 public LinkedList()&#123; dummyHead = new Node(e：null，next null); size = 0; &#125; // 获取链表中的元素个数 public int getSize()&#123; return size; &#125; // 返回链表是否为空 public boolean isEmpty()&#123; return size == 0; &#125; // 在链表的index(0-based)位置添加新的元素e // 在链表中不是一个常用的操作，练习用：） public void add(int index, E e)&#123; if(index &lt; 0 || index &gt; size) throw new IllegalArgumentException(\"Add failed. Illegal index.\"); Node prev = dummyHead; for(int i = 0 ; i &lt; index ; i ++) prev = prev.next; prev.next = new Node(e, prev.next); size ++; &#125; // 在链表头添加新的元素e public void addFirst(E e)&#123; add(0, e); &#125; // 在链表末尾添加新的元素e public void addLast(E e)&#123; add(size, e); &#125;&#125; 3.4 链表的遍历，查询和修改 代码实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061 // 获得链表的第index(0-based)个位置的元素 // 在链表中不是一个常用的操作，练习用：） public E get(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Get failed. Illegal index.\"); Node cur = dummyHead.next; for(int i = 0 ; i &lt; index ; i ++) cur = cur.next; return cur.e; &#125; // 获得链表的第一个元素 public E getFirst()&#123; return get(0); &#125; // 获得链表的最后一个元素 public E getLast()&#123; return get(size - 1); &#125; // 修改链表的第index(0-based)个位置的元素为e // 在链表中不是一个常用的操作，练习用：） public void set(int index, E e)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Set failed. Illegal index.\"); Node cur = dummyHead.next; for(int i = 0 ; i &lt; index ; i ++) cur = cur.next; cur.e = e; &#125; // 查找链表中是否有元素e public boolean contains(E e)&#123; Node cur = dummyHead.next; while(cur != null)&#123; if(cur.e.equals(e)) return true; cur = cur.next; &#125; return false; &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder();// Node cur = dummyHead.next;// while(cur != null)&#123;// res.append(cur + \"-&gt;\");// cur = cur.next;// &#125; for(Node cur = dummyHead.next ; cur != null ; cur = cur.next) res.append(cur + \"-&gt;\"); res.append(\"NULL\"); return res.toString(); &#125; 测试： 1234567891011121314public class Test &#123; public static void main(String[] args) &#123; LinkedList&lt;Integer&gt; linkedList = new LinkedList&lt;Integer&gt;(); for(int i = 0 ; i &lt; 5 ; i ++)&#123; linkedList.addFirst(i); System.out.println(linkedList); &#125; linkedList.add(2, 666); System.out.println(linkedList); &#125;&#125; 运行结果： 123456780-&gt;NULL1-&gt;0-&gt;NULL2-&gt;1-&gt;0-&gt;NULL3-&gt;2-&gt;1-&gt;0-&gt;NULL4-&gt;3-&gt;2-&gt;1-&gt;0-&gt;NULL4-&gt;3-&gt;666-&gt;2-&gt;1-&gt;0-&gt;NULLProcess finished with exit code 0 3.5 从链表中删除元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 从链表中删除index(0-based)位置的元素, 返回删除的元素// 在链表中不是一个常用的操作，练习用：）public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Remove failed. Index is illegal.\"); Node prev = dummyHead; for(int i = 0 ; i &lt; index ; i ++) prev = prev.next; Node retNode = prev.next; prev.next = retNode.next; retNode.next = null; size --; return retNode.e;&#125;// 从链表中删除第一个元素, 返回删除的元素public E removeFirst()&#123; return remove(0);&#125;// 从链表中删除最后一个元素, 返回删除的元素public E removeLast()&#123; return remove(size - 1);&#125;// 从链表中删除元素epublic void removeElement(E e)&#123; Node prev = dummyHead; while(prev.next != null)&#123; if(prev.next.e.equals(e)) break; prev = prev.next; &#125; if(prev.next != null)&#123; Node delNode = prev.next; prev.next = delNode.next; delNode.next = null; size --; &#125;&#125; 时间复杂度分析： 链表不适合用于索引有语意的情况，增删改查时只能遍历 1.添加元素 如果要从末尾添加元素，由于不能通过索引查询，要依次遍历前面的节点 2.删除元素 修改和查找也是O（n）级别的。 链表适用于： 3.6 使用链表实现栈 1.定义栈的接口 1234567public interface Stack&lt;E&gt; &#123; int getSize(); boolean isEmpty(); void push(E e); //添加元素 E pop(); //取出元素 E peek(); //查看栈顶元素&#125; 2.实现链表栈 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class LinkedListStack&lt;E&gt; implements Stack&lt;E&gt; &#123; private LinkedList&lt;E&gt; list; public LinkedListStack()&#123; list = new LinkedList&lt;&gt;(); &#125; @Override public int getSize()&#123; return list.getSize(); &#125; @Override public boolean isEmpty()&#123; return list.isEmpty(); &#125; @Override public void push(E e)&#123; list.addFirst(e); &#125; @Override public E pop()&#123; return list.removeFirst(); &#125; @Override public E peek()&#123; return list.getFirst(); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(\"Stack: top \"); res.append(list); return res.toString(); &#125; public static void main(String[] args) &#123; LinkedListStack&lt;Integer&gt; stack = new LinkedListStack&lt;&gt;(); for(int i = 0 ; i &lt; 5 ; i ++)&#123; stack.push(i); System.out.println(stack); &#125; stack.pop(); System.out.println(stack); &#125;&#125; 3.测试结果 12345678Stack: top 0-&gt;NULLStack: top 1-&gt;0-&gt;NULLStack: top 2-&gt;1-&gt;0-&gt;NULLStack: top 3-&gt;2-&gt;1-&gt;0-&gt;NULLStack: top 4-&gt;3-&gt;2-&gt;1-&gt;0-&gt;NULLStack: top 3-&gt;2-&gt;1-&gt;0-&gt;NULLProcess finished with exit code 0 4.时间复杂度分析 123456789101112131415161718192021222324252627282930313233public class Test &#123; // 测试使用stack运行opCount个push和pop操作所需要的时间，单位：秒 private static double testStack(Stack&lt;Integer&gt; stack, int opCount)&#123; long startTime = System.nanoTime(); Random random = new Random(); for(int i = 0 ; i &lt; opCount ; i ++) stack.push(random.nextInt(Integer.MAX_VALUE)); for(int i = 0 ; i &lt; opCount ; i ++) stack.pop(); long endTime = System.nanoTime(); return (endTime - startTime) / 1000000000.0; &#125; public static void main(String[] args) &#123; int opCount = 100000; ArrayStack&lt;Integer&gt; arrayStack = new ArrayStack&lt;Integer&gt;(); double time1 = testStack(arrayStack, opCount); System.out.println(\"ArrayStack, time: \" + time1 + \" s\"); LinkedListStack&lt;Integer&gt; linkedListStack = new LinkedListStack&lt;Integer&gt;(); double time2 = testStack(linkedListStack, opCount); System.out.println(\"LinkedListStack, time: \" + time2 + \" s\"); // 其实这个时间比较很复杂，因为LinkedListStack中包含更多的new操作 &#125;&#125; 两者处于同一时间复杂度，但当添加的元素比较多时，LinkedListStack可能更耗时，因为包含更多的new操作（开辟节点空间）。 3.7 带有尾指针的链表：使用链表实现队列 1.定义接口 12345678public interface Queue&lt;E&gt; &#123; int getSize(); boolean isEmpty(); void enqueue(E e); E dequeue(); E getFront();&#125; 2.具体实现 实现没有虚拟头节点，因为没有从中间添加或删除元素，只在队尾添加元素，在队首删除元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108public class LinkedListQueue&lt;E&gt; implements Queue&lt;E&gt; &#123; private class Node&#123; public E e; public Node next; public Node(E e, Node next)&#123; this.e = e; this.next = next; &#125; public Node(E e)&#123; this(e, null); &#125; public Node()&#123; this(null, null); &#125; @Override public String toString()&#123; return e.toString(); &#125; &#125; private Node head, tail; private int size; public LinkedListQueue()&#123; head = null; tail = null; size = 0; &#125; @Override public int getSize()&#123; return size; &#125; @Override public boolean isEmpty()&#123; return size == 0; &#125; //在队尾添加一个元素 @Override public void enqueue(E e)&#123; if(tail == null)&#123; tail = new Node(e); head = tail; &#125; else&#123; tail.next = new Node(e); tail = tail.next; &#125; size ++; &#125; //在队首删除一个元素 @Override public E dequeue()&#123; if(isEmpty()) throw new IllegalArgumentException(\"Cannot dequeue from an empty queue.\"); Node retNode = head; head = head.next; retNode.next = null; if(head == null) tail = null; size --; return retNode.e; &#125; @Override public E getFront()&#123; if(isEmpty()) throw new IllegalArgumentException(\"Queue is empty.\"); return head.e; &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(\"Queue: front \"); Node cur = head; while(cur != null) &#123; res.append(cur + \"-&gt;\"); cur = cur.next; &#125; res.append(\"NULL tail\"); return res.toString(); &#125; public static void main(String[] args)&#123; LinkedListQueue&lt;Integer&gt; queue = new LinkedListQueue&lt;Integer&gt;(); for(int i = 0 ; i &lt; 10 ; i ++)&#123; queue.enqueue(i); System.out.println(queue); if(i % 3 == 2)&#123; queue.dequeue(); System.out.println(queue); &#125; &#125; &#125;&#125; 3.测试结果 123456789101112131415Queue: front 0-&gt;NULL tailQueue: front 0-&gt;1-&gt;NULL tailQueue: front 0-&gt;1-&gt;2-&gt;NULL tailQueue: front 1-&gt;2-&gt;NULL tailQueue: front 1-&gt;2-&gt;3-&gt;NULL tailQueue: front 1-&gt;2-&gt;3-&gt;4-&gt;NULL tailQueue: front 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL tailQueue: front 2-&gt;3-&gt;4-&gt;5-&gt;NULL tailQueue: front 2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;NULL tailQueue: front 2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;7-&gt;NULL tailQueue: front 2-&gt;3-&gt;4-&gt;5-&gt;6-&gt;7-&gt;8-&gt;NULL tailQueue: front 3-&gt;4-&gt;5-&gt;6-&gt;7-&gt;8-&gt;NULL tailQueue: front 3-&gt;4-&gt;5-&gt;6-&gt;7-&gt;8-&gt;9-&gt;NULL tailProcess finished with exit code 0 与循环队列一样，在队首删除一个元素，时间复杂度也是O（1）级别的。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第2章 栈和队列","date":"2019-03-28T13:06:09.000Z","path":"数据结构与算法/第2章 栈和队列/","text":"第2章 栈和队列 2.1 栈和栈的应用：撤销操作和系统栈 概述 栈也是一种线性结构 相比数组，栈对应的操作是数组的子集 只能从一端添加元素，也只能从一端取出元素 这一端称为栈顶 栈是一种后进先出的数据结构 在计算机的世界里，栈拥有着不可思议的作用 应用 无处不在的Undo操作（撤销） 程序调用的系统栈 2.2 栈的基本实现 利用之前实现的动态数组实现一个简单的栈 1.定义一个接口 1234567public interface Stack&lt;E&gt; &#123; int getSize(); boolean isEmpty(); void push(E e); //添加元素 E pop(); //取出元素 E peek(); //查看栈顶元素&#125; 2.实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class ArrayStack&lt;E&gt; implements Stack&lt;E&gt; &#123; private Array&lt;E&gt; array; public ArrayStack(int capacity)&#123; array = new Array&lt;E&gt;(capacity); &#125; public ArrayStack()&#123; array = new Array&lt;E&gt;(); &#125; //获取栈的元素个数 @Override public int getSize()&#123; return array.getSize(); &#125; //判断栈是否为空 @Override public boolean isEmpty()&#123; return array.isEmpty(); &#125; //因为是通过动态数组实现的，所以有容积 public int getCapacity()&#123; return array.getCapacity(); &#125; //添加元素 @Override public void push(E e)&#123; array.addLast(e); &#125; //取出元素 @Override public E pop()&#123; return array.removeLast(); &#125; //获取栈顶元素 @Override public E peek()&#123; return array.getLast(); &#125; //重写toString方法 @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(\"Stack: \"); res.append('['); for(int i = 0 ; i &lt; array.getSize() ; i ++)&#123; res.append(array.get(i)); if(i != array.getSize() - 1) res.append(\", \"); &#125; res.append(\"] top\"); return res.toString(); &#125;&#125; 3.测试 123456789101112131415public class TestApp &#123; public static void main(String[] args) &#123; ArrayStack&lt;Integer&gt; stack = new ArrayStack&lt;&gt;(); for(int i = 0 ; i &lt; 5 ; i ++)&#123; stack.push(i); System.out.println(stack); &#125; stack.pop(); System.out.println(stack); &#125;&#125; 运行结果： 12345678Stack: [0] topStack: [0, 1] topStack: [0, 1, 2] topStack: [0, 1, 2, 3] topStack: [0, 1, 2, 3, 4] topStack: [0, 1, 2, 3] topProcess finished with exit code 0 2.3 栈的另一个应用：括号匹配 LeetCode第20题： 给定一个只包括 ‘(’，’)’，’{’，’}’，’[’，’]’ 的字符串，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 注意空字符串可被认为是有效字符串。 栈顶元素反映了在嵌套的层次关系中，最近的需要匹配的元素，实现如下： 1234567891011121314151617181920212223242526272829import java.util.Stack;class Solution &#123; public boolean isValid(String s) &#123; Stack&lt;Character&gt; stack = new Stack&lt;&gt;(); for(int i = 0; i &lt; s.length(); i ++)&#123; char c = s.charAt(i); if (c == '(' || c == '[' || c == '&#123;') stack.push(c); else&#123; if (stack.empty())&#123; return false; &#125; //取出栈顶元素 char topChar = stack.pop(); if (c == ')' &amp;&amp; topChar != '(') return false; if (c == ']' &amp;&amp; topChar != '[') return false; if (c == '&#125;' &amp;&amp; topChar != '&#123;') return false; &#125; &#125; return stack.isEmpty(); &#125;&#125; 2.4 数组队列 队列也是一种线性结构 相比数组，队列对应的操作是数组的子集 只能从一端（队尾）添加元素，只能从一端（队首）取出元素 队列是一种先进先出的数据结构（先到先得） 利用之前实现的动态数组实现一个简单的数组队列 1.定义一个接口 1234567public interface Queue&lt;E&gt; &#123; int getSize(); boolean isEmpty(); void enqueue(E e); //向队尾添加元素 E dequeue(); //从队首取出第一个元素 E getFront(); //从队首拿到第一个元素&#125; 2.具体实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class ArrayQueue&lt;E&gt; implements Queue&lt;E&gt; &#123; private Array&lt;E&gt; array; public ArrayQueue(int capacity)&#123; array = new Array&lt;&gt;(capacity); &#125; public ArrayQueue()&#123; array = new Array&lt;&gt;(); &#125; @Override public int getSize()&#123; return array.getSize(); &#125; @Override public boolean isEmpty()&#123; return array.isEmpty(); &#125; public int getCapacity()&#123; return array.getCapacity(); &#125; @Override public void enqueue(E e)&#123; array.addLast(e); &#125; @Override public E dequeue()&#123; return array.removeFirst(); &#125; @Override public E getFront()&#123; return array.getFirst(); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(\"Queue: \"); res.append(\"front [\"); for(int i = 0 ; i &lt; array.getSize() ; i ++)&#123; res.append(array.get(i)); if(i != array.getSize() - 1) res.append(\", \"); &#125; res.append(\"] tail\"); return res.toString(); &#125; public static void main(String[] args) &#123; ArrayQueue&lt;Integer&gt; queue = new ArrayQueue&lt;&gt;(); for(int i = 0 ; i &lt; 10 ; i ++)&#123; queue.enqueue(i); System.out.println(queue); if(i % 3 == 2)&#123; queue.dequeue(); System.out.println(queue); &#125; &#125; &#125;&#125; 注：从队首取出一个元素时间复杂度是 O(n)级别的。 2.5 循环队列 1.类比钟表 2.具体实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109public class LoopQueue&lt;E&gt; implements Queue&lt;E&gt; &#123; private E[] data; private int front, tail; private int size; // 有兴趣的同学，在完成这一章后，可以思考一下： // LoopQueue中不声明size，如何完成所有的逻辑？ // 这个问题可能会比大家想象的要难一点点：） public LoopQueue(int capacity)&#123; data = (E[])new Object[capacity + 1]; front = 0; tail = 0; size = 0; &#125; public LoopQueue()&#123; this(10); &#125; public int getCapacity()&#123; return data.length - 1; &#125; @Override public boolean isEmpty()&#123; return front == tail; &#125; @Override public int getSize()&#123; return size; &#125; //循环队列的入队 @Override public void enqueue(E e)&#123; if((tail + 1) % data.length == front) resize(getCapacity() * 2); data[tail] = e; tail = (tail + 1) % data.length; size ++; &#125; //循环队列的出队 @Override public E dequeue()&#123; if(isEmpty()) throw new IllegalArgumentException(\"Cannot dequeue from an empty queue.\"); E ret = data[front]; data[front] = null; front = (front + 1) % data.length; size --; if(size == getCapacity() / 4 &amp;&amp; getCapacity() / 2 != 0) resize(getCapacity() / 2); return ret; &#125; //查看队首元素 @Override public E getFront()&#123; if(isEmpty()) throw new IllegalArgumentException(\"Queue is empty.\"); return data[front]; &#125; private void resize(int newCapacity)&#123; E[] newData = (E[])new Object[newCapacity + 1]; for(int i = 0 ; i &lt; size ; i ++) newData[i] = data[(i + front) % data.length]; data = newData; front = 0; tail = size; &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format(\"Queue: size = %d , capacity = %d\\n\", size, getCapacity())); res.append(\"front [\"); for(int i = front ; i != tail ; i = (i + 1) % data.length)&#123; res.append(data[i]); if((i + 1) % data.length != tail) res.append(\", \"); &#125; res.append(\"] tail\"); return res.toString(); &#125; public static void main(String[] args)&#123; LoopQueue&lt;Integer&gt; queue = new LoopQueue&lt;&gt;(); for(int i = 0 ; i &lt; 10 ; i ++)&#123; queue.enqueue(i); System.out.println(queue); if(i % 3 == 2)&#123; queue.dequeue(); System.out.println(queue); &#125; &#125; &#125;&#125; 注：循环队列从队首取出一个元素时间复杂度是 O(1)级别的。 2.5 数组队列和循环队列的比较 1.测试 向数组队列和循环队列分别插入和取出相同数量的元素，比较程序运行的时间 123456789101112131415161718192021222324252627282930313233public class TestApp &#123; // 测试使用q运行opCount个enqueueu和dequeue操作所需要的时间，单位：秒 private static double testQueue(Queue&lt;Integer&gt; q, int opCount)&#123; long startTime = System.nanoTime(); Random random = new Random(); for(int i = 0 ; i &lt; opCount ; i ++) q.enqueue(random.nextInt(Integer.MAX_VALUE)); for(int i = 0 ; i &lt; opCount ; i ++) q.dequeue(); long endTime = System.nanoTime(); return (endTime - startTime) / 1000000000.0; &#125; public static void main(String[] args) &#123; int opCount = 100000; //O(n²)级别 ArrayQueue&lt;Integer&gt; arrayQueue = new ArrayQueue&lt;Integer&gt;(); double time1 = testQueue(arrayQueue, opCount); System.out.println(\"ArrayQueue, time: \" + time1 + \" s\"); //O(n)级别 LoopQueue&lt;Integer&gt; loopQueue = new LoopQueue&lt;Integer&gt;(); double time2 = testQueue(loopQueue, opCount); System.out.println(\"LoopQueue, time: \" + time2 + \" s\"); &#125;&#125; 2.运行结果： 1234ArrayQueue, time: 4.2900581 sLoopQueue, time: 0.014636 sProcess finished with exit code 0","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":"第1章 不要小瞧数组","date":"2019-03-21T02:06:09.000Z","path":"数据结构与算法/第1章 不要小瞧数组/","text":"第1章 不要小瞧数组 1.1 使用Java中的数组 示例： 12345678910111213141516171819202122public static void main(String[] args) &#123; //创建一个数组，里面可以装10个元素 int[] arr = new int[10]; for(int i = 0 ; i &lt; arr.length ; i ++) arr[i] = i; //数组在声明的时候就设置了初始值， int[] scores = new int[]&#123;100, 99, 66&#125;; //方法1 for(int i = 0 ; i &lt; scores.length ; i ++) System.out.println(scores[i]); //方法2，使用增强for循环foreach for(int score: scores) System.out.println(score); //修改第一个元素 scores[0] = 96; for(int i = 0 ; i &lt; scores.length ; i ++) System.out.println(scores[i]); &#125; 1.2 二次封装属于我们自己的数组 基于Java数组，二次封装我们自己的数组类 12345678910111213141516171819202122232425262728293031public class Array &#123; private int[] data; private int size; // 构造函数，传入数组的容量capacity构造Array public Array(int capacity)&#123; data = new int[capacity]; size = 0; &#125; // 无参数的构造函数，默认数组的容量capacity=10 public Array()&#123; this(10); &#125; // 获取数组的容量 public int getCapacity()&#123; return data.length; &#125; // 获取数组中的元素个数 public int getSize()&#123; return size; &#125; // 返回数组是否为空 public boolean isEmpty()&#123; return size == 0; &#125;&#125; 1.3 向数组中添加元素 代码如下： 1234567891011121314151617181920212223242526272829303132 // 向所有元素后添加一个新元素 public void addLast(int e)&#123;// if(size == data.length) &#123;// throw new IllegalArgumentException(\"AddLast failed. Array is full.\");// &#125;// data[size] = e;// size ++; add(size, e); &#125; // 在所有元素前添加一个新元素 public void addFirst(int e)&#123; add(0, e); &#125; // 在index索引的位置插入一个新元素e public void add(int index, int e)&#123; if(size == data.length) throw new IllegalArgumentException(\"Add failed. Array is full.\"); if(index &lt; 0 || index &gt; size) throw new IllegalArgumentException(\"Add failed. Require index &gt;= 0 and index &lt;= size.\"); for(int i = size - 1; i &gt;= index ; i --) data[i + 1] = data[i]; data[index] = e; size ++; &#125; 1.4 在数组中查询元素和修改元素 1234567891011121314151617181920212223242526272829// 获取index索引位置的元素public int get(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Get failed. Index is illegal.\"); return data[index];&#125;// 修改index索引位置的元素为epublic void set(int index, int e)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Set failed. Index is illegal.\"); data[index] = e;&#125;// 重写父类的toString方法@Overridepublic String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format(\"Array: size = %d , capacity = %d\\n\", size, data.length)); res.append('['); for(int i = 0 ; i &lt; size ; i ++)&#123; res.append(data[i]); if(i != size - 1) res.append(\", \"); &#125; res.append(']'); return res.toString();&#125; 测试： 123456789101112131415public class Main &#123; public static void main(String[] args) &#123; Array arr = new Array(20); for (int i = 0; i &lt; 10; i++) arr.addLast(i); System.out.println(arr); System.out.println(arr.get(2)); arr.set(2,100); System.out.println(arr); &#125;&#125; 结果： 1234567Array: size = 10 , capacity = 20[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]2Array: size = 10 , capacity = 20[0, 1, 100, 3, 4, 5, 6, 7, 8, 9]Process finished with exit code 0 1.5 数组中的包含，搜索和删除元素 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 查找数组中是否有元素epublic boolean contains(int e)&#123; for(int i = 0 ; i &lt; size ; i ++)&#123; if(data[i] == e) return true; &#125; return false;&#125;// 查找数组中元素e所在的索引，如果不存在元素e，则返回-1public int find(int e)&#123; for(int i = 0 ; i &lt; size ; i ++)&#123; if(data[i] == e) return i; &#125; return -1;&#125;// 从数组中删除index位置的元素, 返回删除的元素public int remove(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Remove failed. Index is illegal.\"); int ret = data[index]; for(int i = index + 1 ; i &lt; size ; i ++) data[i - 1] = data[i]; size --; return ret;&#125;// 从数组中删除第一个元素, 返回删除的元素public int removeFirst()&#123; return remove(0);&#125;// 从数组中删除最后一个元素, 返回删除的元素public int removeLast()&#123; return remove(size - 1);&#125;// 从数组中删除元素epublic void removeElement(int e)&#123; int index = find(e); if(index != -1) remove(index);&#125; 1.6 使用泛型 上面实现的数组类只能承载int型的变量，但是实际上我们的数组类作为一个存放数据的容器，应该能够存放任意类型的数据，才足够的实用，下面我们使用泛型，来让数据结构可以放置&quot;任何&quot;数据类型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135public class Array&lt;E&gt; &#123; private E[] data; private int size; // 构造函数，传入数组的容量capacity构造Array public Array(int capacity)&#123; data = (E[])new Object[capacity]; size = 0; &#125; // 无参数的构造函数，默认数组的容量capacity=10 public Array()&#123; this(10); &#125; // 获取数组的容量 public int getCapacity()&#123; return data.length; &#125; // 获取数组中的元素个数 public int getSize()&#123; return size; &#125; // 返回数组是否为空 public boolean isEmpty()&#123; return size == 0; &#125; // 在index索引的位置插入一个新元素e public void add(int index, E e)&#123; if(size == data.length) throw new IllegalArgumentException(\"Add failed. Array is full.\"); if(index &lt; 0 || index &gt; size) throw new IllegalArgumentException(\"Add failed. Require index &gt;= 0 and index &lt;= size.\"); for(int i = size - 1; i &gt;= index ; i --) data[i + 1] = data[i]; data[index] = e; size ++; &#125; // 向所有元素后添加一个新元素 public void addLast(E e)&#123; add(size, e); &#125; // 在所有元素前添加一个新元素 public void addFirst(E e)&#123; add(0, e); &#125; // 获取index索引位置的元素 public E get(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Get failed. Index is illegal.\"); return data[index]; &#125; // 修改index索引位置的元素为e public void set(int index, E e)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Set failed. Index is illegal.\"); data[index] = e; &#125; // 查找数组中是否有元素e public boolean contains(E e)&#123; for(int i = 0 ; i &lt; size ; i ++)&#123; if(data[i].equals(e)) return true; &#125; return false; &#125; // 查找数组中元素e所在的索引，如果不存在元素e，则返回-1 public int find(E e)&#123; for(int i = 0 ; i &lt; size ; i ++)&#123; if(data[i].equals(e)) return i; &#125; return -1; &#125; // 从数组中删除index位置的元素, 返回删除的元素 public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Remove failed. Index is illegal.\"); E ret = data[index]; for(int i = index + 1 ; i &lt; size ; i ++) data[i - 1] = data[i]; size --; data[size] = null; // loitering objects != memory leak return ret; &#125; // 从数组中删除第一个元素, 返回删除的元素 public E removeFirst()&#123; return remove(0); &#125; // 从数组中删除最后一个元素, 返回删除的元素 public E removeLast()&#123; return remove(size - 1); &#125; // 从数组中删除元素e public void removeElement(E e)&#123; int index = find(e); if(index != -1) remove(index); &#125; @Override public String toString()&#123; StringBuilder res = new StringBuilder(); res.append(String.format(\"Array: size = %d , capacity = %d\\n\", size, data.length)); res.append('['); for(int i = 0 ; i &lt; size ; i ++)&#123; res.append(data[i]); if(i != size - 1) res.append(\", \"); &#125; res.append(']'); return res.toString(); &#125;&#125; 测试：放置自定义Student类型 12345678910111213141516171819202122232425 public class Student &#123; private String name; private int score; public Student(String studentName, int studentScore)&#123; name = studentName; score = studentScore; &#125; @Override public String toString()&#123; return String.format(\"Student(name: %s, score: %d)\", name, score); &#125; public static void main(String[] args) &#123; Array&lt;Student&gt; arr = new Array&lt;&gt;(); arr.addLast(new Student(\"Alice\", 100)); arr.addLast(new Student(\"Bob\", 66)); arr.addLast(new Student(\"Charlie\", 88)); System.out.println(arr); &#125;&#125; 运行结果： 1234Array: size = 3 , capacity = 10[Student(name: Alice, score: 100), Student(name: Bob, score: 66), Student(name: Charlie, score: 88)]Process finished with exit code 0 1.7 动态数组 在Array类中添加私有扩容方法 12345678// 将数组空间的容量变成newCapacity大小private void resize(int newCapacity)&#123; E[] newData = (E[])new Object[newCapacity]; for(int i = 0 ; i &lt; size ; i ++) newData[i] = data[i]; data = newData;&#125; 当数组中元素的数量大于数组的长度时，将数组的长度扩大为原来的2倍 12345678910111213141516// 在index索引的位置插入一个新元素epublic void add(int index, E e)&#123; if(index &lt; 0 || index &gt; size) throw new IllegalArgumentException(\"Add failed. Require index &gt;= 0 and index &lt;= size.\"); if(size == data.length) resize(2 * data.length); for(int i = size - 1; i &gt;= index ; i --) data[i + 1] = data[i]; data[index] = e; size ++;&#125; 当数组中元素的数量小于数组的长度的一半时，将数组的长度缩减为原来的1/2 123456789101112131415 // 从数组中删除index位置的元素, 返回删除的元素 public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Remove failed. Index is illegal.\"); E ret = data[index]; for(int i = index + 1 ; i &lt; size ; i ++) data[i - 1] = data[i]; size --; data[size] = null; // loitering objects != memory leak if(size == data.length / 2) resize(data.length / 2); return ret;&#125; 运行结果： 1234567891011121314Array: size = 10 , capacity = 10[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]Array: size = 11 , capacity = 20[0, 100, 1, 2, 3, 4, 5, 6, 7, 8, 9]Array: size = 12 , capacity = 20[-1, 0, 100, 1, 2, 3, 4, 5, 6, 7, 8, 9]Array: size = 11 , capacity = 20[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]Array: size = 10 , capacity = 10[-1, 0, 1, 2, 3, 5, 6, 7, 8, 9]Array: size = 9 , capacity = 10[0, 1, 2, 3, 5, 6, 7, 8, 9]Process finished with exit code 0 1.8 简单的时间复杂度分析 实际运行时间T与数组元素个数n的关系分析，以最坏情况分析 1.9 均摊复杂度和防止复杂度的震荡 对动态数组来说，以最坏情况分析，不可能每次添加元素都触发resize，稍微有些不合理，下面用均摊复杂度的方式分析： 防止复杂度的震荡及代码实现 如果add和remove交替进行，会导致反复的增容减容，引起复杂度的震荡 代码实现： 123456789101112131415 // 从数组中删除index位置的元素, 返回删除的元素 public E remove(int index)&#123; if(index &lt; 0 || index &gt;= size) throw new IllegalArgumentException(\"Remove failed. Index is illegal.\"); E ret = data[index]; for(int i = index + 1 ; i &lt; size ; i ++) data[i - 1] = data[i]; size --; data[size] = null; // loitering objects != memory leak if(size == data.length / 4 &amp;&amp; data.length / 2 != 0) //lazy resize(data.length / 2); return ret;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://zzuuriel.github.io/categories/数据结构与算法/"}],"tags":[]},{"title":" Hadoop架构详解 ","date":"2019-01-08T07:18:25.000Z","path":"大数据技术/3.Hadoop架构详解/","text":"Hadoop架构详解 Hadoop架构概述 Hadoop是一个能够对大量数据进行分布式处理的软件框架，以一种可靠、高效、可拓展的方式进行数据处理。Hadoop在设计之初就考虑了各种问题，包括，容错、处理大数据集、数据本地化、不同硬件和软件平台间的可移植性等等。 下面是Hadoop架构图： Hadoop具有主从（master-slave）拓扑结构。在这样的拓扑结构中，有一个主节点和多个从节点。主节点负责为各种从节点分配任务并管理资源，从节点完成实际的计算工作。从节点存储实际数据而主节点仅持有元数据，也就是存储关于数据的数据。 Hadoop包含3个主要的层，它们是： HDFS（Hadoop Distributed File System，Hadoop分布式文件系统） MapReduce Yarn HDFS HDFS表示Hadooop分布式文件系统，它为Hadoop提供数据存储能力。HDFS将数据切分成更小的单元（称为块），并且以分布式的方式进行存储。它有两个进程运行，一个在主节点上叫NameNode，另外一个在从节点上叫DataNode。 NameNode和DataNode HDFS具有主从架构，以管理者-工作者的模式运行。NameNode进程运行在master服务器上，它负责命名空间的管理以及控制客户端的文件访问。Namenode管理文件系统命名空间的修改，一些类似打开、关闭和重命名文件和目录的行为，NameNode也跟踪数据块到DataNode的映射。 DataNode进程运行在slave节点，它负责存储实际的业务数据。在系统内部，文件被分割成一定数量的数据块，并且存储到一组slave节点机器中。Datanode是文件系统的工作者，它们存储并提供定位块的服务（被用户或namenode调用时），并且定时的向namenode发送它们存储的块的列表。DataNode完成文件系统客户端请求的读写服务，DataNode也响应NameNode的需求完成数据块的创建、删除和复制工作。 HDFS的数据块 数据块是计算机上的最小存储单元，在Hadoop中，默认的数据块大小是128MB或者256MB。关于Hadoop的误解之一是认为较小的块(小于块大小)在文件系统中仍然会占用整个块。事实并非如此。较小的块只占用它们所需要大小的磁盘空间。 但这并不意味着大量小文件可以有效地利用 HDFS。无论块大小是多大，其元数据在 NameNode 中所占的内存完全相同。其结果是，数目众多的 HDFS 小文件(小于块大小)会占用大量的 NameNode 内存，从而给 HDFS 的可扩展性和性能带来负面影响。 在现实系统中，几乎不可能避免出现较小的HDFS块。比较大的可能性是某个给定的 HDFS文件会占用一些完整的块和一些较小的块。这会成为一个问题吗？考虑到大多数 HDFS文件会相当庞大，整个系统中此类较小块的数量将相对较少，因此通常没有问题。 复制管理 为了提供容错性，HDFS使用了复制技术。它建立几份数据块拷贝，并在不同的DataNode间进行存储。复制因子决定了数据块的拷贝数量。复制因子的默认值是3，当然你也可以配置成其他值。 假设我们有一个1GB的文件，由于复制因子是3，那么它实际需要3GB的存储空间。为了维持复制因子，NameNode会从每个DataNode中收集数据块报告。只要一个数据块的数量低于或者高于复制因子，NameNode就会添加或者删除相应的拷贝。 机架感知 HDFS 数据复制的最重要特性叫做机架感知，机架感知算法提供了低延迟和容错性。运行在计算机集群上的大型 HDFS 实例通常跨越许多个机架。通常情况下，相同机架上机器之间的网络带宽(以及与之相关联的网 络性能)远大于不同机架上机器之间的网络带宽。 NameNode 通过 Hadoop 机架感知进程确定各个 DataNode 所属的机架 ID。一种简单的策略是将各个副本分别放置于不同的机架上。这种策略能够在整个机架失效时防止数据丢失，且将副本均匀地分布到集群中。它也允许在读取数据时使用源自多个机架的带宽。但由于在这种情况下，写操作必须将块传输到多个机架上，因此写入性能会受影响。 机架感知策略的一个优化方案是让占用的机架数少于副本数，以减少跨机架写入流量 (进而提高写入性能)。例如，当复制因子为3时，将两个副本置于同一个机架上，并将第三个副本放在另一个不同的机架上。如果可能，在同一个机架上不会存储超过2个数据块。 MapReduce 同HDFS一样，Hadoop MapReduce也采用了Master/Slave(M/S)架构，具体如下图所示： 它主要由以下几个组件组成：Client、JobTracker、TaskTracker和Task。下面分别对这几个组件进行介绍。 Client 用户编写的MapReduce程序通过Client提交到JobTracker端；同时，用户可通过Client提供的一些接口查看作业运行状态。在Hadoop内部用作业（Job）表示MapReduce程序。一个MapReduce程序可对应若干个作业，而每个作业会被分解成若干个Map/Reduce(Task)。 JobTracker JobTracker主要负责资源监控和作业调度。JobTraker监控所有TaskTracker与作业的健康状况，一旦发现失败情况后，其会将相应的任务转移到其他节点；同时，JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。 TaskTracker TaskTracker会周期性地通过Heartbeat将本节点上的资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker发送的过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker使用&quot;slot&quot;等量划分本节点上的资源量。&quot;slot&quot;代表计算资源（CPU、内存等）。一个Task获取到一个slot后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map slot和Reduce slot两种，分别供Map Task和Reduce Task使用。TaskTracker通过slot数目（可配置参数）限定Task的并发度。 Task Task分为Map Task和Reduce Task两种，均由TaskTracker启动。HDFS以固定大小的block为基本单位存储数据，而对于MapReduce而言，其处理单位是split。split是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。split的多少决定了Map Task的数目，因为每个split会交由一个Map Task处理。split与block的对应关系如下图所示： Map Task先将对应的split迭代解析成一个个key/value对，依次调用用户自定义的map()函数进行处理，最终将临时结果存放到本地磁盘上，其中临时数据被分成若干个partition,每个partition将被一个Reduce Task处理。Map Task执行过程如下图所示： Reduce Task执行过程分为三个阶段： 1.从远程节点读取Map Task中间结果（称为&quot;Shuffle阶段&quot;）； 2.按照key对key/value进行排序(称为&quot;Sort阶段&quot;)； 3.依次读取&lt;key,value list&gt;,调用用户自定义的reduce()函数处理，并将最终结果存到HDFS上（称为&quot;Reduce阶段&quot;）。 Reduce Task执行过程如下图所示： YARN YARN（Yet Another Resource Negotiator）是Hadoop的资源管理层。YARN背后的基本原则是将资源管理和任务调度/监控功能分离成独立进程。YARN通过两类长期运行的守护进程提供核心服务：管理集群上资源使用的资源管理器Resource manager（RM），能够启动和监控容器（container）的节点管理器Node Manager（NM），container用于执行特定的应用程序进程。ResourceManager为系统中发生竞争的应用程序间仲裁资源，NodeManager的任务是监控容器对资源的占用情况，并且向ResourceManager进行汇报，这些资源指的是：CPU，内存，磁盘，网络等等。与HDFS一样，YARN也是master/slave结构，一个RM管理着多个NM。 YARN应用运行机制 YARN运行机制： 1.client首先联系Resource Manager，要求它运行一个Applcation Master（AM），Applcation Master负责和Resource Manager协调资源，并且和NodeManager一起执行作业，并且监控作业。； 2.Resource Manager找到一个能够在container中启动Applcation Master的Node Manager； 3.Applcation Master运行起来之后做什么依赖于应用本身，有可能在所处的container中简单地执行一个计算并将结果返回给客户端，或是像Resource Manager请求更多的container，用于运行一个分布式计算，例如MapReduce。 ResourceManager有两个重要的组件：Scheduler(调度器)和ApplicationManager（应用程序管理器）。 Scheduler Scheduler负责为各种应用程序指派资源。它是一个纯调度器，因为它并不对应用程序状态进行跟踪。对于那些发生软硬件故障的任务，它也不会再次为其调度资源。调度器根据应用程序的需求来分配资源。 ApplicationManager 应用程序管理器有如下功能： 1)接受作业提交 2)为执行中的ApplicationMaster协调第一个容器。容器由CPU，内存，磁盘及网络等元素构成。 3)失败时重启ApplicationMaster容器 通过YARN联盟（YARN Federation）特性，我们可以在几千个节点上扩展YARN。这个特性可以将多个YARN集群附属到一个更大型的集群，这样我们通过这样独立的集群拼凑起来完成更大型的作业。 YARN资源请求 YARN有一个灵活的资源请求模型，请求多个container时可以指定每个container的资源，还可以治container的本地限制要求。 本地化对于确保分布式数据处理算法高效使用集群带宽非常重要，本地限制有时也存在无法被满足的情况，例如一个Node Manager节点已经运行了别的container而无法再启动新的container，此时若有应用请求该节点，则YARN会尝试在同一机架的其他节点上启动一个container，如果还不行则会尝试集群中的任意节点。 YARN应用可以在运行中的任意时刻提出资源申请，可以在最开始提出所有的请求，或者为满足不断变化的应用需要，采取更为动态的方式在需要更多资源时提出请求。Spark采用的就是第一种方式，MapReduce采用的就是第二种方式，在最开始时申请map任务的container，后期再申请reduce任务的container，如果任何任务出现失败，会另外申请容器重新运行失败的任务。 YARN特性 YARN具有如下特性： 多租户模式（Multi-tenancy） YARN允许多种（开源或商业）访问引擎工作在同一Hadoop数据集上面。这些访问引擎可以以分批、实时或者迭代等方式处理数据。 充分利用集群 通过动态分配资源，YARN能够充分利用集群。相对于Hadoop以前版本的静态MapReduce规则，其集群利用度更高。 可缩放 任何处理功能强大的数据中心都要具备可扩展。YARN的ResourceManager关注调度并且应当每个扩展的集群，从容处理PB级的数据。 兼容性 为Hadoop1.x开发的MapReduce程序仍旧可以运行在YARN上面。而且不会影响已有作业的执行。 总结 Hadoop具有自我恢复能力，冗余的存储使其具备容错性，从而更加健壮。MapReduce遵从数据本地化处理原则，使计算向数据靠近，从而降低网络带宽。Hadoop的整个架构都是经济，可缩放而且高效的。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://zzuuriel.github.io/tags/Hadoop/"}]},{"title":" hadoop官网翻译:Setting up a Single Node Cluster","date":"2018-12-11T04:00:00.000Z","path":"大数据技术/1.single-noge-cluster/","text":"hadoop官网翻译:Setting up a Single Node Cluster. Hadoop: Setting up a Single Node Cluster. 设置单节点集群。 ▪ Purpose ▪ Prerequisites ▪ Supported Platforms ▪ Required Software ▪ Installing Software ▪ Download ▪ Prepare to Start the Hadoop Cluster ▪ Standalone Operation ▪ Pseudo-Distributed Operation ▪ Configuration ▪ Setup passphraseless ssh ▪ Execution ▪ YARN on a Single Node ▪ Fully-Distributed Operation ▪ 目的 先决条件 ▪ 支持平台 ▪ 所需软件 ▪ 安装软件 ▪ 下载 ▪ 准备开始安装Hadoop集群 ▪ 独立操作 ▪ 伪分布式操作 ▪ 配置 ▪ 设置无密码ssh ▪ 执行 ▪ YARN在单节点上运行 ▪ 全分布式操作 Purpose This document describes how to set up and configure a single-node Hadoop installation so that you can quickly perform simple operations using Hadoop MapReduce and the Hadoop Distributed File System (HDFS) 本文档描述了如何去建立并且配置一个单节点hadoop的安装，这样你就可以使用MR和HDFS快速执行简单的操作 Prerequisites Supported Platforms GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes. Windows is also a supported platform but the followings steps are for Linux only. To set up Hadoop on Windows, see wiki page． ▪ 支持GNU/Linux作为开发和生产平台。Hadoop已经在具有2000个节点的GNU/Linux集群上进行了演示。 ▪ Windows也是一个受支持的平台，但是下面的步骤只适用于Linux。要在Windows上配置Hadoop，请参阅wiki页面。 Required Software Required software for Linux include: 1.Java™ must be installed. Recommended Java versions are described at HadoopJavaVersions. 2.ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used. Additionally, it is recommmended that pdsh also be installed for better ssh resource management. linux所需的软件 1.必须安装java，在HadoopJavaVersions中查看适合的java版本 2.如果要使用可选的启动和停止脚本，则必须安装 ssh 并运行 sshd 以使用管理远程 hadoop 守护进程的 hadoop 脚本。 此外，还建议安装 pdsh，以便更好地管理 ssh 资源。 Installing Software If your cluster doesn’t have the requisite software you will need to install it. For example on Ubuntu Linux: 如果你的集群没有安装所需的软件，你需要安装它 例如在 Ubuntu Linux: 12 $ sudo apt-get install ssh $ sudo apt-get install pdsh Download To get a Hadoop distribution, download a recent stable release from one of the Apache Download Mirrors. 要获得Hadoop发行版，请从Apache下载镜像下载一个最新的稳定版本 Prepare to Start the Hadoop Cluster Unpack the downloaded Hadoop distribution. In the distribution, edit the file etc/hadoop/hadoop-env.sh to define some parameters as follows: 12 # set to the root of your Java installation export JAVA_HOME=/usr/java/latest Try the following command: 1 $ bin/hadoop This will display the usage documentation for the hadoop script. Now you are ready to start your Hadoop cluster in one of the three supported modes: Local (Standalone) Mode Pseudo-Distributed Mode Fully-Distributed Mode 解压缩下载的Hadoop发行版。在发行版中，编辑文件etc/hadoop/hadoop-env.sh以定义以下一些参数： 12 # set to the root of your Java installation export JAVA_HOME=/usr/java/latest 尝试以下命令： 1 $ bin/hadoop 这个会显示hadoop脚本的使用文档 现在，您已经准备好在三种支持的模式之一中启动Hadoop集群 ▪ 单机模式：默认情况下运行在一个单独机器上的独立Java进程，主要用于调试环境 ▪ 伪分布模式：在单个机器上模拟成分布式多节点环境，每一个Hadoop守护进程都作为一个独立的Java进程运行 ▪ 完全分布式模式：真实的生产环境，搭建在完全分布式的集群环境 Standalone Operation By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging. The following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory. 1234 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos; $ cat output/* 默认情况下，hadoop 被配置为以非分布式模式(作为单个 java 进程)运行。 这对调试很有用。 下面的示例复制未打包的conf目录作为输入，然后查找并显示给定正则表达式的每个匹配。输出被写入给定的输出目录。 1234 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos; $ cat output/* Pseudo-Distributed Operation Hadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process. Hadoop还可以在一个伪分布式模式的单个节点上运行，其中每个Hadoop守护进程在单独的Java进程中运行。 Configuration Use the following: 使用以下方法： etc/hadoop/core-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 常用配置项说明： ▪ fs.defaultFS这是默认的HDFS路径。当有多个HDFS集群同时工作时，用户在这里指定默认HDFS集群，该值来自于hdfs-site.xml中的配置。 ▪ fs.default.name这是一个描述集群中NameNode结点的URI(包括协议、主机名称、端口号)，集群里面的每一台机器都需要知道NameNode的地址。DataNode结点会先在NameNode上注册，这样它们的数据才可以被使用。独立的客户端程序通过这个URI跟DataNode交互，以取得文件的块列表。 ▪ hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配置namenode和datanode的存放位置，默认就放在/tmp/hadoop-${user.name}这个路径中。 更多说明请参考core-default.xml ，包含配置文件所有配置项的说明和默认值。 Setup passphraseless ssh Now check that you can ssh to the localhost without a passphrase: 现在检查是否可以在没有密码的情况下ssh到本地主机： 1 $ ssh localhost If you cannot ssh to localhost without a passphrase, execute the following commands: 如果没有密码无法ssh到localhost，请执行以下命令： 123 $ ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa $ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys Execution The following instructions are to run a MapReduce job locally. If you want to execute a job on YARN, see YARN on Single Node. 以下说明将在本地运行MapReduce作业。如果您想在YARN上执行作业，请参阅YARN on Single Node。 在使用hadoop前，必须格式化一个全新的HDFS安装，通过创建存储目录和NameNode持久化数据结构的初始版本，格式化过程创建了一个空的文件系统。由于NameNode管理文件系统的元数据，而DataNode可以动态的加入或离开集群，因此这个格式化过程并不涉及DataNode。同理，用户也无需关注文件系统的规模。集群中DataNode的数量决定着文件系统的规模。DataNode可以在文件系统格式化之后的很长一段时间内按需增加。 Format the filesystem: 格式化文件系统: 1 $ bin/hdfs namenode -format Start NameNode daemon and DataNode daemon: 启动NameNode和DataNode的守护进程: 1 $ sbin/start-dfs.sh The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs). hadoop守护进程日志输出被写入$HADOOP_LOG_DIR目录（默认为$HADOOP_HOME/logs）。 Browse the web interface for the NameNode; by default it is available at: ▪ NameNode - http://localhost:9870/ 浏览用于NameNode的网络接口；默认情况下，可以在：NameNode - http://localhost:9870/ Make the HDFS directories required to execute MapReduce jobs: 创建执行MapReduce作业所需的HDFS目录： 12 $ bin/hdfs dfs -mkdir /user $ bin/hdfs dfs -mkdir /user/&lt;username&gt; Copy the input files into the distributed filesystem: 将输入文件复制到分布式文件系统中： 12 $ bin/hdfs dfs -mkdir input $ bin/hdfs dfs -put etc/hadoop/*.xml input Run some of the examples provided: 运行所提供的一些示例： 1 $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos; Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them: 检查输出文件：将输出文件从分布式文件系统复制到本地文件系统，并检查它们： 12 $ bin/hdfs dfs -get output output $ cat output/* or View the output files on the distributed filesystem: 或者直接在分布式文件系统中查看： 1 $ bin/hdfs dfs -cat output/* When you’re done, stop the daemons with: 完成后，使用以下命令停止守护进程： 1 $ sbin/stop-dfs.sh YARN on a Single Node You can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition. The following instructions assume that 1. ~ 4. steps of the above instructions are already executed. 您可以通过设置一些参数并运行ResourceManager守护进程和NodeManager守护进程，在伪分布式模式下在YARN上运行MapReduce作业。 下面的指令假设已经执行上述指令1. ~ 4. 的步骤。 Configure parameters as follows: etc/hadoop/mapred-site.xml: 1234567 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;更多说明请参考mapred-default.xml，包含配置文件所有配置项的说明和默认值 etc/hadoop/yarn-site.xml: 12345678&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;注：yarn.nodemanager.aux-services通过该配置，用户可以自定义一些服务更多说明请参考yarn-default.xml，包含配置文件所有配置项的说明和默认值 Start ResourceManager daemon and NodeManager daemon: 启动ResourceManager守护进程和NodeManager守护进程： 1 $ sbin/start-yarn.sh Browse the web interface for the ResourceManager; by default it is available at: 浏览ResourceManager的网页界面；默认情况下，可以在： ResourceManager - http://localhost:8088/ Run a MapReduce job. When you’re done, stop the daemons with: 完成后，使用以下命令停止守护进程： 1 $ sbin/stop-yarn.sh Fully-Distributed Operation For information on setting up fully-distributed, non-trivial clusters see Cluster Setup. 有关全分布集群设置的信息，请参见Cluster Setup.","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://zzuuriel.github.io/tags/Hadoop/"}]},{"title":"查看、关闭登录到linux的终端","date":"2018-08-21T12:12:36.000Z","path":"Linux基础/查看、关闭登录到linux的终端/","text":"查看、关闭登录到linux的终端 基本概念 1.tty(终端设备的统称): tty一词源于Teletypes，原来指的是电传打字机，是通过串行线用打印机键盘阅读和发送信息的东西，后来这东西被键盘和显示器取代，所以现在叫终端比较合适。 终端是一种字符型设备，通常用tty来简称各种类型的终端设备。 2.pty（虚拟终端): 远程telnet到主机时不也需要一个终端交互么？是的，这就是虚拟终端pty(pseudo-tty) 虚拟终端是成对的逻辑终端设备，包含主从设备。 3.pts/ptmx(pts/ptmx结合使用，进而实现pty): pts(pseudo-terminal slave)是ptyde实现方法，与ptmx(pseudo-terminal master)配合使用实现pty。 Linux终端 在Linux系统的设备特殊文件目录/dev/下，终端特殊设备文件一般有以下几种： 1.串行端口终端(/dev/ttySn) 串行端口终端(Serial Port Terminal)是使用计算机串行端口连接的终端设备。计算机把每个串行端口都看作是一个字符设备。有段时间这些串行端口设备通常被称为终端设备，因为那时它的最大用途就是用来连接终端。这些串行端口所对应的设备名称是/dev/tts/0(或/dev/ttyS0), /dev/tts/1(或/dev/ttyS1)等，设备号分别是(4,0), (4,1)等，分别对应于DOS系统下的COM1、COM2等。若要向一个端口发送数据，可以在命令行上把标准输出重定向到这些特殊文件名上即可。例如，在命令行提示符下键入：echo test &gt; /dev/ttyS1会把单词”test”发送到连接在ttyS1(COM2)端口的设备上。可接串口来实验。 2.伪终端(/dev/pty/) 伪终端(Pseudo Terminal)是成对的逻辑终端设备(即master和slave设备, 对master的操作会反映到slave上)。 例如/dev/ptyp3和/dev/ttyp3(或者在设备文件系统中分别是/dev/pty/m3和 /dev/pty/s3)。它们与实际物理设备并不直接相关。如果一个程序把ptyp3(master设备)看作是一个串行端口设备，则它对该端口的读/ 写操作会反映在该逻辑终端设备对应的另一个ttyp3(slave设备)上面。而ttyp3则是另一个程序用于读写操作的逻辑设备。 这样，两个程序就可以通过这种逻辑设备进行互相交流，而其中一个使用ttyp3的程序则认为自己正在与一个串行端口进行通信。这很象是逻辑设备对之间的管道操作。对于ttyp3(s3)，任何设计成使用一个串行端口设备的程序都可以使用该逻辑设备。但对于使用ptyp3的程序，则需要专门设计来使用 ptyp3(m3)逻辑设备。 例如，如果某人在网上使用telnet程序连接到你的计算机上，则telnet程序就可能会开始连接到设备 ptyp2(m2)上(一个伪终端端口上)。此时一个getty程序就应该运行在对应的ttyp2(s2)端口上。当telnet从远端获取了一个字符时，该字符就会通过m2、s2传递给 getty程序，而getty程序就会通过s2、m2和telnet程序往网络上返回”login:”字符串信息。这样，登录程序与telnet程序就通过“伪终端”进行通信。通过使用适当的软件，就可以把两个甚至多个伪终端设备连接到同一个物理串行端口上。 在使用设备文件系统 (device filesystem)之前，为了得到大量的伪终端设备特殊文件，使用了比较复杂的文件名命名方式。因为只存在16个ttyp(ttyp0—ttypf) 的设备文件，为了得到更多的逻辑设备对，就使用了象q、r、s等字符来代替p。例如，ttys8和ptys8就是一个伪终端设备对。不过这种命名方式目前仍然在RedHat等Linux系统中使用着。 但Linux系统上的Unix98并不使用上述方法，而使用了”pty master”方式，例如/dev/ptm3。它的对应端则会被自动地创建成/dev/pts/3。这样就可以在需要时提供一个pty伪终端。目录 /dev/pts是一个类型为devpts的文件系统，并且可以在被加载文件系统列表中看到。虽然“文件”/dev/pts/3看上去是设备文件系统中的一项，但其实它完全是一种不同的文件系统。 即: TELNET —&gt; TTYP3(S3: slave) —&gt; PTYP3(M3: master) —&gt; GETTY 12345实验：1、在X下打开一个或N个终端窗口2、#ls /dev/pt*3、关闭这个X下的终端窗口，再次运行；比较两次输出信息就明白了。在RHEL4环境下: 输出为/dev/ptmx /dev/pts/1存在一(master)对多(slave)的情况 3.控制终端(/dev/tty) 如果当前进程有控制终端(Controlling Terminal)的话，那么/dev/tty就是当前进程的控制终端的设备特殊文件。可以使用命令”ps –ax”来查看进程与哪个控制终端相连。对于你登录的shell，/dev/tty就是你使用的终端，设备号是(5,0)。使用命令”tty”可以查看它具体对应哪个实际终端设备。/dev/tty有些类似于到实际所使用终端设备的一个联接。 4.控制台终端(/dev/ttyn, /dev/console) 在Linux 系统中，计算机显示器通常被称为控制台终端 (Console)。它仿真了类型为Linux的一种终端(TERM=Linux)，并且有一些设备特殊文件与之相关联：tty0、tty1、tty2 等。当你在控制台上登录时，使用的是tty1。使用Alt+[F1—F6]组合键时，我们就可以切换到tty2、tty3等上面去。tty1–tty6等称为虚拟终端，而tty0则是当前所使用虚拟终端的一个别名，系统所产生的信息会发送到该终端上。因此不管当前正在使用哪个虚拟终端，系统信息都会发送到控制台终端上。你可以登录到不同的虚拟终端上去，因而可以让系统同时有几个不同的会话期存在。只有系统或超级用户root可以向 /dev/tty0进行写操作 即下例： 12341.# tty(查看当前TTY)/dev/tty12.#echo \"test tty0\" &gt; /dev/tty0test tty0 5.虚拟终端(/dev/pts/n) 在Xwindows模式下的伪终端. 6.其它类型 Linux系统中还针对很多不同的字符设备存在有很多其它种类的终端设备特殊文件。例如针对ISDN设备的/dev/ttyIn终端设备等。这里不再赘述。 常用命令 平时通过SSH，telnet等登录到linux系统时，就会生成一个PTS表征某一虚拟终端的设备。 下面是相关常用的命令： 1.列出当前登录到系统的用户 命令：who 12345root@hadoop000:/# who root pts/0 2018-08-21 09:51 (192.168.3.151) root pts/1 2018-08-21 18:27 (192.168.3.151) root pts/2 2018-08-21 18:27 (192.168.3.151) root pts/3 2018-08-21 18:27 (192.168.3.151) 2.关闭某一登录到系统的用户 命令：fuser -k /dev/pts/用户ID 12root@hadoop000:/# fuser -k /dev/pts/3 /dev/pts/3: 9339 3.查看当前登录的用户 命令：tty 12root@hadoop000:/# tty /dev/pts/1","categories":[{"name":"Linux基础","slug":"Linux基础","permalink":"https://zzuuriel.github.io/categories/Linux基础/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zzuuriel.github.io/tags/Linux/"}]},{"title":"Linux用户和用户组","date":"2018-08-20T12:12:36.000Z","path":"Linux基础/Linux用户和用户组/","text":"Linux用户和用户组 添加用户 12345678910111213141516171819202122232425262728293031323334353637383940414243# 查看关于user的命令[hadoop@hadoop000 sh]$ ll /usr/sbin/user*-rwxr-x---. 1 root root 118192 Nov 6 2016 /usr/sbin/useradd-rwxr-x---. 1 root root 80360 Nov 6 2016 /usr/sbin/userdel-rws--x--x. 1 root root 40312 Jun 10 2014 /usr/sbin/userhelper-rwxr-x---. 1 root root 113840 Nov 6 2016 /usr/sbin/usermod-rwsr-xr-x. 1 root root 11288 Aug 4 2017 /usr/sbin/usernetctl# 查看关于group的命令[hadoop@hadoop000 sh]$ ll /usr/sbin/group*-rwxr-x---. 1 root root 65480 Nov 6 2016 /usr/sbin/groupadd-rwxr-x---. 1 root root 57016 Nov 6 2016 /usr/sbin/groupdel-rwxr-x---. 1 root root 57064 Nov 6 2016 /usr/sbin/groupmems-rwxr-x---. 1 root root 76424 Nov 6 2016 /usr/sbin/groupmod# 添加用户uriel[root@hadoop000 ~]# useradd uriel[root@hadoop000 ~]# id urieluid=1001(uriel) gid=1001(uriel) groups=1001(uriel)释义：创建一个普通用户uriel，默认创建用户的用户组uriel，且设置用户的主组为uriel，并且创建/home/uriel# 查看linux上的所有普通用户[root@hadoop000 ~]# cd /home[root@hadoop000 home]# lltotal 4drwxrwxr-x. 34 hadoop hadoop 4096 Aug 20 15:58 hadoopdrwx------ 3 uriel uriel 78 Aug 20 21:38 uriel# 添加uriel用户到另外一个组bigdata[root@hadoop000 home]# groupadd bigdata[root@hadoop000 home]# cat /etc/group | grep bigdatabigdata:x:1002:[root@hadoop000 home]# usermod -a -G bigdata uriel[root@hadoop000 home]# id urieluid=1001(uriel) gid=1001(uriel) groups=1001(uriel),1002(bigdata)# 修改bigdata为uriel的主组[root@hadoop000 home]# usermod --gid bigdata uriel[root@hadoop000 home]# id urieluid=1001(uriel) gid=1002(bigdata) groups=1002(bigdata)[root@hadoop000 home]# usermod -a -G uriel uriel[root@hadoop000 home]# id urieluid=1001(uriel) gid=1002(bigdata) groups=1002(bigdata),1001(uriel) 删除用户 123456789101112131415161718192021222324252627282930313233[root@hadoop000 home]# userdel uriel[root@hadoop000 home]# id urielid: uriel: no such user[root@hadoop000 home]# cat /etc/passwd | grep uriel[root@hadoop000 home]# cat /etc/group | grep uriel因为uriel用户组只有uriel用户，当这个用户删除时，组会校验就它自己，会自动删除但是/home下仍然有uriel文件夹[root@hadoop000 home]# lltotal 4drwxrwxr-x. 34 hadoop hadoop 4096 Aug 20 15:58 hadoopdrwx------ 3 1001 1001 78 Aug 20 21:26 uriel# 重新创建用户uriel[root@hadoop000 home]# useradd urieluseradd: warning: the home directory already exists.Not copying any file from skel directory into it.Creating mailbox file: File exists[root@hadoop000 home]# id urieluid=1001(uriel) gid=1001(uriel) groups=1001(uriel)[root@hadoop000 home]# lltotal 4drwxrwxr-x. 34 hadoop hadoop 4096 Aug 20 15:58 hadoopdrwx------ 3 uriel uriel 78 Aug 20 22:16 uriel# 此时切换至vinx用户时[root@hadoop000 home]# su - uriel[uriel@hadoop000 ~]$ # 处理切换用户出现-bash-4.2$的情况[root@hadoop000 vinx]# rm -rf .bash*[root@hadoop000 vinx]# cp /etc/skel/.* /home/uriel[root@hadoop000 home]# chown uriel:uriel uriel[root@hadoop000 vinx]# chown uriel:uriel .bash* 设置密码 123456[root@hadoop000 home]# passwd urielChanging password for user uriel.New password: BAD PASSWORD: The password is shorter than 8 charactersRetype new password: passwd: all authentication tokens updated successfully. 切换用户 1234su urielsu - uriel 切换至该用户的家目录，且执行环境变量文件.bash_profile su vinx不会执行，su - vinx执行.bashrc su vinx和su - vinx都执行 普通用户获取root的最大权限 12345vi /etc/sudoers# 在文件中添加root ALL=(ALL) ALLuriel ALL=(root) NOPASSWD:ALL切换至uriel用户，命令前使用sudo 权限 12345678910111213141516171819# 通用格式chmod -R 777 directory/filechown -R user:group directory/file-----------------------------------------------------------------------------drwxr-xr-x 2 root root 6 Aug 20 22:14 test-rw-r--r-- 1 root root 0 Aug 24 22:14 test.log第一个字母：d文件夹，-文件，l连接rwx r-x r-xr: read 4w: write 2x: 执行 1-: 没权限 07=rwx3=-wx5=r-xrwx 第一组 7 代表文件或文件夹的用户root的权限：读写执行r-x 第二组 5 代表文件或文件夹的用户组root的权限：读执行r-x 第三组 5 代表其他组的所属用户对这个文件或文件夹的权限：读执行777 代表任意的用户用户组，都读写执行 查看大小 12345# 文件lldu -sh xxx.log# 文件夹du -sh xxx 搜索 1234# 从根目录全局查找find / -name '*hadoop*'# 指定目录中查找find /usr/local -name '*hadoop*'","categories":[{"name":"Linux基础","slug":"Linux基础","permalink":"https://zzuuriel.github.io/categories/Linux基础/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zzuuriel.github.io/tags/Linux/"}]},{"title":"Linux常用命令&定位ERROR","date":"2018-08-19T12:12:36.000Z","path":"Linux基础/Linux常用命令&定位ERROR/","text":"Linux常用命令&amp;定位ERROR Linux常用命令 12345678910111213141516171819202122pwd 查看当前目录路径cd /home 切换到/home目录cd - 回退到上一次目录cd 切换到用户家目录cd ~ 同上cd ../ 回退到上一层目录cd ../../ 回退到上两层目录ls 查看当前目录的所有文件ll 查看所有文件，相当于ls -lll -a 查看所有文件，包括隐藏文件ll -h 查看所有文件，并显示文件大小ll -rt 查看所有文件，并按时间排序clear 清空屏幕ls --help 查看ls相关的命令帮助mkdir 创建文件夹mkdir -p a/b/c 级联创建文件夹mkdir a b c 创建多个文件夹mv 移动文件cp 拷贝文件touch test.log 创建一个新文件echo “” &gt; test.log 创建一个新文件（慎用，会产生一个字节）cat /dev/null &gt; test.log 把一个文件设置为空 如何定位ERROR 1.查看文件内容 cat 文件内容全部显示 more 文件内容一页一页的往下翻，按空格键翻页，按q退出 less 查看文件内容，按上下箭头往上下翻页，按q键退出 2.监控日志文件 tail -f test.log tail -F test.log ==&gt; -f + retry tail -300f test.log 实时监控倒数300行 3.定位ERROR 场景1：文件内容很小，几十M。 解决方法：下载到windows，使用编辑器查找ERROR 安装上传下载工具：yum install -y lrzsz 场景2：文件内容很大，几百M。 解决方法：cat test.log | grep ERROR 定位ERROR的3种命令： cat test.log | grep -A 10 ERROR 后10行 cat test.log | grep -B 10 ERROR 前10行 cat test.log | grep -C 30 ERROR 前后各30行 cat test.log | grep -C 30 ERROR &gt; error.log 错误信息写入到error.log文件","categories":[{"name":"Linux基础","slug":"Linux基础","permalink":"https://zzuuriel.github.io/categories/Linux基础/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zzuuriel.github.io/tags/Linux/"}]},{"title":"Linux vi模式&查看系统资源","date":"2018-08-17T12:12:36.000Z","path":"Linux基础/Linux vi模式&查看系统资源/","text":"Linux vi模式&amp;查看系统资源 vi命令 1.vi命令的三种模式 模拟流程：vi test.log进入命令模式，i键进入编辑模式，esc键退出编辑模式并进入命令模式，shift+:进入尾行模式，输入wq并回车保存并退出。 2.搜索 shift+:进入尾行模式后，/keyword，回车后自动匹配，N键寻找下一个 3.设置行号 进入尾行模式： set nu 显示行号 set nonu 取消行号显示 4.清空文件内容 cat /dev/null &gt; test.log echo ‘’ &gt; test.log 存在1个字节，慎用 5.命令模式常用快捷键 12345678910111213dd 删除当前行dG 删除光标当前及以下的所有行ndd 删除光标当前及以下的n行gg 跳转到第一行的第一个字母G 跳转到最后一行的第一个字母shift + $ 行尾gg + dG 清空文件 系统命令 1.查看系统资源 123456789101112131415161718192021222324252627282930313233# 磁盘[hadoop@hadoop000 ~]$ df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/centos-root 37G 23G 15G 62% /devtmpfs 1.9G 0 1.9G 0% /devtmpfs 1.9G 0 1.9G 0% /dev/shmtmpfs 1.9G 9.1M 1.9G 1% /runtmpfs 1.9G 0 1.9G 0% /sys/fs/cgroup/dev/sda1 1014M 179M 836M 18% /boottmpfs 379M 12K 379M 1% /run/user/42tmpfs 379M 0 379M 0% /run/user/1000# 内存[hadoop@hadoop000 ~]$ free -m total used free shared buff/cache availableMem: 3782 1048 2091 9 643 2439Swap: 2047 0 2047# 负载[hadoop@hadoop000 ~]$ toptop - 23:31:24 up 1 day, 17:34, 3 users, load average: 0.05, 0.03, 0.05Tasks: 173 total, 1 running, 172 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.1 us, 0.1 sy, 0.0 ni, 99.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stKiB Mem : 3873760 total, 2140760 free, 1074088 used, 658912 buff/cacheKiB Swap: 2097148 total, 2097148 free, 0 used. 2497104 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 36683 hadoop 20 0 157764 2304 1568 R 0.3 0.1 0:00.22 top 1 root 20 0 190996 4084 2552 S 0.0 0.1 0:10.76 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.08 kthreadd 3 root 20 0 0 0 0 S 0.0 0.0 0:00.35 ksoftirqd/0 5 root 0 -20 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0H 7 root rt 0 0 0 0 S 0.0 0.0 0:06.38 migration/0 2.查看进程 1234567891011[hadoop@hadoop000 ~]$ ps -ef | grep sshroot 1015 1 0 Jun17 ? 00:00:00 /usr/sbin/sshd -Droot 29127 1015 0 20:55 ? 00:00:00 sshd: hadoop [priv]hadoop 29129 29127 0 20:55 ? 00:00:06 sshd: hadoop@pts/2hadoop 36724 29132 0 23:33 pts/2 00:00:00 grep --color=auto ssh[hadoop@hadoop000 ~]$ ps -ef | grep ssh | grep -v greproot 1015 1 0 Jun17 ? 00:00:00 /usr/sbin/sshd -Droot 29127 1015 0 20:55 ? 00:00:00 sshd: hadoop [priv]hadoop 29129 29127 0 20:55 ? 00:00:06 sshd: hadoop@pts/2进程用户 进程pid 父id 进程用户的内容 3.查看端口 123[root@hadoop000 ~]# netstat -nlp | grep 29129tcp 0 0 127.0.0.1:6012 0.0.0.0:* LISTEN 29129/sshd: hadoop@ tcp6 0 0 ::1:6012 :::* LISTEN 29129/sshd: hadoop@","categories":[{"name":"Linux基础","slug":"Linux基础","permalink":"https://zzuuriel.github.io/categories/Linux基础/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zzuuriel.github.io/tags/Linux/"}]},{"title":"Linux source命令、sh和./区别","date":"2018-08-15T12:12:36.000Z","path":"Linux基础/Linux source命令、sh和.区别/","text":"Linux source命令、sh和./区别 转载自： https://www.cnblogs.com/pkufork/p/linux_source.html Linux source命令 通常用法：source filepath 或 . filepath 功能：使当前shell读入路径为filepath的shell文件并依次执行文件中的所有语句，通常用于重新执行刚修改的初始化文件，使之立即生效，而不必注销并重新登录。例如，当我们修改了/etc/profile文件，并想让它立刻生效，而不用重新登录，就可以使用source命令，如source /etc/profile。 source命令(从 C Shell 而来)是bash shell的内置命令；点命令(.)，就是个点符号(从Bourne Shell而来)是source的另一名称。这从用法中也能看出来。 source filepath 与 sh filepath 、./filepath的区别 当shell脚本具有可执行权限时，用sh filepath与./filepath是没有区别的。./filepath是因为当前目录没有在PATH中，所有&quot;.&quot;是用来表示当前目录的。 sh filepath 会重新建立一个子shell，在子shell中执行脚本里面的语句，该子shell继承父shell的环境变量，但子shell是新建的，其改变的变量不会被带回父shell，除非使用export。 source filename其实只是简单地读取脚本里面的语句依次在当前shell里面执行，没有建立新的子shell。那么脚本里面所有新建、改变变量的语句都会保存在当前shell里面。 举例说明： 新建一个test.sh脚本，内容为:A=1； 修改其可执行权限：chmod +x test.sh； 运行sh test.sh后，echo $A，显示为空，因为A=1并未传回给当前shell； 运行./test.sh后，也是一样的效果； 运行source test.sh 或者 . test.sh，然后echo $A，则会显示1，说明A=1的变量在当前shell中；","categories":[{"name":"Linux基础","slug":"Linux基础","permalink":"https://zzuuriel.github.io/categories/Linux基础/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zzuuriel.github.io/tags/Linux/"}]}]