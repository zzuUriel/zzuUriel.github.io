[{"title":"修改Flume源码使taildir source支持递归（可配置）","date":"2020-03-06T14:12:36.000Z","path":"大数据技术/修改Flume源码使taildir source支持递归（可配置）/","text":"修改Flume源码使taildir source支持递归（可配置） taildir Flume的source选哪个？taildir source首选！ 1.断点还原 positionFile可以记录偏移量 2.可配置文件组，里面使用正则表达式配置多个要监控的文件 这么好的taildir source有一点不完美，只支持监控一级目录下的文件，不能支持递归监控文件夹。 例如： /log/login/20200306/1.1og /log/login/20200307/1.1og 这两个文件无法用taildir采集 为实现目录递归, 即可以同时监控子目录，让上述两个文件可以通过/log/login/*.log采集，就只能修改源代码了。 改源码，先读源码 Flume的taildir source启动会调用start()方法作初始化，里面创建一个ReliableTaildirEventReader,这里用到了建造者模式。 12345678910111213141516171819202122232425262728293031@Override public synchronized void start() &#123; logger.info(\"&#123;&#125; TaildirSource source starting with directory: &#123;&#125;\", getName(), filePaths); try &#123; reader = new ReliableTaildirEventReader.Builder() .filePaths(filePaths) .headerTable(headerTable) .positionFilePath(positionFilePath) .skipToEnd(skipToEnd) .addByteOffset(byteOffsetHeader) .cachePatternMatching(cachePatternMatching) .annotateFileName(fileHeader) .fileNameHeader(fileHeaderKey) .build(); &#125; catch (IOException e) &#123; throw new FlumeException(\"Error instantiating ReliableTaildirEventReader\", e); &#125; idleFileChecker = Executors.newSingleThreadScheduledExecutor( new ThreadFactoryBuilder().setNameFormat(\"idleFileChecker\").build()); idleFileChecker.scheduleWithFixedDelay(new idleFileCheckerRunnable(), idleTimeout, checkIdleInterval, TimeUnit.MILLISECONDS); positionWriter = Executors.newSingleThreadScheduledExecutor( new ThreadFactoryBuilder().setNameFormat(\"positionWriter\").build()); positionWriter.scheduleWithFixedDelay(new PositionWriterRunnable(), writePosInitDelay, writePosInterval, TimeUnit.MILLISECONDS); super.start(); logger.debug(\"TaildirSource started\"); sourceCounter.start(); &#125; taildir source属于PollableSource，下面是PollableSource的文档注释： 123456789/** * A &#123;@link Source&#125; that requires an external driver to poll to determine * whether there are &#123;@linkplain Event events&#125; that are available to ingest * from the source. * * @see org.apache.flume.source.EventDrivenSourceRunner */public interface PollableSource extends Source &#123;... 这段注释的意思是PollableSource是需要一个外部驱动去查看有没有需要消费的事件，从而拉取事件，讲白了就是定时拉取。所以flume也不一定是真正实时的，只是隔一会儿不停地来查看事件而已(与之相应的是另一种EventDrivenSourceRunner)。 那么taildir source在定时拉取事件的时候是调用的process方法。 123456789101112131415161718192021222324@Override public Status process() &#123; Status status = Status.READY; try &#123; existingInodes.clear(); existingInodes.addAll(reader.updateTailFiles()); for (long inode : existingInodes) &#123; TailFile tf = reader.getTailFiles().get(inode); if (tf.needTail()) &#123; tailFileProcess(tf, true); &#125; &#125; closeTailFiles(); try &#123; TimeUnit.MILLISECONDS.sleep(retryInterval); &#125; catch (InterruptedException e) &#123; logger.info(\"Interrupted while sleeping\"); &#125; &#125; catch (Throwable t) &#123; logger.error(\"Unable to tail files\", t); status = Status.BACKOFF; &#125; return status; &#125; 重点就是下面这几行: 1234567existingInodes.addAll(reader.updateTailFiles()); for (long inode : existingInodes) &#123; TailFile tf = reader.getTailFiles().get(inode); if (tf.needTail()) &#123; tailFileProcess(tf, true); &#125; &#125; 从reader.updateTailFiles()获取需要监控的文件，然后对每一个进行处理，查看最后修改时间，判定是否需要tail，需要tail就tail。 那么进入reader.updateTailFiles() 1234567891011121314for (File f : taildir.getMatchingFiles()) &#123; long inode; try &#123; inode = getInode(f); &#125; catch (NoSuchFileException e) &#123; logger.info(\"File has been deleted in the meantime: \" + e.getMessage()); continue; &#125; TailFile tf = tailFiles.get(inode); if (tf == null || !tf.getPath().equals(f.getAbsolutePath())) &#123; long startPos = skipToEnd ? f.length() : 0; tf = openFile(f, headers, inode, startPos); &#125; else &#123; ... 遍历每一个正则表达式匹配对应的匹配器，每个匹配器去获取匹配的文件！taildir.getMatchingFiles() 123456789101112131415161718192021List&lt;File&gt; getMatchingFiles() &#123; long now = TimeUnit.SECONDS.toMillis( TimeUnit.MILLISECONDS.toSeconds(System.currentTimeMillis())); long currentParentDirMTime = parentDir.lastModified(); List&lt;File&gt; result; // calculate matched files if // - we don't want to use cache (recalculate every time) OR // - directory was clearly updated after the last check OR // - last mtime change wasn't already checked for sure // (system clock hasn't passed that second yet) if (!cachePatternMatching || lastSeenParentDirMTime &lt; currentParentDirMTime || !(currentParentDirMTime &lt; lastCheckedTime)) &#123; lastMatchedFiles = sortByLastModifiedTime(getMatchingFilesNoCache(isRecursive)); lastSeenParentDirMTime = currentParentDirMTime; lastCheckedTime = now; &#125; return lastMatchedFiles; &#125; 可以看到getMatchingFilesNoCache(isRecursive)就是获取匹配的文件的方法，也就是需要修改的方法了！ ps：这里的isRecursive是我加的~ 点进去： 123456789101112private List&lt;File&gt; getMatchingFilesNoCache() &#123; List&lt;File&gt; result = Lists.newArrayList(); try (DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(parentDir.toPath(), fileFilter)) &#123; for (Path entry : stream) &#123; result.add(entry.toFile()); &#125; &#125; catch (IOException e) &#123; logger.error(\"I/O exception occurred while listing parent directory. \" + \"Files already matched will be returned. \" + parentDir.toPath(), e); &#125; return result;&#125; 源码是用了Files.newDirectoryStream(parentDir.toPath(), fileFilter))，将父目录下符合正则表达式的文件都添加到一个迭代器里。（这里还用了try (…)的语法糖） 找到地方了，开始改 在getMatchingFilesNoCache()方法下面加一个重载的方法, 可增加扩展性： 123456789101112131415161718192021222324252627private List&lt;File&gt; getMatchingFilesNoCache(boolean recursion) &#123; if (!recursion) &#123; return getMatchingFilesNoCache(); &#125; List&lt;File&gt; result = Lists.newArrayList(); // 使用非递归的方式遍历文件夹 Queue&lt;File&gt; dirs = new ArrayBlockingQueue&lt;&gt;(10); dirs.offer(parentDir); while (dirs.size() &gt; 0) &#123; File dir = dirs.poll(); try &#123; DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(dir.toPath(), fileFilter); stream.forEach(path -&gt; result.add(path.toFile())); &#125; catch (IOException e) &#123; logger.error(\"I/O exception occurred while listing parent directory. \" + \"Files already matched will be returned. (recursion)\" + parentDir.toPath(), e); &#125; File[] dirList = dir.listFiles(); assert dirList != null; for (File f : dirList) &#123; if (f.isDirectory()) &#123; dirs.add(f); &#125; &#125; &#125; return result;&#125; 使用了非递归的方式遍历文件夹，就是树到队列的转换。 到这里，核心部分就改完了，接下来要处理这个recursion的参数。 华丽的分割线后，顺腾摸瓜 一路改构造方法，添加这个参数，最终参数从哪来呢？ flume的source启动时会调用configure方法，将Context中的内容配置进reader等对象中。 isRecursive = context.getBoolean(RECURSIVE, DEFAULT_RECURSIVE); context从TaildirSourceConfigurationConstants中获取配置名和默认值。 1234/** * Whether to support recursion. */public static final String RECURSIVE = \"recursive\";public static final boolean DEFAULT_RECURSIVE = false; 这里的recursive也就是flume配置文件里配置项了 12# Whether to support recusiona1.sources.r1.recursive = true 大功告成，打包试试 执行package将其放在flume的lib下，替换原来的flume-taildir-source***.jar 启动，测试，成功！","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Flume","slug":"Flume","permalink":"https://zzuuriel.github.io/tags/Flume/"}]},{"title":"解决Spark on YARN时jar包乱飞的问题","date":"2020-03-06T12:09:39.000Z","path":"大数据技术/解决Spark on YARN时jar包乱飞的问题/","text":"解决Spark on YARN时jar包乱飞的问题 存在问题 使用Spark on YARN运行自带的SparkPi作业 123456./spark-submit \\--class org.apache.spark.examples.SparkPi \\--master yarn \\--deploy-mode client \\/home/hadoop/app/spark-2.4.5-bin-2.6.0-cdh5.16.2/examples/jars/spark-examples_2.12-2.4.5.jar \\2 查看日志 123WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.INFO yarn.Client: Uploading resource file:/tmp/spark-47b51eb1-58db-4683-8172-7c6db44c94d8/__spark_libs__5430804291093212829.zip -&gt; hdfs://hadoop000:8020/user/hadoop/.sparkStaging/application_1589545550777_0002/__spark_libs__5430804291093212829.zipINFO yarn.Client: Uploading resource file:/tmp/spark-47b51eb1-58db-4683-8172-7c6db44c94d8/__spark_conf__3780031617713858810.zip -&gt; hdfs://hadoop000:8020/user/hadoop/.sparkStaging/application_1589545550777_0002/__spark_conf__.zip 提示我们既没有设置spark.yarn.archive也没有设置spark.yarn.jars，所以spark自己把所有jar包打成zip包上传到hdfs上(用完后又删掉了) 解决方法 根据日志提示，将spark jar包上传到hdfs上。 1、把spark/jars下的jar包打成zip包，防止小文件过多，影响hdfs 12cd jarszip sparkjars.zip *.jar 2、上传到hdfs 1hadoop fs -put sparkjars.zip /lib 3、在spark-default配置中指定jar包的地址 12vi spark-defaults.confspark.yarn.archive hdfs://hadoop000:8020/lib/sparkjars.zip 结果验证 重新运行作业，查看日志 1INFO yarn.Client: Source and destination file systems are the same. Not copying hdfs://hadoop000:8020/lib/sparkjars.zip 可以发现spark已经从我们配置的路径读取了jar包","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Flink之Watermark理解","date":"2020-03-02T14:12:36.000Z","path":"大数据技术/Flink之Watermark理解/","text":"Flink之Watermark理解 概述 我们知道实时计算中，数据时间比较敏感，有eventTime和processTime之分，一般来说eventTime是从原始的消息中提取过来的，processTime是Flink自己提供的，Flink中一个亮点就是可以基于eventTime计算，这个功能很有用，因为实时数据可能会经过比较长的链路，多少会有延时，并且有很大的不确定性，对于一些需要精确体现事件变化趋势的场景中，单纯使用processTime显然是不合理的。 为了处理事件时间，首先，流程序需要相应地设置时间特性，代码如下： 12val env = StreamExecutionEnvironment.getExecutionEnvironmentenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) 其次，需要提取eventTime和设置WaterMark。下面我们具体分析一下，eventTime结合Watermark的工作方式。 案例分析 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556object WMApp &#123; def main(args: Array[String]): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment env.setParallelism(1) env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) val output = new OutputTag[Info](\"late\") val MAX_ALLOWED_UNBOUNDED_TIME = 10000L val stream = env.socketTextStream(\"hadoop000\", 9999) .map(x =&gt; &#123; val splits = x.split(\",\") Info(splits(0).trim, splits(1).trim.toDouble, splits(2).trim, splits(3).trim.toLong) &#125;).assignTimestampsAndWatermarks(new RuozedataAssignerWithPeriodicWatermarks(MAX_ALLOWED_UNBOUNDED_TIME)) .keyBy(0) .window(TumblingEventTimeWindows.of(Time.seconds(3))) //.allowedLateness(Time.seconds(2)) // 允许等待两秒 .sideOutputLateData(output) .apply(new RichWindowFunction[Info, String, Tuple, TimeWindow] &#123; override def apply(key: Tuple, window: TimeWindow, input: Iterable[Info], out: Collector[String]): Unit = &#123; val totals = input.size var totalTmp = 0.0 input.foreach(x =&gt; totalTmp = totalTmp + x.temperature) val avg = totalTmp / totals out.collect(s\"$avg, $&#123;new Timestamp(window.getStart)&#125;, $&#123;new Timestamp(window.getEnd)&#125;\") &#125; &#125;) stream.print() stream.getSideOutput(output).print(\"-----\") env.execute(this.getClass.getSimpleName) &#125;&#125;class RuozedataAssignerWithPeriodicWatermarks(maxAllowedUnorderedTime: Long) extends AssignerWithPeriodicWatermarks[Info] &#123; var maxTimestamp: Long = 0 override def extractTimestamp(element: Info, previousElementTimestamp: Long): Long = &#123; val nowTime = element.time * 1000 maxTimestamp = maxTimestamp.max(nowTime) println(new Timestamp(nowTime) + \",\" + new Timestamp(maxTimestamp) + \",\" + new Timestamp(getCurrentWatermark.getTimestamp)) nowTime &#125; override def getCurrentWatermark: Watermark = &#123; new Watermark(maxTimestamp - maxAllowedUnorderedTime) &#125;&#125;case class Info(id: String, temperature: Double, name: String, time: Long) 测试 [hadoop@hadoop000 ~]$ nc -lk 9999 输入： 1231,36.8,a,15831462671,37.1,a,1583146268 1,36.6,a,1583146278 返回： 1232020-03-02 18:51:07.0,2020-03-02 18:51:07.0,2020-03-02 18:50:57.02020-03-02 18:51:08.0,2020-03-02 18:51:08.0,2020-03-02 18:50:58.02020-03-02 18:51:18.0,2020-03-02 18:51:18.0,2020-03-02 18:51:08.0 此时窗口未被触发，窗口触发需要要满足两个条件： 1.Watermark&gt;=window_end_time, 2.此窗口内有数据。 而2020-03-02 18:51:07.0，2020-03-02 18:51:08.0在 [06,09) 区间内，故当Watermark≥09时，窗口就会被触发，输入1,36.6,a,1583146279 ，返回： 122020-03-02 18:51:19.0,2020-03-02 18:51:19.0,2020-03-02 18:51:09.036.95, 2020-03-02 18:51:06.0, 2020-03-02 18:51:09.0 追加一条延时数据1,36.8,a,1583146267，返回： 122020-03-02 18:51:07.0,2020-03-02 18:51:19.0,2020-03-02 18:51:09.0-----&gt; Info(1,36.8,a,1583146267) 分析 1.提取WaterMark的方式两类： 一类是定时提取watermark，对应AssignerWithPeriodicWatermarks，这种方式会定时提取更新wartermark。通常情况下采用定时提取就足够了。 另一类伴随event的到来就提取watermark，就是每一个event到来的时候，就会提取一次watermark，对应AssignerWithPunctuatedWatermarks，这样的方式当然设置watermark更为精准，但是当数据量大的时候，频繁的更新wartermark会比较影响性能。 2.在上面例子中extractTimestamp方法，在每一个event到来之后就会被调用。 这里其实就是为了设置watermark的值，关键代码在于Math.max(timestamp,currentMaxTimestamp)，意思是在当前的水位和当前事件时间中选取一个较大值，来让watermark流动。 为什么要选取最大值，因为理想状态下，消息的事件时间肯定是递增的，实际处理中，消息乱序是大概率事件，所以为了保证watermark递增，要取最大值。 3.getCurrentWatermarker会被定时调用，可以看到方法中watermark减了一个常量（本例设置为10s），为了理解这么做的原因，需要了解Watermark的工作方式，上文提到在基于eventTime的计算中，需要watermark的协助来触发window的计算，触发规则是watermark≥window的结束时间，并且这个窗口中有数据。 上例中，如果watermark没有减去10s，设想理想情况下消息都没有延迟，watermark ≥18:51:09.0的时候，就会触发窗口[06,09)的计算，这个时候因为消息都没有延迟，watermark之前的消息都已经落入到window中，所以会计算window中全量的数据。那么假如有一条消息data1，eventTime是18:51:07，应该属于[06,09)，但是在18:51:10才到达，因为窗口已经关闭，那么这条消息就会被丢弃，没有加入计算，这样就会出现问题。 每次提取eventTime的时候，如果watermark减去10s，那么当data1在18:51:10到达的时候，watermark才18:51:00，这个时候，窗口还没有触发计算，那么data1会被加入窗口[06,09)，这个时候计算完全没有问题，所以减去一个常量是为了对延时的消息进行容错的。 4.watermark是全局性的参数，watermark超过window的endtime之后，就会触发窗口计算。一般情况下，触发窗口计算之后，窗口就销毁掉了，后面再来的数据也不会再计算，但这个一般情况对应的是allowedLateness=0的场景。 allowedLateness是窗口函数的属性，默认情况下，如果不指定allowedLateness，其值是0，即对于watermark超过end-of-window之后，还有此window的数据到达时，这些数据被删除掉了 如果allowedLateness &gt; 0， allowedLateness会再次触发窗口的计算，而之前触发的数据，会buffer起来，直到watermark ≥ end-of-window + allowedLateness（），窗口的数据及元数据信息才会被删除。 5.如果指定窗口已经彻底关闭了，可以利用sideOutputLateData把延迟数据放到侧输出流。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://zzuuriel.github.io/tags/Flink/"}]},{"title":" Spark-2.4.5源码编译","date":"2020-02-21T06:23:09.000Z","path":"大数据技术/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/","text":"Spark-2.4.5源码编译 背景 从Spark官网https://spark.apache.org/ 下载的预编译版本的Spark，受到诸多限制： 比如我们生产环境使用的Hadoop是CDH版本，那么从官网下载的预编译版本就不能使用； 线上生产环境和实际业务需求的复杂性，不可避免地需要修改spark源码； 因此，我们必须学会如何编译Spark源码，以便应用于线上生产环境。 前置条件 官网下载Spark源码 查看官方文档，确定前置条件 Spark2.4.5版本源码编译文档地址：https://spark.apache.org/docs/latest/building-spark.html 根据需要编译的Spark版本，查看对应到官网文档，里面有必备的条件，比如本文的： 条件1：Maven 3.5.4 版本及以上 条件2：Java 8 版本及以上 条件3：需要使用兼容的 scala 版本(2.12.x)。 Maven配置文件增加阿里镜像 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;&lt;/mirror&gt; 解压源码并修改pom.xml文件 由于本文需要编译的Hadoop版本是2.6.0-CDH5.15.1版本，编译过程中需要下载CDH相关的包，因此在pom文件需要添加cloudera仓库。 1234&lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos&lt;/url&gt;&lt;/repository&gt; 编译步骤 官网里介绍了两种编译方式，一是mvn，另外是 make-distribution.sh；下面分别介绍具体实现。 Apache Maven 1.设置Maven的工作使用内存 1export MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\" 如果不设置会报以下错误： 12[INFO] Compiling 203 Scala sources and 9 Java sources to /Users/me/Development/spark/core/target/scala-2.12/classes...[ERROR] Java heap space -&gt; [Help 1] 2.build/mvn 进到解压后源码文件夹下，执行如下命令： 1./build/mvn -Pyarn -Phive -Phive-thriftserver -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.15.1 -DskipTests clean package Building a Runnable Distribution 编译成可部署在分布式系统上的可执行版本，方便后续安装，这种方法是本文推荐使用的。 1.为了加快编译速度，需要修改启动编译脚本-Phadoop-2.6 vim spark-2.4.5/dev/make-distribution.sh 1234567891011121314151617181920212223242526\"\"\"添加以下部分\"\"\"VERSION=2.4.5SCALA_VERSION=2.12SPARK_HADOOP_VERSION=2.6.0-cdh5.15.1SPARK_HIVE=1\"\"\"注释掉以下部分，该部分为了获取版本号将会占用大量时间\"\"\"#VERSION=$(\"$MVN\" help:evaluate -Dexpression=project.version $@ 2&gt;/dev/null\\# | grep -v \"INFO\"\\# | grep -v \"WARNING\"\\# | tail -n 1)#SCALA_VERSION=$(\"$MVN\" help:evaluate -Dexpression=scala.binary.version $@ 2&gt;/dev/null\\# | grep -v \"INFO\"\\# | grep -v \"WARNING\"\\# | tail -n 1)#SPARK_HADOOP_VERSION=$(\"$MVN\" help:evaluate -Dexpression=hadoop.version $@ 2&gt;/dev/null\\# | grep -v \"INFO\"\\# | grep -v \"WARNING\"\\# | tail -n 1)#SPARK_HIVE=$(\"$MVN\" help:evaluate -Dexpression=project.activeProfiles -pl sql/hive $@ 2&gt;/dev/null\\# | grep -v \"INFO\"\\# | grep -v \"WARNING\"\\# | fgrep --count \"&lt;id&gt;hive&lt;/id&gt;\";\\# # Reset exit status to 0, otherwise the script stops here if the last grep finds nothing\\# # because we use \"set -o pipefail\"# echo -n) 2.执行编译脚本 1./dev/make-distribution.sh --name 2.6.0-cdh5.15.1 --tgz -Pyarn -Phive -Phive-thriftserver -Phadoop-2.6 -Dhadoop.version=2.6.0-cdh5.15.1 12345678注：-Phadoop hadoop的大版本号-Dhadoop.version=2.6.0-cdh5.15.1 hadoop的详细版本号--name 编译后spark安装包的名字--tgz 以tgz方式压缩-Phive 编译出来的spark支持hive-Phive-thriftserver 编译出来的spark支持hive-thriftserver-Pyarn 编译出来的spark支持在yarn上运行 3.编译成功 最后编译成功后，还会执行一段脚本将中间过程产生的文件删除，并将编译成功的文件打包。 将编译后成功的安装包spark-2.4.5-bin-2.6.0-cdh5.15.1解压，以local的方式启动spark ， 进入bin目录下执行下面命令： 12345678910111213141516171819[hadoop@hadoop000 bin]$ ./spark-shell master local[2]20/02/21 07:52:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to \"WARN\".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Spark context Web UI available at http://hadoop000:4040Spark context available as 'sc' (master = local[*], app id = local-1583538833348).Spark session available as 'spark'.Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.4.5 /_/ Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_91)Type in expressions to have them evaluated.Type :help for more information.scala&gt;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Flink自定义触发器","date":"2020-02-20T14:12:36.000Z","path":"大数据技术/Flink自定义触发器/","text":"Flink自定义触发器 Flink触发器 触发器决定窗口(由窗口分配者形成)什么时候可以被窗口函数处理。 每个窗户都有一个默认触发器。 如果默认触发器不符合我们的需要，可以使用trigger(…)自定义触发器。 主要方法 触发器接口有五种方法，允许触发器对不同事件做出反应: onElement() 添加到每个窗口的元素都会调用此方法。 onEventTime() 当注册的事件时间计时器触发时，将调用此方法。 onProcessingTime() 当注册的处理时间计时器触发时，将调用此方法。 onMerge() 与有状态触发器相关，并在两个触发器对应的窗口合并时合并它们的状态，例如在使用会话窗口时。 clear() 执行删除相应窗口时所需的任何操作。(一般是删除定义的状态、定时器等) 以上方法有两点需要注意: 1)前三个函数通过返回一个TriggerResult来决定如何对它们的调用事件进行操作。 行动可以是以下任何一种: CONTINUE: 什么也不做 FIRE: 表示触发计算，同时保留窗口中的数据 PURGE: 简单地删除窗口的内容，并保留关于窗口和任何触发器状态的任何潜在元信息。 FIRE_AND_PURGE: 触发计算，然后清除窗口中的元素。（默认情况下，预先实现的触发器只触发而不清除窗口状态） 2)这些方法中的任何一个都可以用来注册处理时间计时器或事件时间计时器，以便将来执行操作。 案例 需求 当窗口中的数据量达到一定数量的时候触发计算 根据执行时间每隔一定时间且窗口中有数据触发计算，如果没有数据不触发计算 窗口关闭的时候清除数据 实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566object TriggerWindowApp &#123; def main(args: Array[String]): Unit = &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment val MAX_WAIT_TIME = 10 // time window val MAX_PERSON_CNTS = 4 // count window env.socketTextStream(\"hadoop000\", 9999) .filter(x =&gt; x.trim.nonEmpty) .map(x =&gt; &#123; (x.split(\",\")(0), 1) &#125;).keyBy(0) .timeWindow(Time.seconds(MAX_WAIT_TIME)) // time window .trigger(new RuozedataTrigger(MAX_PERSON_CNTS)) .sum(1) .print() env.execute(this.getClass.getSimpleName) &#125;&#125;class RuozedataTrigger(max: Int) extends Trigger[(String, Int), TimeWindow] &#123; //定义state描述符 val desc: ReducingStateDescriptor[Int] = new ReducingStateDescriptor(\"count\", new ReduceFunction[Int] &#123; override def reduce(value1: Int, value2: Int): Int = value1 + value2 &#125;, classOf[Int]) override def onElement(element: (String, Int), timestamp: Long, window: TimeWindow, ctx: Trigger.TriggerContext): TriggerResult = &#123; ctx.registerProcessingTimeTimer(window.maxTimestamp) val state = ctx.getPartitionedState(desc) state.add(1) val value = state.get() if (value &gt;= max) &#123; println(\"---count触发---\") state.clear() TriggerResult.FIRE_AND_PURGE &#125; else &#123; TriggerResult.CONTINUE &#125; &#125; override def onProcessingTime(time: Long, window: TimeWindow, ctx: Trigger.TriggerContext): TriggerResult = &#123; println(\"~~~time触发~~~\") TriggerResult.FIRE_AND_PURGE &#125; override def onEventTime(time: Long, window: TimeWindow, ctx: Trigger.TriggerContext): TriggerResult = &#123; TriggerResult.CONTINUE &#125; override def clear(window: TimeWindow, ctx: Trigger.TriggerContext): Unit = &#123; ctx.getPartitionedState(desc).clear() ctx.deleteProcessingTimeTimer(window.maxTimestamp) &#125; override def canMerge: Boolean = true override def onMerge(window: TimeWindow, ctx: Trigger.OnMergeContext): Unit = &#123; ctx.mergePartitionedState(desc) &#125;&#125; 测试 [hadoop@hadoop000 ~]$ nc -lk 9999 1）直接输入4个a 1234aaaa 返回结果： 12---count触发---3&gt; (a,4) 2）输入1个a，等待10s 返回结果： 12~~~time触发~~~3&gt; (a,1)","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Flink","slug":"Flink","permalink":"https://zzuuriel.github.io/tags/Flink/"}]},{"title":"深入理解Spark算子aggregate","date":"2019-12-15T06:15:39.000Z","path":"大数据技术/60.深入理解Spark算子aggregate/","text":"深入理解Spark算子aggregate 源码解读 123456789101112131415161718192021222324 /*** Aggregate the elements of each partition, and then the results for all the partitions, using* given combine functions and a neutral \"zero value\". This function can return a different result* type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U* and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are* allowed to modify and return their first argument instead of creating a new U to avoid memory* allocation.* * @param zeroValue the initial value for the accumulated result of each partition for the* `seqOp` operator, and also the initial value for the combine results from* different partitions for the `combOp` operator - this will typically be the* neutral element (e.g. `Nil` for list concatenation or `0` for summation)* @param seqOp an operator used to accumulate results within a partition* @param combOp an associative operator used to combine results from different partitions */ def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U = withScope &#123; // Clone the zero value since we will also be serializing it as part of tasks var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance()) val cleanSeqOp = sc.clean(seqOp) val cleanCombOp = sc.clean(combOp) val aggregatePartition = (it: Iterator[T]) =&gt; it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp) val mergeResult = (index: Int, taskResult: U) =&gt; jobResult = combOp(jobResult, taskResult) sc.runJob(this, aggregatePartition, mergeResult) jobResult &#125; 翻译： aggregate先对每个分区的元素做聚集，然后对所有分区的结果做聚集，聚集过程中，使用的是给定的聚集函数以及初始值”zero value”。这个函数能返回一个与原始RDD不同的类型U，因此，需要一个合并RDD类型T到结果类型U的函数，还需要一个合并类型U的函数。这两个函数都可以修改和返回他们的第一个参数，而不是重新新建一个U类型的参数以避免重新分配内存。 参数zeroValue：seqOp运算符的每个分区的累积结果的初始值以及combOp运算符的不同分区的组合结果的初始值 - 这通常将是初始元素（例如“Nil”表的列表 连接或“0”表示求和） 参数seqOp： 每个分区累积结果的聚集函数。 参数combOp： 一个关联运算符用于组合不同分区的结果 解读： 这个算子一共有两个参数列表，第一个参数列表中传递 zeroValue （第零个值）第二个参数列表中传递两个函数，传入的第一个函数seqOp函数会作用于每个分区,第二个函数combOp函数在第一个函数执行完之后汇总所有分区结果。 这两个函数的第一个参数都是累加器，第一次执行时，会把zeroValue赋给累加器。第一次j计算之后会把返回值赋给累加器，作为下一次运算的第一个参数。seqOP函数每个分区都有一个累加器，combOp函数只有一个累加器。 示例 1234567891011object test &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val rdd1 = sc.parallelize(1 to 5, 1) def func1(a:Int, b:Int) = a * b def func2(a:Int, b:Int) = a + b println(rdd1.aggregate(3)(func1, func2)) sc.stop() &#125;&#125; 分析： 1）seqOp seqOp对分区内的所有元素遍历计算 这个分区有几个元素执行几次这个方法 def func1(a:Int, b:Int) = a * b 当第一个元素1传进来时 a是seqOp累加器(第一次执行时，会把zeroValue赋给累加器，zeroValue=3），y是第一个元素1，之后将返回值赋回给累加器。 当第二个元素2传进来时 a：上一次运算之后赋了新值的累加器，3 * 1 =3，y：传入的第二个元素2，之后将返回值赋回给累加器。 当第三个元素3传进来时 a：上一次运算之后赋了新值的累加器，3 * 2 =6，y：传入的第二个元素2，之后将返回值赋回给累加器。 … 最后seqOp累加计算得到360 2）combOp 之后combOp会合并所有分区的结果。 def func2(a:Int, b:Int) = a + b 这个函数遍历所有中间结果（累加器：一个分区一个） 第一次执行时， a是combOp累加器（第一次执行时，会把zeroValue赋给累加器，zeroValue=3），b是第一个分区的累加器（360)，之后将返回值赋回给累加器。。 第二次执行时， a是combOp累加器，b是第二个分区的累加器，之后将返回值赋回给累加器。 … 有几个分区，就执行几次combOp函数。 本例只有一个分区，最后计算结果为363 应用 1.两两求和 12345678910object test &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val rdd = sc.parallelize(List(1, 3, 2, 4, 3, 5), 3) println(rdd.sum) //println（rdd.aggregate(0)(_+_,_+_)） sc.stop() &#125;&#125; rdd.sum可以用rdd.aggregate(0)(+,+)替代，最后运行结果都是18 2.求每个分区的最大值之和 12345678910111213object test &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val rdd2 = sc.parallelize(List(List(1,3),List(2,4),List(3,5)), 3) def fun01(x:Int,y:List[Int]) = &#123; x.max(y.max) &#125; def fun02(a:Int, b:Int) = a + b println(rdd2.aggregate(0)(fun01,fun02)) sc.stop() &#125;&#125; 初始值为0，最后运行结果为12， 初始值为10，运行结果为40。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Kudu常用Api(java) ","date":"2019-12-07T12:35:39.000Z","path":"大数据技术/60.Kudu常用Api(java)/","text":"Kudu常用Api(java) 添加依赖 在pom.xml文件中添加以下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kudu&lt;/groupId&gt; &lt;artifactId&gt;kudu-client&lt;/artifactId&gt; &lt;version&gt;$&#123;kudu.version&#125;&lt;/version&gt;&lt;/dependency&gt; 测试 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120public class KuduAPIApp &#123; String masterAddress = \"hadoop000:7051\"; KuduClient client = null; private ColumnSchema newColumn(String name, Type type, Boolean pk) &#123; ColumnSchema.ColumnSchemaBuilder column = new ColumnSchema.ColumnSchemaBuilder(name, type); column.key(pk); return column.build(); &#125; /** * 创建表格 * @throws Exception */ @Test public void CreateTable() throws Exception &#123; LinkedList&lt;ColumnSchema&gt; columns = new LinkedList&lt;&gt;(); columns.add(newColumn(\"id\", Type.INT32, true)); columns.add(newColumn(\"name\", Type.STRING, false)); Schema schema = new Schema(columns); CreateTableOptions options = new CreateTableOptions(); options.setNumReplicas(1); LinkedList&lt;String&gt; partitions = new LinkedList&lt;&gt;(); partitions.add(\"id\"); options.addHashPartitions(partitions, 3); client.createTable(\"student\", schema, options); &#125; /** * 插入数据 * @throws Exception */ @Test public void insert() throws Exception &#123; KuduTable table = client.openTable(\"student\"); KuduSession kuduSession = client.newSession(); kuduSession.setMutationBufferSpace(3000); for (int i = 0; i &lt; 10; i++) &#123; Insert insert = table.newInsert(); insert.getRow().addInt(\"id\", i + 1); insert.getRow().addString(\"name\", \"PK\" + i); kuduSession.flush(); kuduSession.apply(insert); &#125; &#125; /** * 查询表 * @throws Exception */ @Test public void query()throws Exception&#123; KuduTable table = client.openTable(\"student\"); KuduScanner scanner = client.newScannerBuilder(table).build(); while(scanner.hasMoreRows()) &#123; for(RowResult result : scanner.nextRows()) &#123; System.out.println(result.getInt(\"id\") + \"\\t\" + result.getString(\"name\")); &#125; &#125; &#125; /** * 更改表数据 * @throws Exception */ @Test public void update() throws Exception&#123; KuduTable table = client.openTable(\"student\"); KuduSession kuduSession = client.newSession(); Update update = table.newUpdate(); // update xxx set id=1 PartialRow row = update.getRow(); row.addInt(\"id\",1); row.addString(\"name\", \"ruozedata\"); kuduSession.apply(update); &#125; /** * 删除指定行 * @throws Exception */ @Test public void DeleteRow() throws Exception&#123; KuduTable table = client.openTable(\"student\"); KuduSession kuduSession = client.newSession(); Delete delete = table.newDelete(); delete.getRow().addInt(\"id\",2); kuduSession.apply(delete); &#125; /** * 删掉整张表 * @throws Exception */ @Test public void DropTable() throws Exception&#123; client.deleteTable(\"student\"); &#125; @Before public void setUp() &#123; client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); &#125; @After public void tearDown() &#123; if (null != client) &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 创建表 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.urieldata.kudu;import org.apache.kudu.ColumnSchema;import org.apache.kudu.Schema;import org.apache.kudu.Type;import org.apache.kudu.client.CreateTableOptions;import org.apache.kudu.client.KuduClient;import org.apache.kudu.client.KuduException;import java.util.LinkedList;public class CreateTable &#123; private static ColumnSchema newColumn(String name, Type type, Boolean isKey)&#123; ColumnSchema.ColumnSchemaBuilder column = new ColumnSchema.ColumnSchemaBuilder(name, type); column.key(isKey); return column.build(); &#125; public static void main(String[] args) &#123; //master地址 String masterAddress = \"hadoop000:7051\"; //创建kudu的数据库连接 KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); //设置表的schema LinkedList&lt;ColumnSchema&gt; columns = new LinkedList&lt;&gt;(); /* *和RDBMS不同的是，Kudu不提供自动递增列功能，因此应用程序必须始终 *在插入期间提供完整的主键 */ columns.add(newColumn(\"id\",Type.INT32,true)); columns.add(newColumn(\"name\",Type.STRING,false)); Schema schema = new Schema(columns); //创建表时提供的所有选项 CreateTableOptions options = new CreateTableOptions(); //设置表的replica备份和分区规则 LinkedList&lt;String&gt; parcols = new LinkedList&lt;&gt;(); parcols.add(\"id\"); //设置表的备份数 options.setNumReplicas(1); //设置hash分区和数量 options.addHashPartitions(parcols,3); try &#123; client.createTable(\"student\",schema,options); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 插入数据 1234567891011121314151617181920212223242526272829303132333435363738package com.urieldata.kudu;import org.apache.kudu.client.*;public class insert &#123; public static void main(String[] args) &#123; String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; KuduTable table = client.openTable(\"student\"); KuduSession kuduSession = client.newSession(); kuduSession.setFlushMode(SessionConfiguration.FlushMode.MANUAL_FLUSH); kuduSession.setMutationBufferSpace(3000); for (int i = 0; i &lt; 10; i++) &#123; Insert insert = table.newInsert(); insert.getRow().addInt(\"id\", i + 1); insert.getRow().addString(\"name\", \"Uriel\" + i); kuduSession.flush(); kuduSession.apply(insert); &#125; kuduSession.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 查询数据 12345678910111213141516171819202122232425262728293031323334353637package com.urieldata.kudu;import org.apache.kudu.client.*;public class query &#123; public static void main(String[] args) &#123; //master地址 String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; KuduTable table = client.openTable(\"student\"); //创建scanner扫描 KuduScanner scanner = client.newScannerBuilder(table).build(); //遍历数据 while (scanner.hasMoreRows()) &#123; for (RowResult rowResult : scanner.nextRows()) &#123; System.out.println(rowResult.getInt(\"id\") + \"\\t\" + rowResult.getString(\"name\")); &#125; &#125; &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 更改表数据 1234567891011121314151617181920212223242526272829303132333435363738package com.urieldata.kudu;import org.apache.kudu.client.*;public class update &#123; public static void main(String[] args) &#123; String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; KuduTable table = client.openTable(\"student\"); KuduSession session = client.newSession(); session.setFlushMode(SessionConfiguration.FlushMode.AUTO_FLUSH_SYNC); //更新数据 Update update = table.newUpdate(); PartialRow row = update.getRow(); row.addInt(\"id\",1); row.addString(\"name\",\"IU\"); session.apply(update); session.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 删除指定行 123456789101112131415161718192021222324252627282930313233343536package com.urieldata.kudu;import org.apache.kudu.client.*;public class DeleteRow &#123; public static void main(String[] args) &#123; //master地址 String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; KuduTable table = client.openTable(\"student\"); KuduSession session = client.newSession(); //删除数据 Delete delete = table.newDelete(); //行删除和更新操作必须指定要更改行的完整主键 delete.getRow().addInt(\"id\",1); session.flush(); session.apply(delete); session.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 删除整张表 123456789101112131415161718192021222324252627package com.urieldata.kudu;import org.apache.kudu.client.KuduClient;import org.apache.kudu.client.KuduException;public class DropTable &#123; public static void main(String[] args) &#123; String masterAddress = \"hadoop000:7051\"; KuduClient client = new KuduClient.KuduClientBuilder(masterAddress) .defaultSocketReadTimeoutMs(6000).build(); try &#123; client.deleteTable(\"student\"); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125;finally &#123; try &#123; client.close(); &#125; catch (KuduException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Kudu","slug":"Kudu","permalink":"https://zzuuriel.github.io/tags/Kudu/"}]},{"title":"CentOS7安装单机版Kudu ","date":"2019-12-06T12:20:39.000Z","path":"大数据技术/60.CentOS7安装单机版Kudu/","text":"CentOS7安装单机版Kudu 下载rpm包 地址：http://archive.cloudera.com/cdh5/redhat/7/x86_64/cdh/5.16.2/RPMS/x86_64/ package: 123456kudu-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpmkudu-client-devel-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm kudu-client0-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm kudu-debuginfo-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm kudu-master-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpmkudu-tserver-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm 安装rpm包 1[root@hadoop000 kudu]$sudo rpm -ivh kudu* 123456789warning: kudu-1.7.0+cdh5.16.2+0-1.cdh5.16.2.p0.24.el7.x86_64.rpm: Header V4 DSA/SHA1 Signature, key ID e8f86acd: NOKEYPreparing... ################################# [100%]Updating / installing... 1:kudu-1.7.0+cdh5.16.2+0-1.cdh5.16.################################# [ 17%] 2:kudu-client0-1.7.0+cdh5.16.2+0-1.################################# [ 33%] 3:kudu-client-devel-1.7.0+cdh5.16.2################################# [ 50%] 4:kudu-master-1.7.0+cdh5.16.2+0-1.c################################# [ 67%] 5:kudu-tserver-1.7.0+cdh5.16.2+0-1.################################# [ 83%] 6:kudu-debuginfo-1.7.0+cdh5.16.2+0-################################# [100%] 创建目录 [root@hadoop000 ~]#mkdir /data/kudu/kudu_master_dat [root@hadoop000 ~]#mkdir /data/kudu/kudu_tserver_data 将/data/kudu目录的权限修改为kudu [root@hadoop000 ~]# chown -R kudu:kudu /data/kudu 修改配置 进入/etc/kudu/conf 1） vi master.gflagfile 12345--trusted_subnets=0.0.0.0/0--fs_wal_dir=/data/kudu/kudu_master_data--fs_data_dirs=/data/kudu/kudu_master_data #设置备份数 不设置默认为3--default_num_replicas=1 2） vi tserver.gflagfile 123456--trusted_subnets=0.0.0.0/0--fs_wal_dir=/data/kudu/kudu_tserver_data--fs_data_dirs=/data/kudu/kudu_tserver_data #设置备份数 不设置默认为3--default_num_replicas=1--tserver_master_addrs=hadoop000:7051 启动/停止kudu 12/etc/init.d/kudu-master start/stop/etc/init.d/kudu-tserver start/stop 查看WEB-UI 通过master的8051端口查看WEB-UI：http://hadoop000:8051/","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Kudu","slug":"Kudu","permalink":"https://zzuuriel.github.io/tags/Kudu/"}]},{"title":"HBase架构和读写流程","date":"2019-08-31T12:12:36.000Z","path":"大数据技术/HBase架构和读写流程/","text":"HBase架构和读写流程 HBase 简介 HBase 是一个分布式，可扩展，面向列的适合存储海量数据的数据库，其最主要的功能是解决海量数据下的实时随机读写的问题。 通常HBase依赖HDFS做为底层分布式文件系统，本文以此做前提并展开，详细介绍HBase的架构和读写流程。 HBase关键进程 HBase是一个Master/Slave架构的分布式数据库，内部主要有HMaster， HRegionServer两个核心服务，依赖HDFS做底层存储，依赖zookeeper做一致性等协调工作。 HMaster是一个轻量级进程，负责所有DDL操作，负载均衡，region信息管理，并在宕机恢复中起主导作用。 HRegionServer管理HRegion，与客户端点对点通信，负责实时数据的读写。 zookeeper做HMaster选举，关键信息如meta-region地址，replication进度，Regionserver地址与端口等存储。 HBase架构 HBase架构图如下： 架构浅析: 1.HMaster 负责管理HBase元数据，即表的结构、表存储的Region等元信息。 负责表的创建，删除和修改（因为这些操作会导致HBase元数据的变动）。 负责为HRegionServer分配Region，分配好后也会将元数据写入相应位置。 负责HRegionserver失效后的region迁移等。 如果对可用性要求较高，它需要做HA高可用（通过Zookeeper）。但是HMaster不会去处理Client端的数据读写请求，因为这样会加大其负载压力，具体的读写请求它会交给HRegionServer来做。 2.HRegionServer 一个RegionServer里有多个Region。 处理Client端的读写请求（根据从HMaster返回的元数据找到对应的Region来读写数据）。 管理Region的Split分裂、StoreFile的Compaction合并。 RegionServer要和【DN】一起部署 一个RegionServer管理着多个Region对象，在HBase运行期间，可以动态添加、删除HRegionServer 3.HRegion 一个HRegion里可能有1个或多个Store，是通过列族划分的。 HRegionServer维护一个HLog。 HRegion是分布式存储和负载的最小单元。 每个Hregion对应table的一个region，表通常被保存在多个HRegionServer的多个HRegion中。 因为HBase用于存储海量数据，故一张表中数据量非常之大，单机一般存不下这么大的数据，故HBase会将一张表按照行水平将大表划分为多个Region，每个Region保存表的一段连续数据。 初始只有1个Region，当一个Region增大到某个阈值后，便分割为两个。 4.Store Store是存储落盘的最小单元，由内存中的MemStore和磁盘中的若干StoreFile组成。 一个Store里有1个或多个StoreFile和一个memStore。 每个Store存储一个列族。 5.HLog 预写日志，Write Ahead Log，WAL。 每个HRegionServer中都有一个HLog对象。 HLog记录数据的所有变更，可以用来做数据恢复。 6.MemStore MemStore作为HBase的写缓存，保存着数据的最近一次更新，同时是HBase能够实现高性能随机读写的重要组成。 HBase Table的每 Column family 维护一个MemStore，当满足一定条件时MemStore会执行一次flush，文件系统中生成新的HFile。而每次Flush的最小单位是Region。 7.StoreFile memStore内存中的数据写到文件后就是StoreFile（即memstore的每次flush操作都会生成一个新的StoreFile），StoreFile底层是以HFile的格式保存。 storefiles合并后逐步形成越来越大的storefile文件，当region内所有的storefile(hfile)的总大小超过 hbase.hregion.max.filesize触发split,一个region变为2个。 父region下线，新的split的2个region被HMaster分配到合适的RegionServer机器上。使得原先1个region的压力分流到2个region上。 8.Block Cache Block Cache作为HBase的读缓存，会将一次文件查找的Block块缓存到Cache中，以便后续同一请求或者邻近数据查找请求，可以直接从内存中获取，避免昂贵的IO操作 Block Cache是RegionServer级别 ，一个RegionServer只有一个Block Cache ,在RegionServer启动时完成Block Cache的初始化工作。 9.zookeeper 集群存储meta表的地址，而不是内容。 regionserver主动向zk组成，使得master可随时感知各个RegionServer的健康状态。 避免master单点故障(SPOF)。 10.HBase Client HBase Client使用HBase的RPC机制与HMaster和HRegionServer进行通信。 对于管理类操作(DDL)，Client与HMaster进行RPC;对于数据读写类操作(DML)，Client与HRegionServer进行RPC。 写流程 HBase写流程 HBase写流程图如下： Client访问ZK，根据ROOT表获取hbase:meta表所在Region的位置信息，并将该位置信息写入Client Cache。（注：为了加快数据访问速度，我们将元数据、Region位置等信息缓存在Client Cache中。） Client读取meta表，再根据meta表中查询得到的Namespace、表名和RowKey等相关信息，获取将要写入Region的位置信息，最后client端会将meta表写入Client Cache。 Client向上一步HRegionServer发出写请求，HRegionServer先将操作和数据写入HLog（预写日志，Write Ahead Log，WAL），再将数据写入对应的region的store的MemStore，MemStore里面的数据也是对rowkey进行字典排序的。（联想：HDFS中也是如此，EditLog写入时机也是在真实读写之前发生） 当MemStore的数据量超过阈值时，将数据溢写磁盘，生成一个StoreFile文件。 当Store中StoreFile的数量超过阈值时，将若干小StoreFile合并（Compact）为一个大StoreFile。 当Region中最大Store的大小超过阈值时，Region分裂（Split），等分成两个子Region。 memstore flush触发条件（调优关键） memstore级别限制： 当region的任意一个store的memstore的size，达到 hbase.hregion.memstore.flush.size（默认128M）， 会触发memstore flush操作,将数据溢写磁盘，生成一个StoreFile文件。 region级别限制: 当region所有的memstore的size和，达到 hbase.hregion.memstore.block.multiplier * hbase.hregion.memstore.flush.size= 4*128=512M 会触发memstore flush，同时会阻塞所有的写入该store的写请求！ 继续对该region写请求，会抛错 region too busy exception异常。 regionserver级别限制： 1）当rs节点上所有的memstore的size和 ，超过低水位线阈值 hbase.regionserver内存大小* hbase.regionserver.global.memstore.size* hbase.regionserver.global.memstore.size.lower.limit =48G * 0.45 * 0.91 = 19.656G， rs强制执行flush。先flush memstore最大的region，再第二大的， 直到总的memstore大小下降到低水位线的阈值。 2）如果此时写入非常繁忙，导致总的memstore大小达到 hbase.regionserver内存大小* hbase.regionserver.global.memstore.size = 21.6G rs会阻塞写、读的请求，并强制flush，达到低水位阈值(安全阈值) Hlog级别限制 当rs的hlog数量达到hbase.regionserver.max.logs 32 会选择最早的hlog的对应的一个或多个region进行flush。 定期级别限制 hbase.regionserver.optionalcacheflushinterval 默认1h 为避免所有的memstore在同一个时间点进行flush导致的问题， 定期的flush其实会有一定的随机时间延时。设置为0就是禁用。 手动级别限制 flush命令，可以封装脚本。 总结： 在生产上，唯独触发rs级别的限制导致flush,是属于灾难级别的，会阻塞所有落在该rs节点的读写请求，直到总的memstore大小降到低水位线，阻塞时间较长；其他的级别限制，只会阻塞对应的region的读写请求，阻塞时间较短。 读流程 HBase读流程图如下： 先去zk获取hbase:meta表所在的rs节点， 在hbase:meta表根据读rk确定所在的目标rs节点和region 将读请求封装，发送给目标的rs节点，进行处理。 先到memstore查数据，查不到到blockcache查，再查不到就访问磁盘的StoreFile读数据。 compaction 合并 每次flush操作都是将一个memstore数据写到HFile文件，导致hdfs上有很多的hfile文件，小文件多了对后面的读操作有影响，所以hbase会定时将hfile文件合并。 分类 Compaction分为两种： minor compaction 小合并: 选取部分小的 相邻的hfile合并为一个更大的hfile major compaction 大合并：将一个store的所有的hfile文件合并一个hfile。 major compaction过程会清理： 1）TTL过期数据 2）版本号超过设定的数据 3）被删除的数据(delete) 所以 major compaction持续时间较长，整个过程消费大量的系统资源（带宽和短时间的IO压力），对上层业务会有较大的影响！ 生产上尽可能的避免发生major compaction，一般通过关闭自动触发大合并，改为手动触发，在业务低谷时期，执行。（一般在凌晨调度脚本，去执行） 作用 随着hflie文件越来越多，查询需要更多的IO，读取延迟较大。所以需要compaction，主要为了消费带宽和短时间的IO压力，来换取以后查询的低延迟。 合并作用： 合并小文件 减少文件数量 减小稳定度的延迟。 消除无效数据 降低存储空间。 提高数据的本地化率。 触发条件 1.minor compaction合并的触发条件： 1）memstore flush： 合并根源来自flush，当memstore达到阈值或者其他条件就触发flush，将数据写到hflie，正是因为文件多，才需要合 并。 每次flush之后，就当前的store文件数进行校验判断，一旦store的总文件数超过 hbase.hstore.compactionThreshold（默认3），就触发合并，该参数一般需要调大。 一次minor cmpaction最多合并hbase.hstore.compaction.max个文件（默认值10）。 2）后台线程定期检查 后台线程compactchecker 定期检查是否需要执行合并。检查周期为 hbase.server.thread.wakefrequency* hbase.server.compactchecker.interval.multiplier =10000ms*1000。在不做参数修改情况的下，compactchecker 大概是2h,46min,40s执行一次。 当文件小于 hbase.hstore.compaction.min.size（默认128M）会被立即添加到合并的队列。 当合并队列中的StoreFile数量超过参数hbase.hstore.compaction.min（更早的版本中这个的参数的名字为hbase.hstore.compactionThreshold）的值（默认3）时会触发compaction操作。一次minor cmpaction最多合并hbase.hstore.compaction.max个文件（默认值10）。 如果一个文件的大小超过hbase.hstore.compaction.max.size的值（默认值LONG.MAX_VALUE），则会被compaction操作排除。 通过hbase.hstore.compaction.ratio参数（默认值1.2）确定大小超过hbase.hstore.compaction.min.size的文件是否需要进行compaction。如果一个文件的大小小于它后面（按文件产生的先后顺序，总是从新产生的文件开始选择即“老文件”）的hbase.hstore.compaction.max个StoreFile的大小之和乘以hbase.hstore.compaction.ratio，则该StoreFile文件也会加入到合并队列中。 12345678910111213141516171819202122232425&lt;!--表示至少需要三个满足条件的store file时，minor compaction才会启动--&gt;&lt;property&gt; &lt;name&gt;hbase.hstore.compactionThreshold&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!--表示一次minor compaction中最多选取10个store file--&gt;&lt;property&gt; &lt;name&gt;hbase.hstore.compaction.max&lt;/name&gt; &lt;value&gt;10&lt;/value&gt;&lt;/property&gt;&lt;!--默认值为128M,表示文件大小小于该值的store file 一定会加入到minor compaction的store file中--&gt;&lt;property&gt; &lt;name&gt;hbase.hstore.compaction.min.size&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt;&lt;/property&gt;&lt;!--默认值为LONG.MAX_VALUE，表示文件大小大于该值的store file 会被minor compaction排除--&gt;&lt;property&gt; &lt;name&gt;hbase.hstore.compaction.max.size&lt;/name&gt; &lt;value&gt;9223372036854775807&lt;/value&gt;&lt;/property&gt; 2.major compaction合并的触发条件： 12345&lt;!--默认值为7天进行一次大合并，--&gt;&lt;property&gt; &lt;name&gt;hbase.hregion.majorcompaction&lt;/name&gt; &lt;value&gt;604800000&lt;/value&gt;&lt;/property&gt; 一般是使用命令major_compact手动进行，将参数hbase.hregion.majorcompaction的值设为0，表示禁用major compaction。其默认值为7天。 生产上： hbase.hregion.majorcompaction 0 大合并关闭 hbase.hstore.compactionThreshold 6（默认3） 小合并","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://zzuuriel.github.io/tags/HBase/"}]},{"title":"HBase的Rowkey设计","date":"2019-08-29T12:12:36.000Z","path":"大数据技术/HBase的Rowkey设计/","text":"HBase的Rowkey设计 为什么Rowkey这么重要 RowKey 到底是什么 我们常说看一张 HBase 表设计的好不好，就看它的 RowKey 设计的好不好。可见 RowKey 在 HBase 中的地位。那么 RowKey 到底是什么？RowKey 的特点如下： 类似于 MySQL、Oracle中的主键，用于标示唯一的行； 完全是由用户指定的一串不重复的字符串； HBase 中的数据永远是根据 Rowkey 的字典排序来排序的 RowKey的作用 读写数据时通过 RowKey 找到对应的 Region； MemStore 中的数据按 RowKey 字典顺序排序； HFile 中的数据按 RowKey 字典顺序排序。 Rowkey对查询的影响 如果我们的 RowKey 设计为 uid+phone+name，那么这种设计可以很好的支持以下的场景： uid = 111 AND phone = 123 AND name = iteblog uid = 111 AND phone = 123 uid = 111 AND phone = 12? uid = 111 难以支持的场景： phone = 123 AND name = iteblog phone = 123 name = iteblog Rowkey对Region划分影响 HBase 表的数据是按照 Rowkey 来分散到不同 Region，不合理的 Rowkey 设计会导致热点问题。热点问题是大量的 Client 直接访问集群的一个或极少数个节点，而集群中的其他节点却处于相对空闲状态。 如果Region1 上的数据是 Region 2 的5倍，这样会导致Region1 的访问频率比较高，进而影响这Region1 所在机器的其他 Region。 RowKey设计技巧 我们如何避免上面说到的热点问题呢？下面介绍三种方法。 避免热点的方法 - Salting 这里的加盐不是密码学中的加盐，而是在rowkey 的前面增加随机数。具体就是给 rowkey 分配一个随机前缀 以使得它和之前排序不同。分配的前缀种类数量应该和你想使数据分散到不同的 region 的数量一致。 如果你有一些 热点 rowkey 反复出现在其他分布均匀的 rwokey 中，加盐是很有用的。考虑下面的例子：它将写请求分散到多个 RegionServers，但是对读造成了一些负面影响。 假如你有下列 rowkey，你表中每一个 region 对应字母表中每一个字母。 以 ‘a’ 开头是同一个region, 'b’开头的是同一个region。在表中，所有以 'f’开头的都在同一个 region， 它们的 rowkey 像下面这样： 1234foo0001foo0002foo0003foo0004 现在，假如你需要将上面这个 region 分散到 4个 region。你可以用4个不同的盐：‘a’, ‘b’, ‘c’, ‘d’.在这个方案下，每一个字母前缀都会在不同的 region 中。加盐之后，你有了下面的 rowkey: 1234a-foo0003b-foo0001c-foo0004d-foo0002 所以，你可以向4个不同的 region 写。理论上说，如果这四个 Region 存放在不同的机器上，经过加盐之后你将拥有之前4倍的吞吐量。 现在，如果再增加一行，它将随机分配a,b,c,d中的一个作为前缀，并以一个现有行作为尾部结束： 12345a-foo0003b-foo0001c-foo0003c-foo0004d-foo0002 因为分配是随机的，所以如果你想要以字典序取回数据，你需要做更多工作。加盐这种方式增加了写时的吞吐量，但是当读时有了额外代价。 避免热点的方法 - Hashing Hashing 的原理是计算 RowKey 的 hash 值，然后取 hash 的部分字符串和原来的 RowKey 进行拼接。这里说的 hash 包含 MD5、sha1、sha256或sha512等算法。比如我们有如下的 RowKey: 1234foo0001foo0002foo0003foo0004 我们使用 md5 计算这些 RowKey 的 hash 值，然后取前 6 位和原来的 RowKey 拼接得到新的 RowKey： 123495f18cfoo00016ccc20foo0002b61d00foo00031a7475foo0004 优缺点：可以一定程度打散整个数据集，但是不利于 Scan；比如我们使用 md5 算法，来计算Rowkey的md5值，然后截取前几位的字符串。subString(MD5(设备ID), 0, x) + 设备ID，其中x一般取5或6。 避免热点的方法 - Reversing Reversing 的原理是反转一段固定长度或者全部的键。比如我们有以下 URL ，并作为 RowKey： 1234flink.iteblog.comwww.iteblog.comcarbondata.iteblog.comdef.iteblog.com 这些 URL 其实属于同一个域名，但是由于前面不一样，导致数据不在一起存放。我们可以对其进行反转，如下： 1234moc.golbeti.knilfmoc.golbeti.wwwmoc.golbeti.atadnobracmoc.golbeti.fed 经过这个之后，这些 URL 的数据就可以放一起了。 RowKey的长度 RowKey 可以是任意的字符串，最大长度64KB（因为 Rowlength 占2字节）。建议越短越好，原因如下： 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率； MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率； 目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。 RowKey 设计案例剖析 交易类表 Rowkey 设计 查询某个卖家某段时间内的交易记录 sellerId + timestamp + orderId 查询某个买家某段时间内的交易记录 buyerId + timestamp ＋orderId 根据订单号查询 orderNo 如果某个商家卖了很多商品，可以如下设计 Rowkey 实现快速搜索 salt + sellerId + timestamp 其中，salt 是随机数。 可以支持的场景： 1)全表 Scan 2)按照 sellerId 查询 3)按照 sellerId + timestamp 查询 金融风控 Rowkey 设计 查询某个用户的用户画像数据 prefix + uid prefix + idcard prefix + tele 其中 prefix = substr(md5(uid),0 ,x)， x 取 5-6。uid、idcard以及 tele 分别表示用户唯一标识符、身份证、手机号码。 车联网 Rowkey 设计 查询某辆车在某个时间范围的交易记录 carId + timestamp 某批次的车太多，造成热点 prefix + carId + timestamp 其中 prefix = substr(md5(uid),0 ,x) 查询最近的数据 查询用户最新的操作记录或者查询用户某段时间的操作记录，RowKey 设计如下： uid + Long.Max_Value - timestamp 查询用户最新的操作记录 Scan [uid] startRow [uid][000000000000] stopRow [uid][Long.Max_Value - timestamp] 查询用户某段时间的操作记录 Scan [uid] startRow [uid][Long.Max_Value – startTime] stopRow [uid][Long.Max_Value - endTime] 如果 RowKey 无法满足我们的需求，可以尝试二级索引。Phoenix、Solr 以及 ElasticSearch 都可以用于构建二级索引","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"HBase","slug":"HBase","permalink":"https://zzuuriel.github.io/tags/HBase/"}]},{"title":" Spark Streaming中foreachRDD的使用 ","date":"2019-08-06T12:15:39.000Z","path":"大数据技术/9.Spark Streaming中foreachRDD的使用/","text":"Spark Streaming中foreachRDD的使用 概述 在Spark 官网中，foreachRDD被划分到Output Operations on DStreams中，所以我们首先要明确的是，它是一个输出操作的算子，然后再来看官网对它的含义解释： 最常用的输出操作 需要一个函数作为参数，函数作用于DStream中的每一个RDD 函数将RDD中的数据输出到外部系统，如文件、数据库 函数在driver上执行 函数中通常要有action算子，因为foreachRDD本身是transform算子 测试 需求：用Spark Streaming把WC结果写到MySQL MySQLUtils 12345678910111213object MySQLUtils &#123; def getConnection() = &#123; Class.forName(\"com.mysql.jdbc.Driver\") DriverManager.getConnection(\"jdbc:mysql://hadoop000:3306/test\", \"root\", \"123456\") &#125; def closeConnection(connection: Connection): Unit =&#123; if(null != connection) &#123; connection.close() &#125; &#125;&#125; Version1.0 123456789101112 val result = stream.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _) result.foreachRDD(rdd =&gt; &#123; val connection = MySQLUtils.getConnection() //executed at the driver rdd.foreach(pair =&gt; &#123; val sql = \"insert into wc(word,cnt) values(?,?)\" val statement = connection.prepareStatement(sql) //executed at the worker statement.setString(1, pair._1) statement.setInt(2, pair._2) statement.execute() &#125;) MySQLUtils.closeConnection(connection)&#125;) 执行后会报错，Task not serializable 我们使用foreachRDD向外部系统输出数据时，通常要创建一个连接对象，如果像上面的代码中创建在driver上，而foreach这个算子是在executor端执行，connection不是序列化的。通常会报序列化错误或者初始化错误。其实这是一个闭包问题,在函数内部引用了一个外部的变量。 Version2.0 为了解决闭包问题，我们把connection放到foreach中。 123456789val result = stream.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _)result .foreachRDD(rdd =&gt; &#123; rdd.foreach(pair =&gt; &#123; val connection = MySQLUtils.getConnection() //executed at the driver val sql = s\"insert into wc(word,cnt) values('$&#123;pair._1&#125;', $&#123;pair._2&#125;)\" connection.createStatement().execute(sql) MySQLUtils.closeConnection(connection) &#125;)&#125;) 这样虽然不会报错，但是foreach中的每一个元素都会创建连接对象，浪费资源，foreach适用于对每一个元素进行操作的场景。 Version3.0 要创建连接对象时一般使用foreachPartition来解决这个问题，这样每个partition中只创建一个连接对象，使用它来对该partition内的每个元素进行输出。 12345678910val result = stream.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _)result.foreachRDD(rdd =&gt; &#123; rdd.foreachPartition(partition =&gt; &#123; val connection = MySQLUtils.getConnection() //每个分区创建一个partition partition.foreach(pair =&gt; &#123; val sql = s\"insert into wc(word,cnt) values('$&#123;pair._1&#125;',$&#123;pair._2&#125;)\" connection.createStatement().execute(sql) &#125;) MySQLUtils.closeConnection(connection)&#125;) Version4.0 更进一步的话，在处理一批RDD时，可以使用数据库连接池来重复使用连接对象，注意连接池必须是静态、懒加载的。 可以借助Scalikejdbc来实现：http://scalikejdbc.org/ 1）加入依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc_$&#123;scala.tools.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;scalikejdbc.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.scalikejdbc&lt;/groupId&gt; &lt;artifactId&gt;scalikejdbc-config_$&#123;scala.tools.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;scalikejdbc.version&#125;&lt;/version&gt;&lt;/dependency&gt; 2）创建数据库连接配置信息 resources下创建application.conf 12345678910# JDBC Settingsdb.default.driver = \"com.mysql.jdbc.Driver\"db.default.url = \"jdbc:mysql://localhost:3306/demo?useSSL=false&amp;serverTimezone=UTC\"db.default.user = \"root\"db.default.password = \"123456\" # Connection Pool settingsdb.default.poolInitialSize = 10db.default.poolMaxSize = 20db.default.connectionTimeoutMillis = 1000 3）使用 1234567891011121314151617val result = stream.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _)DBs.setupAll() //解析配置文件application.confresult.foreachRDD(rdd =&gt; &#123; rdd.foreachPartition(partition =&gt; &#123; partition.foreach(pair =&gt; &#123; DB.autoCommit &#123;implicit session =&gt; &#123; // 默认就使用了连接池 SQL(\"insert into wc(word,cnt) values(?,?)\") .bind(pair._1, pair._2) .update() .apply() &#125; &#125; &#125;) &#125;)&#125;)","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":" Spark工作模式详解(local/standalone/yarn) ","date":"2019-07-15T06:15:39.000Z","path":"大数据技术/8.spark工作模式详解(localstandaloneyarn)/","text":"Spark工作模式详解(local/standalone/yarn) Spark 运行模式分类 本地模式 standalone模式 spark on yarn 模式，又分未yarn-client和yarn-cluster 本地模式-local Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类 local：只启动一个executor local[k]:启动k个executor local[*]：启动跟cpu数目相同的executor 注意： Spark单机运行，一般用于开发测试。 Local模式又称为本地模式，运行该模式非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用，而不用启动Spark的Master、Worker守护进程( 只有集群的Standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别 。 SparkSubmit进程，既是客户提交任务的Client进程、又是Spark的driver程序、还充当着Spark执行Task的Executor角色。 standalone模式 构建一个由Master+Slave构成的Spark集群，Spark运行在集群中。 分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础。 和单机运行的模式不同，这里必须在执行应用程序前，先启动Spark的Master和Worker守护进程。不用启动Hadoop服务，除非你用到了HDFS的内容 。 standalone模式角色说明: Master进程做为cluster manager，用来对应用程序申请的资源进行管理。 SparkSubmit做为Client端和运行driver程序。 CoarseGrainedExecutorBackend 用来并发执行应用程序。 Driver运行的位置: Standalone模式是Spark实现的资源调度框架，其主要的节点有Client节点、Master节点和 Worker节点。其中Driver既可以运行在Master节点上中，也可以运行在本地Client端。 当用spark-shell交互式工具提交Spark的Job时，Driver在Master节点上运行； 当使用spark-submit工具提交Job或者在Eclips、IDEA等开发平台上使用”new SparkConf.setManager(“spark://master:7077”)”方式运行Spark任务时，Driver是运行在本地Client端上的。 standalone运行流程: 1.SparkContext连接到Master，向Master注册并申请资源（CPU Core 和Memory）； 2.Master根据SparkContext的资源申请要求和Worker心跳周期内报告的信息决定在哪个Worker上分配资源，然后在该Worker上获取资源，然后启动StandaloneExecutorBackend； 3.StandaloneExecutorBackend向SparkContext注册； 4.SparkContext将Applicaiton代码发送给StandaloneExecutorBackend；并且SparkContext解析Applicaiton代码，构建DAG图，并提交给DAG Scheduler分解成Stage（当碰到Action操作时，就会催生Job；每个Job中含有1个或多个Stage，Stage一般在获取外部数据和shuffle之前产生），DAG Scheduler将TaskSet提交给Task Scheduler，Task Scheduler负责将Task分配到相应的Worker，最后提交给StandaloneExecutorBackend执行； 5.StandaloneExecutorBackend会建立Executor线程池，开始执行Task，并向SparkContext报告，直至Task完成； 6.所有Task完成后，SparkContext向Master注销，释放资源。 Spark on Yarn模式 Spark客户端直接连接Yarn。不需要额外构建Spark集群。 分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端。 Spark on yarn client模式 Driver运行位置: Driver在本地运行，并没有在nodemanager上，在nodemanager上启动的applicationMaster仅仅是一个ExecutorLanucher，功能十分有限。 运行流程: 1.Spark Yarn Client向YARN的ResourceManager发送请求，申请启动Application Master。同时在SparkContext初始化中将创建DAGScheduler和TASKScheduler等，由于我们选择的是Yarn-Client模式，程序会选择YarnClientClusterScheduler和YarnClientSchedulerBackend； 2.ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster（实际启动的是ExecutorLanucher，功能十分有限），与YARN-Cluster区别的是在该ApplicationMaster不运行SparkContext，只与SparkContext进行联系进行资源的分派的ExecutorLanucher； 3.Client中的SparkContext初始化完毕后，与ApplicationMaster建立通讯，向ResourceManager注册，根据任务信息向ResourceManager申请资源（Container）； 4.一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向Client中的SparkContext注册并申请Task； 5.Client中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向Driver汇报运行的状态和进度，以让Client随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 6.应用程序运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己。 spark on yarn cluster模式 Driver 运行位置: Driver运行在nodemanager上。 运行流程: Spark Yarn Client向YARN中resourcemanager提交应用程序，包括ApplicationMaster程序、启动ApplicationMaster的命令、需要在Executor中运行的程序等； ResourceManager收到请求后，在集群中选择一个NodeManager，为该应用程序分配第一个Container，要求它在这个Container中启动应用程序的ApplicationMaster(相当于Driver客户端)，其中ApplicationMaster进行SparkContext等的初始化； ApplicationMaster向ResourceManager注册，这样用户可以直接通过ResourceManage查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束； 一旦ApplicationMaster申请到资源（也就是Container）后，便与对应的NodeManager通信，要求它在获得的Container中启动启动CoarseGrainedExecutorBackend，CoarseGrainedExecutorBackend启动后会向ApplicationMaster中的SparkContext注册并申请Task。这一点和Standalone模式一样，只不过SparkContext在Spark Application中初始化时，使用CoarseGrainedSchedulerBackend配合YarnClusterScheduler进行任务的调度，其中YarnClusterScheduler只是对TaskSchedulerImpl的一个简单包装，增加了对Executor的等待逻辑等； ApplicationMaster中的SparkContext分配Task给CoarseGrainedExecutorBackend执行，CoarseGrainedExecutorBackend运行Task并向ApplicationMaster汇报运行的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 应用程序运行完成后，ApplicationMaster向ResourceManager申请注销并关闭自己。 Spark on yarn client与Spark on yarn Cluster之间的区别 yarn-client： 用于测试，因为driver运行在本地客户端，负责调度application，会与yarn集群产生超大量的网络通信。好处是直接执行时，本地可以看到所有的log，方便调试。 Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。 yarn-cluster： 生产环境使用， 因为driver运行在nodemanager上，缺点在于调试不方便，本地用spark-submit提价以后，看不到log，只能通过yarn application-logs application_id这种命令查看，很麻烦。 Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业。 总结： 理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark调优详图","date":"2019-07-03T12:19:36.000Z","path":"大数据技术/Spark调优详图/","text":"","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Core-分组求topn","date":"2019-07-02T12:17:36.000Z","path":"大数据技术/Spark Core-分组求topn/","text":"Spark Core-分组求topn Spark Core-分组求topn 需求： 求相同域名下，访问次数最多的前2个url。 思考： 1.toList能不能用？toList容易引起OOM 2.域名domain如何取出？distinct 3.rdd能嵌套rdd么？ val domains = processRDD.map(_._1._1).distinct().collect()不加collect试试 4.自定义分区器的好处？Web UI查看 虽然做了distinct,但是域名如果多了，UI还是会爆掉(sortBy会很多)，故自定义一个分区器，相同域名放在同一个分区 5.take需要将所有数据都拉取到Driver上才能完成操作，如何优化？treeSet 方法1： 12345678910111213141516171819202122232425/** * toList会炸掉，生产上不能这么用 */object TopNApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val topN = 2; val lines = sc.textFile(\"ruozedata-spark-core/data/site.log\") lines.map(x =&gt; &#123; val splits = x.split(\",\") val domain = splits(10) val url = splits(14) ((domain, url), 1) &#125;).reduceByKey(_+_) .groupBy(_._1._1) // 取出domain，然后分组 .mapValues(x =&gt; &#123; x.toList.sortBy(-_._2).map(x =&gt; (x._1._2, x._2)).take(topN) &#125;) .foreach(println) sc.stop() &#125;&#125; 方法2： 12345678910111213val lines = sc.textFile(\"/ruozedata-spark-core/topn/site.log\")val processRDD = lines.map(x =&gt; &#123; val splits = x.split(\",\") val domain = splits(10) val url = splits(14) ((domain, url), 1)&#125;)val domains = processRDD.map(_._1._1).distinct().collect()domains.foreach(x =&gt; &#123; processRDD.filter(_._1._1 == x).reduceByKey(_ + _) .sortBy(-_._2).take(topN)&#125;) 每个domian进行sortBy、take都是会触发action，domian太多UI会炸掉 方法3、方法4： 自定义分区器，将所有domain放到一个分区，再触发action，UI上的job会少很多。 12345678910111213class RuozedataPartitioner(domains: Array[String]) extends Partitioner&#123; val map = mutable.HashMap[String, Int]()// map的key是domain，value是int for(i&lt;-0 until(domains.length)) &#123; map(domains(i)) = i &#125; override def numPartitions: Int = domains.length override def getPartition(key: Any): Int = &#123;// ((domain, url), 1)，key是 tuple类型 val domain = key.asInstanceOf[(String, String)]._1 map(domain) &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041object TopNApp03 &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val topN = 1; val lines = sc.textFile(\"ruozedata-spark-core/data/site.log\") val processRDD = lines.map(x =&gt; &#123; val splits = x.split(\"\\t\") val domain = splits(10) val url = splits(14) ((domain, url), 1) &#125;) val domains = processRDD.map(_._1._1).distinct().collect() //相同的domain放到一个分区中去 val result = processRDD.reduceByKey(new RuozedataPartitioner(domains),_ + _) //方法3：take// result.mapPartitions(partition =&gt; &#123;// partition.toList.sortBy(-_._2).take(topN).iterator// &#125;).foreach(println)//方法4：使用treeSet // TODO... Tree... order var treeSet = new mutable.TreeSet[((String,String), Int)]()(new MyOrdering()) result.mapPartitions(partition =&gt; &#123; partition.foreach(x =&gt; &#123; // TODO... treeSet.add(x) treeSet.add(x) if(treeSet.size &gt; topN) &#123; treeSet.remove(treeSet.last) &#125; &#125;) treeSet.iterator &#125;).foreach(println) sc.stop() &#125;&#125; 自定义排序： 1234567class MyOrdering extends Ordering[((String,String),Int)]&#123; override def compare(x: ((String,String),Int), y: ((String,String),Int)): Int = &#123; val xField = x._2.toInt val yField = y._2.toInt yField - xField &#125;&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"累加器和广播变量&&持久化","date":"2019-06-28T12:17:36.000Z","path":"大数据技术/累加器和广播变量&&持久化/","text":"累加器和广播变量&amp;&amp;持久化 累加器和广播变量 Spark有两种共享变量：广播变量（broadcast variable）与累加器（accumulator） 累加器用来对信息进行聚合，而广播变量用来高效分发较大的对象。 共享变量出现的原因： 通常情况下，当向Spark操作(如map,reduce)传递一个函数时，它会在一个远程集群节点上执行，它会使用函数中所有变量的副本。这些变量被复制到所有的机器上，远程机器上并没有被更新的变量向驱动程序回传。通常跨任务的读写变量是低效的，但是，Spark还是提供了两种有限的共享变量：广播变量（broadcast variable）和累加器 累加器 在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到累加器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会在driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。 累加器的用法如下所示： (1)通过在Driver中调用 SparkContext.accumulator(initialValue) 方法，创建出存有初始值的累加器。返回值为 org.apache.spark.Accumulator[T] 对象，其中 T 是初始值initialValue 的类型。 (2)Spark闭包（函数序列化）里的excutor代码可以使用累加器的 += 方法（在Java中是 add ）增加累加器的值。 (3)Driver程序可以调用累加器的 value 属性（在 Java 中使用 value() 或 setValue() ）来访问累加器的值。 计数器种类很多，但是经常使用的就是两种，longAccumulator和collectionAccumulator 需要注意的是计数器是lazy的，只有触发action才会进行计数，在不持久的情况下重复触发action,计数器会重复累加 1、LongAccumulator 1234567891011121314val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"MyLongAccumulator\") val sc = new SparkContext(sparkConf) val acc = sc.longAccumulator(\"计数\") val rdd = sc.parallelize(List(1,2,3,4,5,6,7,8,9)) val numberRDD = rdd.map(x=&gt;&#123; acc.add(1L) &#125;) numberRDD.count() println(acc.value) //9 numberRDD.count() println(acc.value) //18 numberRDD.count() println(acc.value) //27 sc.stop() 使用longAccumulator做计数的时候要小心重复执行action导致的acc.value的变化,这是因为重复执行了count,累加器的数量成倍增长，解决方法，在action操作之前调用rdd的cache方法（或persist）,这样在count后数据集就会被缓存下来，而无需从头开始计算 1234567numberRDD.cache() numberRDD.count() println(acc.value) //9 numberRDD.count() println(acc.value) //9 numberRDD.count() println(acc.value) //9 2、CollectionAccumulator CollectionAccumulator,集合计数器，计数器中保存的是集合元素，通过泛型指定 12345678910111213141516171819202122def main(args: Array[String]): Unit = &#123; /** * 需求:id后三位相同的加入计数器 */ val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"MyLongAccumulator\") val sc = new SparkContext(sparkConf) //生成集合计数器 val acc = sc.collectionAccumulator[People](\"集合计数器\") //生成RDD val rdd = sc.parallelize(Array(People(\"p1\", 100000), People(\"p2\", 100001), People(\"p3\", 100222), People(\"p4\", 100003))) rdd.map(x=&gt;&#123; val id = x.id.toString.reverse //满足条件就加入计数器 if(id(0) == id(1) &amp;&amp; id(0) == id(2))&#123; acc.add(x) &#125; &#125;).count() println(acc.value) //[People(p1,100000), People(p3,100222)] sc.stop() &#125; case class People(name:String,id:Long); 广播变量 如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么只是每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。 小表广播案例 spark有一种常见的优化方式就是小表广播，使用map join来代替reduce join,我们通过把小表的数据集广播到各个节点上，节省了shuffle操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\") .setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) // Fact table 航线(起点机场, 终点机场, 航空公司, 起飞时间) val flights = sc.parallelize(List( (\"SEA\", \"JFK\", \"DL\", \"7:00\"), (\"SFO\", \"LAX\", \"AA\", \"7:05\"), (\"SFO\", \"JFK\", \"VX\", \"7:05\"), (\"JFK\", \"LAX\", \"DL\", \"7:10\"), (\"LAX\", \"SEA\", \"DL\", \"7:10\"))) // Dimension table 机场(简称, 全称, 城市, 所处城市简称) val airports = sc.parallelize(List( (\"JFK\", \"John F. Kennedy International Airport\", \"New York\", \"NY\"), (\"LAX\", \"Los Angeles International Airport\", \"Los Angeles\", \"CA\"), (\"SEA\", \"Seattle-Tacoma International Airport\", \"Seattle\", \"WA\"), (\"SFO\", \"San Francisco International Airport\", \"San Francisco\", \"CA\"))) // Dimension table 航空公司(简称,全称) val airlines = sc.parallelize(List( (\"AA\", \"American Airlines\"), (\"DL\", \"Delta Airlines\"), (\"VX\", \"Virgin America\"))) //最终统计结果： //出发城市 终点城市 航空公司名称 起飞时间 //Seattle New York Delta Airlines 7:00 //San Francisco Los Angeles American Airlines 7:05 //San Francisco New York Virgin America 7:05 //New York Los Angeles Delta Airlines 7:10 //Los Angeles Seattle Delta Airlines 7:10 val airportsBc = sc.broadcast(airports.map(x =&gt; (x._1, x._3)).collectAsMap()) val airlinesBc = sc.broadcast(airlines.collectAsMap()) flights.map&#123; case (a,b,c,d) =&gt; (airportsBc.value.get(a).get, airportsBc.value.get(b).get, airlinesBc.value.get(c).get, d ) &#125;.foreach(println) sc.stop() &#125; 为什么只能 broadcast 只读的变量 这就涉及一致性的问题，如果变量可以被更新，那么变量被某个节点更新，其他节点需要一块更新，这涉及了事务一致性。 注意事项： 变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改 能不能将一个RDD使用广播变量广播出去？因为RDD是不存储数据的。可以将RDD的结果广播出去。 广播变量只能在Driver端定义，不能在Executor端定义。 在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。 如果Executor端用到了Driver的变量，不使用广播变量在Executor有多少task就有多少Driver端的变量副本。 如果Executor端用到了Driver的变量，使用广播变量在每个Executor中只有一份Driver端的变量副本。 持久化 Spark非常重要的一个功能特性就是可以将RDD持久化在内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，直接使用内存缓存的partition。这样的话，对于针对一个RDD反复执行多个操作的场景，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要反复计算多次该RDD。 巧妙使用RDD持久化，甚至在某些场景下，可以将spark应用程序的性能提升10倍。对于迭代式算法和快速交互式应用来说，RDD持久化，是非常重要的。 持久化的存储级别很多，常用的是MEMORY_ONLY、MEMORY_ONLY_SER、MEMORY_AND_DISK 如何选择存储级别？ Storage Level的选择是内存和CPU的权衡 1.如果内存足够，默认的存储级别（MEMORY_ONLY (不进行序列化)）是性能最优的，最高效的。 2.如果内存不够且CPU跟的上，可以尝试MEMORY_ONLY_SER 再加上一个序列化框架(kyro），这样内存的空间更好。 3.不要把数据写到磁盘，这样成本是非常高的，当数据太大的时候，可以过滤一部分的数据再存，这样的话可能会更快。 4.可以使用副本的存储级别能更快的容错，所有的storage level都提供了副本机制（从另外的节点拿）。 首选第1种方式，如果满足不了再使用第2种。后两种不推荐。 12345scala&gt; numberRDD.cacheres18: forRDD.type = MapPartitionsRDD[9] at map at &lt;console&gt;:27scala&gt; numberRDD.countres19: Long = 8 结果可以在Web UI的Storage中查看 Spark自动监视每个节点上的缓存使用情况，并以最近最少使用(LRU)的方式删除旧的数据分区。如果想要手动删除一个RDD，而不是等待它从缓存中消失，那么可以使用RDD.unpersist()方法,清除缓存数据是立即执行的 12scala&gt; numberRDD.unpersist()res8: numberRDD.type = MapPartitionsRDD[3] at map at &lt;console&gt;:28 修改存储级别 1234val forRDD = rdd.map(x =&gt; &#123; //计数器做累加 acc.add(1L)&#125;).persist(StorageLevel.MEMORY_ONLY_SER).count() cache和persist有什么区别和联系？ 使用cache()和persist()进行持久化操作，它们都是lazy的，需要action才能触发，默认使用MEMORY_ONLY cache调用的persist，persist调用的persist(storage level) 序列化和非序列化有什么区别？ 序列化将对象转换成字节数组了，节省空间，占CPU 测试：开启kyro序列化（需要注册） 1234567891011121314151617val sparkConf = new SparkConf() .setMaster(\"local[*]\").setAppName(\"SerApp\") ---------开启序列化------ .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") ------注册----- .registerKryoClasses(Array(classOf[Student])) /** * 10w条数据 原大小2908KB * 1:Kryo serialization 注册 2029.1KB * 2:Java serialization 3.1 MB * 3:Kryo serializationkryo 不注册 4.8 MB * */ rdd.persist(StorageLevel.MEMORY_ONLY_SER).count() case class Student(id: String, name: String, age: Int)","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":" Spark Core基础-WordCount增强","date":"2019-06-21T12:17:36.000Z","path":"大数据技术/Spark Core基础-WordCount增强/","text":"Spark Core基础-WordCount增强 生产上很多问题 都是WordCount或者WordCount的变形，下面列举了3个关于WordCount的例子。 WordCount01App 需求： 123456a,1,3a,2,3b,1,1==&gt;a,3,6b,1,1 实现： 123456789101112131415161718192021222324object WordCount01App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) sc.parallelize(List( List(\"a\",1,3), List(\"a\",2,4), List(\"b\",1,1) )).map(x =&gt; &#123; val key = x(0) val v1 = x(1).toString.toInt val v2 = x(2).toString.toInt (key,(v1,v2)) &#125;).reduceByKey((x,y) =&gt; &#123; (x._1+y._1, x._2+y._2) &#125;).map(x =&gt; (x._1,x._2._1,x._2._2)) .foreach(println) sc.stop() &#125;&#125; WordCount02App 需求： 123456789用户 节目 列表 点击001，一起看|电视剧|军旅|亮剑，1,1001，一起看|电视剧|军旅|亮剑，1,0002，一起看|电视剧|军旅|士兵突击，1,1==&gt;001,一起看，2,1001，电视剧，2,1001，军旅，2,1001，亮剑，2,1 实现： 123456789101112131415161718192021222324252627282930313233object WordCount02App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val lines = sc.parallelize(List( \"001,一起看|电视剧|军旅|亮剑,1,1\", \"001,一起看|电视剧|军旅|亮剑,1,0\", \"002,一起看|电视剧|军旅|士兵突击,1,1\" )) lines.flatMap(x =&gt; &#123; val splits = x.split(\",\") val id = splits(0) val word = splits(1) val imp = splits(2).toInt val click = splits(3).toInt val words = word.split(\"\\\\|\")//两种实现方法，一种是reducebykey，一种是groupbykey words.map(x =&gt; ((id,x),(imp,click))) &#125;).groupByKey().mapValues(x =&gt; &#123; val imps = x.map(_._1).sum val clicks = x.map(_._2).sum (imps, clicks) &#125;).foreach(println) //.reduceByKey((x,y) =&gt; (x._1+y._1, x._2+y._2)).foreach(println) sc.stop() &#125;&#125; WordCount03App 需求： 多目录输出文件，按照使用平台作为key，输出文件名为使用平台名。 实现： Spark内部没有多文件输出的函数供大家直接调用，需要我们自定义一个类继承与MultipleOutputFormat，代码如下： 123456789101112131415161718/** * 继承MultipleTextOutputFormat[Any,Any]，实现多目录输出 */class RuozedataMultipleTextOutputFormat extends MultipleTextOutputFormat[Any,Any]&#123; override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = &#123; s\"$key/$name\"// 自定义输出路径// s\"$key/20281022\" &#125; //key的输出 override def generateActualKey(key: Any, value: Any): Any = NullWritable.get() //value的输出 override def generateActualValue(key: Any, value: Any): Any = &#123; value.asInstanceOf[String] &#125;&#125; 调用saveAsHadoopFile函数并传入自定义的RuozedataMultipleTextOutputFormat类，代码如下 1234567891011121314151617object WordCount03App &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val out = \"out\" FileUtils.delete(sc.hadoopConfiguration, out) sc.textFile(\"ruozedata-spark-core/data/access.log\",1) .map(x =&gt; &#123; val splits = x.split(\"\\t\") (splits(1), x) &#125;).saveAsHadoopFile(out, classOf[String],classOf[String],classOf[RuozedataMultipleTextOutputFormat]) sc.stop() &#125;&#125; access.log示例： 12345ruozedata Android 10.0.2ruozedata Symbian 10.0.1ruozedata Symbian 10.0.2ruozedata iOS 10.0.1ruozedata iOS 10.0.2","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Core基础之Spark监控","date":"2019-06-19T13:12:36.000Z","path":"大数据技术/Spark Core基础之Spark监控/","text":"Spark Core基础之Spark监控 官方文档:http://spark.apache.org/docs/latest/monitoring.html 历史日志监控 如果应用程序的事件日志存在，仍然可以通过 spark 的历史服务器构建应用程序的UI。 可以通过执行以下命令来启动历史服务器: 1./sbin/start-history-server.sh 这将创建一个默认 http://&lt; server-url &gt;:18080 的 web 界面，列出不完整和已完成的应用程序和尝试。 spark 作业本身必须配置为记录事件，并将它们记录到同一个共享的可写目录。 例如，如果服务器配置了hdfs://namenode/shared/spark-logs的日志目录， 那么客户端的选择就是: 在spark/conf下的spark-defaults.conf配置 12spark.eventLog.enabled truespark.eventLog.dir hdfs://namenode/shared/spark-logs 在spark-env.sh中配置 12默认端口为18080export SPARK_HISTORY_OPTS=\"-Dspark.history.fs.logDirectory=hdfs://namenode/shared/spark-logs -Dspark.history.retainedApplications=30 -Dspark.history.ui.port=47653\" 参数说明： spark.eventLog.dir：Application在运行过程中所有的信息均记录在该属性指定的路径下； spark.history.ui.port=18080 WEBUI访问的端口号为18080 spark.history.fs.logDirectory 配置了该属性后，在start-history-server.sh时就无需再显式的指定路径，Spark History Server页面只展示该指定路径下的信息 spark.history.retainedApplications=30 指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。 web 界面地址： http://&lt; server-url &gt;:47653 代码监控（ metrics） 1.创建一个监听器，继承于SparkListener 1234567891011121314151617181920212223242526272829303132333435363738394041class RuozedataSparkListener(conf:SparkConf) extends SparkListener with Logging&#123; override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = &#123; /** * 从TaskMetrics能够拿到很多信息，可以把信息都持久化到Hbase/Redis/RDBMS * private val _executorDeserializeTime = new LongAccumulator * private val _executorDeserializeCpuTime = new LongAccumulator * private val _executorRunTime = new LongAccumulator * private val _executorCpuTime = new LongAccumulator * private val _resultSize = new LongAccumulator * private val _jvmGCTime = new LongAccumulator * private val _resultSerializationTime = new LongAccumulator * private val _memoryBytesSpilled = new LongAccumulator * private val _diskBytesSpilled = new LongAccumulator * private val _peakExecutionMemory = new LongAccumulator * private val _updatedBlockStatuses = new CollectionAccumulator[(BlockId, BlockStatus)] */ //通过SparkConf拿到Spark作业的名字 val appname = conf.get(\"spark.app.name\") val metrics = taskEnd.taskMetrics val taskMetricsMap = mutable.HashMap( \"executorDeserializeTime\" -&gt; metrics.executorDeserializeTime, \"executorDeserializeCpuTime\" -&gt; metrics.executorDeserializeCpuTime, \"executorRunTime\" -&gt; metrics.executorRunTime, \"resultSize\" -&gt; metrics.resultSize, \"taskId\" -&gt; taskEnd.taskInfo.taskId ) logError(appname) //如果spark.send.mail.enabled为true，发送邮件，否则不发送 if (\"true\" == conf.get(\"spark.send.mail.enabled\")) &#123; MsgUtils.send(\"桶子\",s\"$appname _task\", Json(DefaultFormats).write(taskMetricsMap)) &#125; &#125;&#125; 2.邮件发送 12345678910111213141516171819202122232425262728293031323334public class MsgUtils &#123; public static void send(String recivers, String title, String content) throws Exception &#123; Properties properties = new Properties(); properties.setProperty(\"mail.host\",\"smtp.qq.com\"); properties.setProperty(\"mail.transport.protocol\",\"smtp\"); properties.setProperty(\"mail.smtp.auth\", \"true\"); properties.setProperty(\"mail.smtp.ssl.enable\",\"true\"); MailSSLSocketFactory factory = new MailSSLSocketFactory(); factory.setTrustAllHosts(true); properties.put(\"mail.smtp.ssl.socketFactory\", factory); Authenticator authenticator = new Authenticator() &#123; @Override protected PasswordAuthentication getPasswordAuthentication() &#123; String username = \"123456@qq.com\"; String password = \"mmuvyreksgfudhhb\"; return new PasswordAuthentication(username, password); &#125; &#125;; Session session = Session.getInstance(properties, authenticator); MimeMessage message = new MimeMessage(session); InternetAddress from = new InternetAddress(\"123456@qq.com\"); message.setFrom(from); InternetAddress[] tos = InternetAddress.parse(\"5555555@qq.com\"); message.setRecipients(Message.RecipientType.TO, tos); message.setSubject(title); message.setContent(content, \"text/html;charset=UTF-8\"); Transport.send(message); &#125; 3.测试使用自定义监听器 1234567891011121314151617object SparkListenerWC &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf() .setMaster(\"local[2]\") //设置是否需要发送邮件 .set(\"spark.send.mail.enabled\",\"true\") //设置使用自定义的监听器 .set(\"spark.extraListeners\", \"com.ruozedata.spark.SparkListener.RuozedataSparkListener\") .setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val sc2 = sc.parallelize(List(\"pk,pk,pk\", \"J,J\", \"xingxing\")).repartition(3) val result = sc2.flatMap(_.split(\",\")).map((_, 1)).reduceByKey(_ + _).collect() sc.stop() &#125;&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"窄依赖和宽依赖&Spark任务中的Stage(源码解读)","date":"2019-06-15T12:12:36.000Z","path":"大数据技术/窄依赖和宽依赖&Spark任务中的Stage(源码解读)/","text":"窄依赖和宽依赖&amp;Spark任务中的Stage(源码解读) RDD 的依赖关系 RDD的依赖 RDD之间的依赖关系分为窄依赖（narrow dependency）和宽依赖（wide dependency, 也称 shuffle dependency）. 窄依赖 窄依赖是指父RDD的每个分区只被子RDD的一个分区所使用，子RDD一般对应父RDD的一个或者多个分区，不会产生shuffle。 宽依赖 宽依赖指父RDD的多个分区可能被子RDD的一个分区所使用，子RDD分区通常对应所有的父RDD分区，会产生shuffle。 Lineage（血统） Lineage：RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。 Dependency源码 Dependency是一个抽象类： 1234// org.apache.spark.Dependency.scalaabstract class Dependency[T] extends Serializable &#123; def rdd: RDD[T]&#125; 它有两个子类：NarrowDependency 和 ShuffleDenpendency，分别对应窄依赖和宽依赖。 NarrowDependency抽象类 定义了抽象方法getParents，输入partitionId，用于获得child RDD 的某个partition依赖的parent RDD的所有 partitions。 12345678910// Dependency.scalaabstract class NarrowDependency[T](_rdd: RDD[T]) extends Dependency[T] &#123; /** * Get the parent partitions for a child partition. * @param partitionId a partition of the child RDD * @return the partitions of the parent RDD that the child partition depends upon */ def getParents(partitionId: Int): Seq[Int] override def rdd: RDD[T] = _rdd&#125; 窄依赖分为两种： 一种是一对一的依赖，即OneToOneDependency，指子RDD的partition只依赖于父RDD 的一个partition，产生OneToOneDependency的算子有map，filter，flatMap等。 还有一个是范围的依赖，即RangeDependency，它仅仅被org.apache.spark.rdd.UnionRDD使用。UnionRDD是把多个RDD合成一个RDD，这些RDD是被拼接而成，即每个parent RDD的Partition的相对顺序不会变，只不过每个parent RDD在UnionRDD中的Partition的起始位置不同。 1.OneToOneDependency源码 123class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency[T](rdd) &#123; override def getParents(partitionId: Int): List[Int] = List(partitionId)&#125; 可以看到getParents实现很简单，就是传进去一个partitionId，再把partitionId放在List里面传出去。 2.RangeDependency源码 1234567891011//inStart表示parent RDD的开始索引，outStart表示child RDD 的开始索引class RangeDependency[T](rdd: RDD[T], inStart: Int, outStart: Int, length: Int) extends NarrowDependency[T](rdd) &#123; override def getParents(partitionId: Int): List[Int] = &#123; if (partitionId &gt;= outStart &amp;&amp; partitionId &lt; outStart + length) &#123; List(partitionId - outStart + inStart) //表示于当前索引的相对位置 &#125; else &#123; Nil &#125; &#125;&#125; ShuffleDependency抽象类 表示一个父RDD的partition会被子RDD的partition使用多次，需要经过shuffle。 1234567891011121314151617181920212223242526272829class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ &lt;: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false) extends Dependency[Product2[K, V]] &#123;//shuffle都是基于PairRDD进行的，所以传入的RDD要是key-value类型的 if (mapSideCombine) &#123; require(aggregator.isDefined, \"Map-side combine without Aggregator specified!\") &#125; override def rdd: RDD[Product2[K, V]] = _rdd.asInstanceOf[RDD[Product2[K, V]]] private[spark] val keyClassName: String = reflect.classTag[K].runtimeClass.getName private[spark] val valueClassName: String = reflect.classTag[V].runtimeClass.getName // Note: It's possible that the combiner class tag is null, if the combineByKey // methods in PairRDDFunctions are used instead of combineByKeyWithClassTag. private[spark] val combinerClassName: Option[String] = Option(reflect.classTag[C]).map(_.runtimeClass.getName) val shuffleId: Int = _rdd.context.newShuffleId() val shuffleHandle: ShuffleHandle = _rdd.context.env.shuffleManager.registerShuffle( shuffleId, _rdd.partitions.length, this) _rdd.sparkContext.cleaner.foreach(_.registerShuffleForCleanup(this))&#125;&#125; partitioner：重输出的数据如何分区，partition shuffle结果 serializer: 由于shuffle涉及到网络传输，所以要有序列化serializer keyOrdering: shuffle结果key如何排序 aggregator：map/reduce端的聚合 mapSideCombine: 是否map端聚合 为了减少网络传输，可以map端聚合，通过mapSideCombine和aggregator控制 两种依赖的区分 首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）计算所有父分区。例如，逐个元素地执行map、然后filter操作；而宽依赖则需要首先计算好所有父分区数据，然后在节点之间进行Shuffle，这与MapReduce类似。 第二，窄依赖能够更有效地进行失效节点的恢复，即只需重新计算丢失RDD分区的父分区，而且不同节点之间可以并行计算；而对于一个宽依赖关系的Lineage图，单个节点失效可能导致这个RDD的所有祖先丢失部分分区，因而需要整体重新计算。 Spark任务中的Stage DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。 RDD存在着依赖关系，这些依赖关系形成了有向无环图DAG，DAG通过DAGScheduler进行Stage的划分，并基于每个Stage生成了TaskSet，提交给TaskScheduler。 作业的提交 SparkContext.scala 12345678910111213141516171819//SparkContext.scaladef runJob[T, U: ClassTag]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], resultHandler: (Int, U) =&gt; Unit): Unit = &#123; if (stopped.get()) &#123; throw new IllegalStateException(\"SparkContext has been shutdown\") &#125; val callSite = getCallSite val cleanedFunc = clean(func) logInfo(\"Starting job: \" + callSite.shortForm) if (conf.getBoolean(\"spark.logLineage\", false)) &#123; logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString) &#125; dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get) progressBar.foreach(_.finishAll()) rdd.doCheckpoint() &#125; DAGScheduler.scala 123456789101112131415161718192021222324// DAGScheduler.scaladef runJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =&gt; Unit, properties: Properties): Unit = &#123; val start = System.nanoTime val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) ThreadUtils.awaitReady(waiter.completionFuture, Duration.Inf) waiter.completionFuture.value.get match &#123; case scala.util.Success(_) =&gt; logInfo(\"Job %d finished: %s, took %f s\".format (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9)) case scala.util.Failure(exception) =&gt; logInfo(\"Job %d failed: %s, took %f s\".format (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9)) // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler. val callerStackTrace = Thread.currentThread().getStackTrace.tail exception.setStackTrace(exception.getStackTrace ++ callerStackTrace) throw exception &#125; &#125; 可以看到，SparkContext的runjob方法调用了DAGScheduler的runjob方法正式向集群提交任务，最终调用了submitJob方法。 1234567891011121314151617181920212223242526272829// DAGScheduler.scaladef submitJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =&gt; Unit, properties: Properties): JobWaiter[U] = &#123; // Check to make sure we are not launching a task on a partition that does not exist. val maxPartitions = rdd.partitions.length partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; 0).foreach &#123; p =&gt; throw new IllegalArgumentException( \"Attempting to access a non-existent partition: \" + p + \". \" + \"Total number of partitions: \" + maxPartitions) &#125; val jobId = nextJobId.getAndIncrement() if (partitions.size == 0) &#123; // Return immediately if the job is running 0 tasks return new JobWaiter[U](this, jobId, 0, resultHandler) &#125; assert(partitions.size &gt; 0) val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _] val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler) //给eventProcessLoop发送JobSubmitted消息 eventProcessLoop.post(JobSubmitted( jobId, rdd, func2, partitions.toArray, callSite, waiter, SerializationUtils.clone(properties))) waiter&#125; 这里向eventProcessLoop对象发送了JobSubmitted消息。 123456789101112131415161718192021222324252627282930313233343536// DAGScheduler.scala private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this) eventProcessLoop是DAGSchedulerEventProcessLoop类的一个对象。// DAGScheduler.scala private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123; case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt; dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt; dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties) case StageCancelled(stageId) =&gt; dagScheduler.handleStageCancellation(stageId) case JobCancelled(jobId) =&gt; dagScheduler.handleJobCancellation(jobId) case JobGroupCancelled(groupId) =&gt; dagScheduler.handleJobGroupCancelled(groupId) case AllJobsCancelled =&gt; dagScheduler.doCancelAllJobs() case ExecutorAdded(execId, host) =&gt; dagScheduler.handleExecutorAdded(execId, host) case ExecutorLost(execId, reason) =&gt; val filesLost = reason match &#123; case SlaveLost(_, true) =&gt; true case _ =&gt; false &#125; dagScheduler.handleExecutorLost(execId, filesLost) case BeginEvent(task, taskInfo) =&gt; dagScheduler.handleBeginEvent(task, taskInfo) case GettingResultEvent(taskInfo) =&gt; dagScheduler.handleGetTaskResult(taskInfo) case completion: CompletionEvent =&gt; dagScheduler.handleTaskCompletion(completion) case TaskSetFailed(taskSet, reason, exception) =&gt; dagScheduler.handleTaskSetFailed(taskSet, reason, exception) case ResubmitFailedStages =&gt; dagScheduler.resubmitFailedStages() &#125; DAGSchedulerEventProcessLoop对接收到的消息进行处理，在doOnReceive方法中形成一个event loop。 接下来将调用submitStage()方法进行stage的划分。 stage的划分 12345678910111213141516171819202122// DAGScheduler.scala private def submitStage(stage: Stage) &#123; val jobId = activeJobForStage(stage)//查找该Stage的所有激活的job if (jobId.isDefined) &#123; logDebug(\"submitStage(\" + stage + \")\") if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123; val missing = getMissingParentStages(stage).sortBy(_.id)//得到Stage的父Stage，并排序 logDebug(\"missing: \" + missing) if (missing.isEmpty) &#123; logInfo(\"Submitting \" + stage + \" (\" + stage.rdd + \"), which has no missing parents\") submitMissingTasks(stage, jobId.get)//如果Stage没有父Stage，则提交任务集 &#125; else &#123; for (parent &lt;- missing) &#123;//如果有父Stage，递归调用submiStage submitStage(parent) &#125; waitingStages += stage//将其标记为等待状态，等待下次提交 &#125; &#125; &#125; else &#123; abortStage(stage, \"No active job for stage \" + stage.id, None)//如果该Stage没有激活的job，则丢弃该Stage &#125; &#125; 在submitStage方法中判断Stage的父Stage有没有被提交，直到所有父Stage都被提交，只有等父Stage完成后才能调度子Stage。 123456789101112131415161718192021222324252627282930// DAGScheduler.scalaprivate def getMissingParentStages(stage: Stage): List[Stage] = &#123; val missing = new HashSet[Stage] //用于存放父Stage val visited = new HashSet[RDD[_]] //用于存放已访问过的RDD val waitingForVisit = new Stack[RDD[_]] def visit(rdd: RDD[_]) &#123; if (!visited(rdd)) &#123; //如果RDD没有被访问过，则进行访问 visited += rdd //添加到已访问RDD的HashSet中 val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil) if (rddHasUncachedPartitions) &#123; for (dep &lt;- rdd.dependencies) &#123; //获取该RDD的依赖 dep match &#123; case shufDep: ShuffleDependency[_, _, _] =&gt;//若为宽依赖，则该RDD依赖的RDD所在的stage为父stage val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)//生成父Stage if (!mapStage.isAvailable) &#123;//若父Stage不存在，则添加到父Stage的HashSET中 missing += mapStage &#125; case narrowDep: NarrowDependency[_] =&gt;//若为窄依赖，则继续访问父RDD waitingForVisit.push(narrowDep.rdd) &#125; &#125; &#125; &#125; &#125; waitingForVisit.push(stage.rdd) while (waitingForVisit.nonEmpty) &#123;//循环遍历所有RDD visit(waitingForVisit.pop()) &#125; missing.toList &#125; getmissingParentStages()方法为核心方法。 Stage是通过shuffle划分的，所以每一个Stage都是以shuffle开始的，若一个RDD是宽依赖，则必然说明该RDD的父RDD在另一个Stage中，若一个RDD是窄依赖，则该RDD所依赖的父RDD还在同一个Stage中，我们可以根据这个逻辑，找到该Stage的父Stage。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Shell脚本高级","date":"2019-06-13T12:35:36.000Z","path":"article/Shell脚本高级/","text":"Shell脚本高级 生产shell规范 脚本后缀 .sh 第一行 #!/bin/bash [hadoop@ruozedata002 shell]$ vi standard.sh #!/bin/bash 脚本解释器 开头加作者版本等信息 12345678910111213141516171819202122232425262728293031#!/bin/bash# ./standard.sh db table where#---------------------------------------------#FileName: standard.sh#Version: 1.0#Date: 2019-06-13#Author: ruozedata-J#Description: example of shell script#Notes: project ....#---------------------------------------------USAGE=\"Usage : $0 db table where\"[ $# -ne 3 ] &amp;&amp; echo \"$USAGE\" &amp;&amp; exit 1#startecho \"www.ruozedata.com\"if elifelifelse exit fi#endexit 0 exit 0 正常结束 非0 异常退出 脚本尽量不要有中文字符 变量定义要大写 = 前后不能空格 成对的符号尽量一次性写出来 防止遗漏 {} [] [[]] ‘’ “” [] 里面两端要有空格[ “$RZ” == “RUOZEDATA” ] 公共模块 抽离出来 debug 1234567[hadoop@ruozedata002 shell]$ sh -x standard.sh+ USAGE='Usage : standard.sh db table where'+ '[' 0 -ne 3 ']'+ echo 'Usage : standard.sh db table where'Usage : standard.sh db table where+ exit 1[hadoop@ruozedata002 shell]$ 函数式编程 Shell 函数的本质是一段可以重复使用的脚本代码，这段代码被提前编写好了，放在了指定的位置，使用时直接调取即可。 Shell 函数定义的语法格式如下： 1234function name() &#123; statements [return value]&#125; 对各个部分的说明： function是 Shell 中的关键字，专门用来定义函数； name是函数名； statements是函数要执行的代码，也就是一组语句； return value表示函数的返回值，其中 return 是 Shell 关键字，专门用在函数中返回一个值；这一部分可以写也可以不写。 由{ }包围的部分称为函数体，调用一个函数，实际上就是执行函数体中的代码。 示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[hadoop@ruozedata002 shell]$ vi function.shfunction ruozedata()&#123; echo \"hello ruozedata\"&#125;function ruozedataByParameter()&#123; echo \"$&#123;1&#125;,$&#123;2&#125;\" SUM=$(( $&#123;1&#125; + $&#123;2&#125; )) return $&#123;SUM&#125;&#125;function ruozedataByParameter1()&#123; echo \"$&#123;1&#125;,$&#123;2&#125;\" SUM=$(( $&#123;1&#125; + $&#123;2&#125; )) echo \"$&#123;SUM&#125;\"&#125;#COMMsource /home/hadoop/shell/mysqlconn.shecho $&#123;URL&#125;#call funtionruozedata#use $? 0-255ruozedataByParameter 1000 2000echo $?echo \"---------------\"RESULT=$(ruozedataByParameter 1000 2000)echo $RESULTecho \"---------------\"RESULT1=$(ruozedataByParameter1 1000 2000)echo $RESULT1echo \"---------------\"RESULT2=$(ruozedata)echo $RESULT2","categories":[],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://zzuuriel.github.io/tags/Shell/"}]},{"title":"Shell脚本if判断（数字条件，字符串条件，文件或文件夹是否存在","date":"2019-06-12T14:12:36.000Z","path":"article/Shell脚本if判断（数字条件，字符串条件，文件或文件夹是否存在/","text":"Shell脚本if判断（数字条件，字符串条件，文件或文件夹是否存在） 整数比较 12345678910-eq 等于,如:if [ \"$a\" -eq \"$b\" ] -ne 不等于,如:if [ \"$a\" -ne \"$b\" ] -gt 大于,如:if [ \"$a\" -gt \"$b\" ] -ge 大于等于,如:if [ \"$a\" -ge \"$b\" ] -lt 小于,如:if [ \"$a\" -lt \"$b\" ] -le 小于等于,如:if [ \"$a\" -le \"$b\" ] &lt; 小于(需要双括号),如:((\"$a\" &lt; \"$b\")) &lt;= 小于等于(需要双括号),如:((\"$a\" &lt;= \"$b\")) &gt; 大于(需要双括号),如:((\"$a\" &gt; \"$b\")) &gt;= 大于等于(需要双括号),如:((\"$a\" &gt;= \"$b\")) 示例： 12345678910111213141516171819202122232425262728293031323334353637#!/bin/bash i=6a=10 if [ $a -eq 10 ]then echo \"a = 10\"fi if [ $a -ne $i ]then echo \"a != $i\"fi if [ $a -gt $i ]then echo \"a &gt; i\"fi if [ $a -lt $i ]then echo \"a &lt; i\"else echo \"a &gt; i\"fi if((\"$a\" &gt; \"$i\"))then echo \"(())a&gt;i\"fi if(($a != $i))then echo \"(())a!=i\"fi 字符串比较 12= 等于,如:if [ \"$a\" = \"$b\" ] == 等于,如:if [ \"$a\" == \"$b\" ],与=等价 注意:==的功能在[[]]和[]中的行为是不同的,如下: [[ $a == z* ]] # 如果$a以&quot;z&quot;开头(模式匹配)那么将为true [[ $a == “z*” ]] # 如果$a等于z*(字符匹配),那么结果为true [ $a == z* ] # File globbing 和word splitting将会发生 [ “$a” == “z*” ] # 如果$a等于z*(字符匹配),那么结果为true 示例： 12345678910111213141516#!/bin/bash a=\"123\"b=\"1234\"c=\"123\" if [ \"$a\"x != \"$b\"x ]then echo \"a != b\"fi if [ \"$a\"x = \"$c\"x ]then echo \"a == c\"fi 判断字符串为空： 1234if [ -z \"$d\" ]then echo \"d is empty\"fi 文件或文件夹是否存在 1234567891011121314151617181920212223242526#!/bin/bash#判断文件存在，判断是否为文件夹等testPath=\"/home/hadoop/shell\"testFile=\"/home/hadoop/shell/test.txt\"#判断文件夹是否存在 -dif [[ ! -d \"$testPath\" ]]; then echo \"文件夹不存在\"else echo \"文件夹存在\"fi#判断文件夹是否存在，并且具有可执行权限if [[ ! -x \"$testFile\" ]]; then echo \"文件不存在并且没有可执行权限\"else echo \"文件存在并有可执行权限\"fi#判断文件是否存在if [[ ! -f \"$testFile\" ]]; then echo \"文件不存在\"else echo \"文件存在\"fi 文件比较符： 12345678910111213-e 判断对象是否存在-d 判断对象是否存在，并且为目录-f 判断对象是否存在，并且为常规文件-L 判断对象是否存在，并且为符号链接-h 判断对象是否存在，并且为软链接-s 判断对象是否存在，并且长度不为0-r 判断对象是否存在，并且可读-w 判断对象是否存在，并且可写-x 判断对象是否存在，并且可执行-O 判断对象是否存在，并且属于当前用户-G 判断对象是否存在，并且属于当前用户组-nt 判断file1是否比file2新 [ \"/data/file1\" -nt \"/data/file2\" ]-ot 判断file1是否比file2旧 [ \"/data/file1\" -ot \"/data/file2\" ] 备注 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253-e 文件存在-a 文件存在（已被弃用）-f 被测文件是一个regular文件（正常文件，非目录或设备）-s 文件长度不为0-d 被测对象是目录-b 被测对象是块设备-c 被测对象是字符设备-p 被测对象是管道-h 被测文件是符号连接-L 被测文件是符号连接-S(大写) 被测文件是一个socket-t 关联到一个终端设备的文件描述符。用来检测脚本的stdin[-t0]或[-t1]是一个终端-r 文件具有读权限，针对运行脚本的用户-w 文件具有写权限，针对运行脚本的用户-x 文件具有执行权限，针对运行脚本的用户-u set-user-id(suid)标志到文件，即普通用户可以使用的root权限文件，通过chmod +s file实现-k 设置粘贴位-O 运行脚本的用户是文件的所有者-G 文件的group-id和运行脚本的用户相同-N 从文件最后被阅读到现在，是否被修改f1 -nt f2 文件f1是否比f2新f1 -ot f2 文件f1是否比f2旧f1 -ef f2 文件f1和f2是否硬连接到同一个文件二元比较操作符，比较变量或比较数字整数比较：-eq 等于 if [ \"$a\" -eq \"$b\" ]-ne 不等于 if [ \"$a\" -ne \"$b\" ]-gt 大于 if [ \"$a\" -gt \"$b\" ]-ge 大于等于 if [ \"$a\" -ge \"$b\" ]-lt 小于 if [ \"$a\" -lt \"$b\" ]-le 小于等于 if [ \"$a\" -le \"$b\" ]&lt; 小于（需要双括号） (( \"$a\" &lt; \"$b\" ))&lt;= 小于等于(...) (( \"$a\" &lt;= \"$b\" ))&gt; 大于(...) (( \"$a\" &gt; \"$b\" ))&gt;= 大于等于(...) (( \"$a\" &gt;= \"$b\" ))字符串比较：= 等于 if [ \"$a\" = \"$b\" ]== 与=等价!= 不等于 if [ \"$a\" = \"$b\" ]&lt; 小于，在ASCII字母中的顺序：if [[ \"$a\" &lt; \"$b\" ]]if [ \"$a\" \\&lt; \"$b\" ] #需要对&lt;进行转义&gt; 大于-z 字符串为null，即长度为0-n 字符串不为null，即长度不为0","categories":[],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://zzuuriel.github.io/tags/Shell/"}]},{"title":"Shell脚本基础","date":"2019-06-12T12:12:36.000Z","path":"article/Shell脚本基础/","text":"Shell脚本基础 Shell入门 定义 123[root@hadoop000 shell]# vi wordcount.sh#!/bin/bashecho \"www.ruozedata.com\" 赋x权限 123456789101112[root@hadoop000 shell]# lltotal 4-rw-r--r--. 1 root root 40 Jun 12 21:11 wordcount.shchmod +x wordcount.sh[root@hadoop000 shell]# chmod 654 wordcount.sh[root@hadoop000 shell]# lltotal 4-rw-r-xr--. 1 root root 40 Jun 12 21:11 wordcount.sh[root@hadoop000 shell]# ./wordcount.sh www.ruozedata.com 调试与sh命令 12345678[root@hadoop000 shell]# vi wordcount.sh #!/bin/bash -xecho \"www.ruozedata.com\"[root@hadoop000 shell]# ./wordcount.sh + echo www.ruozedata.comwww.ruozedata.com 123[root@hadoop000 shell]# sh -x wordcount.sh+ echo www.ruozedata.comwww.ruozedata.com 总结: 1.脚本开头定义 #!/bin/bash 2.调试的方法有两种 直接执行sh -x xxx.sh 开头定义#!/bin/bash -x 执行./xxx.sh #!/bin/bash -x 这种方法会影响结果查看 变量定义及引用 12345678910111213141516171819202122232425262728293031[root@hadoop000 shell]# vi variable.sh#!/bin/bashrz=\"www.ruozedata.com\"date=`date`echo $rzecho $date[root@hadoop000 shell]# sh variable.shwww.ruozedata.comTue Jun 12 21:28:01 CST 2018[root@hadoop000 shell]# sh variable.shwww.ruozedata.comTue Jun 12 21:28:03 CST 2018[root@hadoop000 shell]# 静态: k=\"v\"动态: k=`v`=前后不能有空格引用: $k ==&gt; $rz $&#123;k&#125; ==&gt; $&#123;rz&#125;jepsonecho $rzjepsonecho $&#123;rz&#125;jepson 总结: 1.=前后不能有空格 2.字符串建议双引号 3.引用变量加上{} 传递参数 1234567891011121314151617181920212223242526272829[root@hadoop000 shell]# vi parameter.sh#!/bin/bashecho $1echo $2echo \"个数:$#\"echo \"传递参数作为1个字符串显示: $*\"echo \"PID: $$\"~~~~~\"parameter.sh\" 10L, 113C written[root@hadoop000 shell]# sh parameter.sh a b ab个数:2传递参数作为1个字符串显示: a bPID: 2647[root@hadoop000 shell]# [root@hadoop000 shell]# [root@hadoop000 shell]# sh parameter.sh \"a b\" a b个数:1传递参数作为1个字符串显示: a bPID: 2649[root@hadoop000 shell]# 数组 123456789101112131415[root@hadoop000 shell]# vi array.sh#!/bin/basharr=(rz jepson xingxing huhu qianxi)echo $&#123;arr[*]&#125;echo $&#123;arr[4]&#125;echo $&#123;#arr[*]&#125;[root@hadoop000 shell]# sh array.sh rz jepson xingxing huhu qianxiqianxi5[root@hadoop000 shell]# 这里的* 也可以用@表示 if判断 方法1： if [ $a == $b ];then 1234567891011121314151617181920212223[root@hadoop000 shell]# vi if.sh#!/bin/bash a=\"abc\"b=\"jepson\"if [ $a == $b ];then echo \"==\"else echo \"!=\"fi[root@hadoop000 shell]# sh if.sh !=[root@hadoop000 shell]# [root@hadoop000 shell]# sh -x if.sh + a=abc+ b=jepson+ '[' abc == jepson ']'+ echo '!='!=[root@hadoop000 shell] 方法2： if [ “$a” == “$b” ] then 1234567891011121314151617181920212223242526272829[root@hadoop000 shell]# vi if.shthen#!/bin/basha=\"ccc\"b=\"jepson\"if [ \"$a\" == \"$b\" ]then echo \"==\"elif [ \"$a\" == \"ccc\" ]then echo \"ccc\"else echo \"!=\"fi[root@hadoop000 shell]# [root@hadoop000 shell]# [root@hadoop000 shell]# sh -x if.sh + a=ccc+ b=jepson+ '[' ccc == jepson ']'+ '[' ccc == ccc ']'+ echo cccccc[root@hadoop000 shell]# 循环 123456789101112131415161718192021222324252627282930[root@hadoop000 shell]# vi forwhile.sh#!/bin/bash#方法1：for x in 1 2 3 4 5do echo $x let \"j++\"doneecho \" \"#方法2：for ((i=1;i&lt;10;i++))do echo $idoneecho \" \"#方法3：j=1while(($j&lt;10))do echo $j let \"j++\"done 推荐方法2 分割语法 12345678910111213141516171819202122232425[root@hadoop000 shell]# vi spilt.sh #!/bin/bash#方法1s=\"rz,jepson,xingxing,huhu,qianxi\"OLD_IFS=\"$IFS\"IFS=\",\"arr=($s)IFS=\"$OLD_IFS\"for x in $&#123;arr[*]&#125;do echo $xdoneecho \"-----------------------\"#方法2arr2=($&#123;s//,/ &#125;)for x in $&#123;arr2[*]&#125;do echo $xdone","categories":[],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://zzuuriel.github.io/tags/Shell/"}]},{"title":"Spark Cluster Mode Overview 官网翻译","date":"2019-06-11T12:12:36.000Z","path":"大数据技术/Spark Cluster Mode Overview 官网翻译/","text":"Spark Cluster Mode Overview 官网翻译 文档地址：http://spark.apache.org/docs/latest/cluster-overview.html Cluster Mode Overview(集群模式概述) This document gives a short overview of how Spark runs on clusters, to make it easier to understand the components involved. Read through the application submission guide to learn about launching applications on a cluster. 这篇文档介绍了Spark在集群上运行的大概情况，让我们更容易理解其各个组件是如何交互的。我们可以通读 application submission guide 来学习如何在集群上部署应用程序。 Components(组件) Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program). Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run. Spark应用程序在一个集群上运行着一组独立的进程，包括一个driver和多个executors，在你应用程序的main函数里通过SparkContext对象来协调组织，我们也把Spark applications称之为driver program。 具体来说，集群模式下，SparkContext能够连接不同类型的cluster managers，比如说Spark自己的standalone cluster manager， Mesos或者YARN，而这些cluster managers所扮演的角色是在各个应用程序application之间分配资源。一旦Spark连接上这些cluster managers，Spark就获得了分布在集群各个节点上的executors，这些executors其实是一系列的进程，这些进程执行我们的应用程序application中的计算并存储相关的数据。接着，SparkContext将我们的应用程序application代码发送给executors，这些应用程序application代码是由JAR或者Python文件所定义并且传给SparkContext。最后，SparkContext把tasks发送给executors去执行。 There are several useful things to note about this architecture: Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system. Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN). The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes. Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes. 关于这个架构，有以下几个有用的地方需要注意： 每个应用程序application都有属于它自己本身的executor进程，这些进程横跨这个application的整个生命周期并且以多线程的方式来执行内部的多个tasks。这有个好处，每个application之间无论是在调度层面scheduling side还是在执行层面 executor side都是相互隔离的。也就是说从调度层面来看，每个driver调度属于它自身的tasks，从执行层面上来看，属于不同applications的tasks运行在不同的JVM上。然而，这也意味着不同的Spark applications（也可以说是SparkContext的实例）是不能共享各自所属的数据，除非，你把数据写到外部存储系统，比如说Alluxio。 Spark不关心底层的cluster manager是哪种类型。只要Spark可以获取得到executor进程，并且这些executor进程能够互相通信，那么对于同样支持其他applications的cluster manager来说，比如Mesos/YARN，都是能去运行Spark程序的。 在driver program的整个生命周期中，它一直在监听并且接收来自属于它本身的executors的连接。可以查看 spark.driver.port in the network config section。因此，driver program必须跟各个worker nodes节点网络互通。 由于driver是在集群上调度各个任务的，按理来说，它应该运行在靠近worker nodes的节点上，最好是在同一个局域网里。如果你想发送请求给远端的集群，最佳的方式是你给driver开一个RPC，并且让driver在靠近worker nodes的节点上提交作业，而不是在远离worker nodes的节点上运行driver。 Cluster Manager Types(Cluster Manager类型) The system currently supports several cluster managers: Standalone – a simple cluster manager included with Spark that makes it easy to set up a cluster. Apache Mesos – a general cluster manager that can also run Hadoop MapReduce and service applications. Hadoop YARN – the resource manager in Hadoop 2. Kubernetes – an open-source system for automating deployment, scaling, and management of containerized applications. A third-party project (not supported by the Spark project) exists to add support for Nomad as a cluster manager. 如今Spark生态目前支持以下几个cluster managers: Standalone – 一个简单的cluster manager，它内置了Spark，使得我们能够快速的启动一个集群。 Apache Mesos – 一个通用的cluster manager，它能够运行Hadoop MapReduce以及service applications。 Hadoop YARN – Hadoop 2的resource manager。 Kubernetes – 一个开源的项目，用于自动部署，扩容以及容器内部应用的管理。 Submitting Applications（提交Applications） Applications can be submitted to a cluster of any type using the spark-submit script. The application submission guide describes how to do this. 通过spark-submit脚本，我们可以把应用Applications提交到任何类型的集群上。这篇 application submission guide 文章描述了具体的实现方式。 Monitoring（监控） Each driver program has a web UI, typically on port 4040, that displays information about running tasks, executors, and storage usage. Simply go to http://:4040 in a web browser to access this UI. The monitoring guide also describes other monitoring options. 每个driver program都有它自己的一套web UI界面，通常运行在4040端口，它详细展示了当前运行的tasks，executors，和存储使用情况等相关信息。我们可以通过浏览器访问http://:4040来浏览这个UI界面。这篇 monitoring guide 文章详细介绍了其他监控选项。 Job Scheduling(作业调度) Spark gives control over resource allocation both across applications (at the level of the cluster manager) and within applications (if multiple computations are happening on the same SparkContext). The job scheduling overview describes this in more detail. Spark不仅仅通过cluster manager在各个应用applications之间来控制资源的分配，而且在应用applications内部，当同一个SparkContext里面同时有多个计算同时运行的时候，Spark同样会去控制资源如何分配。详情请看 job scheduling overview。 Glossary(术语) The following table summarizes terms you’ll see used to refer to cluster concepts: 下面的表格总结了一些你可能会碰到的关于集群的概念和术语","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":" Spark RDD定义与特性 ","date":"2019-06-11T01:16:09.000Z","path":"大数据技术/4.spark RDD定义与特性/","text":"Spark RDD定义与特性 官网 http://spark.apache.org 什么是RDD RDD（Resilient Distributed Dataset）叫做分布式数据集，是spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性 RDD的特性 RDD在源码的介绍 12345- A list of partitions- A function for computing each split- A list of dependencies on other RDDs- Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)- Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file) 一组分片（partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度，用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值，默认值就是程序所分配到的CPU core的数目。 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 RDD在源码的体现 打开源码RDD.scala 对应第二个特性 def compute(split: Partition, context: TaskContext): Iterator[T] 计算：其实是对RDD立面的每个分区做计算 传入的参数：split类型是Partition，context类型是TaskContext 对应第一个特性 protected def getPartitions: Array[Partition] 得到分区，返回的类型Array[Partition]，是一个数组或集合，数组或集合的类型是Partition 对应第三个特性 protected def getDependencies: Seq[Dependency[_]] = deps 得到一个Dependencies 对应第五个特性 protected def getPreferredLocations(split: Partition): Seq[String] = Nil 对应第四个特性 @transient val partitioner: Option[Partitioner] = None","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark中job、stage、task的划分","date":"2019-06-10T12:12:36.000Z","path":"大数据技术/Spark中job、stage、task的划分/","text":"Spark中job、stage、task的划分 参考博客： Spark_Spark 中 Stage, Job 划分依据 , Job, Stage, Task 相关概念 Spark的Job、Stage、Task是按何种规则产生的 （图中最小的方块代表一个partition，包裹partition的方块是RDD，忽略颜色） Spark的Stage是分割RDD执行的各种transformation而来。如上图，将这些转化步骤分为了3个Stage，分别为Stage1，Stage2和Stage3。这里最重要的是搞清楚分割Stage的规则，其实只有一个：从宽依赖处分割。 知道了这个分割规则，其实还是有一点疑惑，为什么这么分？ 其实道理蛮明显的，子RDD的partition会依赖父RDD中多个partition，这样就可能会有一些partition没有准备好，导致计算不能继续，所以就分开了，直到准备好了父RDD中所有partition，再继续进行将父RDD转换为子RDD的计算。而窄依赖完全不会有这个顾虑，窄依赖是父RDD一个partition对应子RDD一个partition，那么直接计算就可以了。 Job Spark的Job来源于用户执行action操作，就是从RDD中获取结果的操作，而不是将一个RDD转换成另一个RDD的transformation操作。 Stage 一个Job会被拆分为多组Task，每组任务被称为一个Stage就像Map Stage， Reduce Stage。 在Spark中有两类task，一类是shuffleMapTask，一类是resultTask，第一类task的输出是shuffle所需数据，第二类task的输出是result，stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。比如： rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个； rdd.map(x =&gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。 Task task是stage下的一个任务执行单元，一般来说，一个rdd有多少个partition，就会有多少个task，因为每一个task只是处理一个partition上的数据。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Core之读写数据","date":"2019-06-06T12:12:36.000Z","path":"大数据技术/Spark Core之读写数据/","text":"Spark Core之读写数据 Spark支持多种数据源，从总体来分分为两大部分：文件系统和数据库。 文件系统 文件系统主要有本地文件系统、Amazon S3、HDFS等。 文件系统中存储的文件有多种存储格式。Spark支持的一些常见格式有： 格式名称 结构化 说明 文本文件 否 普通文本文件，每行一条记录 JSON 半结构化 常见的基于文本的半结构化数据 CSV 是 常见的基于文本的格式，在电子表格应用中使用 SequenceFiles 是 一种用于键值对数据的常见Hadoop文件格式 以文本文件为例： 1.读取 读取单个文件，参数为文件全路径，输入的每一行都会成为RDD的一个元素。 1val lines = sc.textFile(\"spark-core/data/cdnlog.txt\") 读取多个文件时，可以使用textFile将参数改为目录或以逗号文件的多个文件名。如果是小文件，也可以使用wholeTextFiles读取为一个Pair RDD（键是文件名，值是文件内容）。 1234567val input = sc.wholeTextFiles(\"spark-core/data/test.txt\")val result = input.mapValues&#123; y =&gt; &#123; val nums = y.split(\" \").map(x =&gt; x.toDouble) nums.sum / nums.size.toDouble &#125;&#125; 2.写入 输出文本文件时，可使用saveAsTextFile()方法接收一个目录，将RDD中的内容输出到目录中的多个文件中。 1result.saveAsTextFile(outputFile) 数据库 数据库主要分为关系型数据库（MySQL、PostgreSQL等）和非关系型数据库（HBase、ElasticSearch等）。Spark使用JDBC访问关系型数据库（MySQL、PostgreSQL等），只需要构建一个org.apache.spark.rdd.JdbcRDD即可。 以MySQL为例： 1.读取： 12345678910111213def readMySQL(sc: SparkContext) = &#123; val stat: JdbcRDD[(String, Int)] = new JdbcRDD(sc, () =&gt; &#123; DriverManager.getConnection(\"jdbc:mysql://hadoop000:3306/ruozedata_spark_online\", \"root\", \"123456\") &#125;, sql = \"select * from stat where cnt &gt;= ? and cnt &lt;= ?\" , lowerBound = 40, upperBound = 110, numPartitions = 1 resultSet =&gt; (resultSet.getString(1), resultSet.getInt(2)) ) println(\"count: \" + stat.count()) stat.foreach(println)&#125; 2.写入: 123456789101112131415161718192021def saveMySQL(sc: SparkContext) = &#123; val rdd = sc.parallelize(Array((\"BJ\",100),(\"SH\",10),(\"SZ\",30)),1) rdd.foreachPartition(partition =&gt; &#123; val connection = DriverManager.getConnection(\"jdbc:mysql://hadoop000:3306/ruozedata_spark_online\",\"root\",\"123456\") println(\"---------------------\") connection.setAutoCommit(false) //表名stat,列名province, cnt val pstmt = connection.prepareStatement(\"ins(province, cnt) ert into stat values (?,?)\") partition.foreach(x =&gt; &#123; pstmt.setString(1, x._1) pstmt.setInt(2, x._2) //批处理 pstmt.addBatch() &#125;) pstmt.executeBatch() connection.commit() connection.close() &#125;)&#125;","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Spark Core基础-常用算子","date":"2019-06-04T12:12:36.000Z","path":"大数据技术/Spark Core基础-常用算子/","text":"Spark Core基础-常用算子 常用算子 map、mapPartition、mapPartitionWithIndex 1.map map对rdd中的每一个元素进行操作。 2.mapPartition mapPartitions是对rdd中的每个分区的迭代器进行操作。 mapPartitions的优点： 如果是普通的map，比如一个partition中有1万条数据。那么你的function要执行和计算1万次。使用mapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。SparkSql或DataFrame默认会对程序进行mapPartition的优化。 mapPartitions的缺点： 如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下， 比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。 所以说普通的map操作通常不会导致内存的OOM异常。 但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。 3.mapPartitionWithIndex mapPartitions则是对rdd中的每个分区的迭代器进行操作，并且可以将分区的编号取出来。 示例： 12345678910111213141516171819202122232425262728object mapPartitionsWithIndexApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val rdd1: RDD[String] = sc.parallelize(List( \"spark1\", \"spark2\", \"spark3\", \"spark4\", \"spark5\", \"spark6\", \"spark7\", \"spark8\", \"spark9\"), 3) val rdd2: RDD[String] = rdd1.mapPartitionsWithIndex &#123; (index, iter) =&gt; &#123; println() var result = List[String]() var subfix = \"\" if (index == 0) subfix = \"【北京区】\" else if (index == 1) subfix = \"【上海区】\" else subfix = \"【广州区】\" while (iter.hasNext) &#123; val str: String = iter.next() result = result :+ subfix + str &#125; result.iterator &#125; &#125; rdd2.foreach(println(_)) sc.stop() &#125;&#125; 运行结果： 12345678910111213【北京区】spark1【北京区】spark2【北京区】spark3【上海区】spark4【上海区】spark5【上海区】spark6【广州区】spark7【广州区】spark8【广州区】spark9Process finished with exit code 0 zip、zipWithIndex 1.zip zip把两个集合按元素顺序合并成元组： 12345678scala&gt; val rdd1 = sc.parallelize(List(\"pk\", \"J\", \"xingxing\"))rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; val rdd2 = sc.parallelize(List(\"1\", \"2\", \"3\"))rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24scala&gt; rdd1.zip(rdd2).collectres4: Array[(String, String)] = Array((pk,1), (J,2), (xingxing,3)) 注意： 和scala不同,spark中必须保证: 元素数目相同 分区数相同 2.zipWithIndex 该函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对。 示例： 12345scala&gt; val rdd3 = sc.parallelize(List(\"hadoop\", \"spark\", \"flink\"))rdd3: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24scala&gt; rdd3.zipWithIndex().collectres6: Array[(String, Long)] = Array((hadoop,0), (spark,1), (flink,2)) union、distinct、intersection、substract、cartesian 创建两个rdd： 12345scala&gt; val rdd4 = sc.parallelize(List(\"a\", \"b\", \"b\",\"c\",\"c\"))rdd4: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[8] at parallelize at &lt;console&gt;:24scala&gt; val rdd5 = sc.parallelize(List(\"c\", \"d\", \"e\"))rdd5: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[9] at parallelize at &lt;console&gt;:24 1.union 返回一个新的数据集，该数据集包含源数据集和参数中的元素的联合。（不去重） 12scala&gt; rdd4.union(rdd5).collectres7: Array[String] = Array(a, b, b, c, c, c, d, e) 2.distinct 返回包含源数据集去重后元素的新数据集。 源码： 123def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope &#123; map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)&#125; 示例： 12scala&gt; rdd4.distinct().collectres8: Array[String] = Array(b, a, c) 3.intersection 交集 intersection(otherDataset) 返回一个新的RDD，它包含源数据集中元素和参数的交集。（去重） 12scala&gt; rdd4.intersection(rdd5).collectres9: Array[String] = Array(c) 4.substract 差集 rdd1.subtract (rdd2) 返回在rdd1中出现，但是不在rdd2中出现的元素。（不去重） 12scala&gt; rdd4.subtract(rdd5).collectres10: Array[String] = Array(b, b, a) 5.cartesian 笛卡尔 cartesian(otherDataset) 当调用类型T和U的数据集时，返回(T,U)对的数据集(所有对元素)。 12scala&gt; rdd4.cartesian(rdd5).collectres11: Array[(String, String)] = Array((a,c), (b,c), (a,d), (a,e), (b,d), (b,e), (b,c), (c,c), (c,c), (b,d), (b,e), (c,d), (c,e), (c,d), (c,e)) sortBy、sortByKey 在Spark中存在两种对RDD进行排序的函数，分别是 sortBy和sortByKey函数。sortBy是对标准的RDD进行排序，它是从Spark 0.9.0之后才引入的（可以参见SPARK-1063）。而sortByKey函数是对PairRDD进行排序，也就是有Key和Value的RDD。 1.sortBy sortBy函数函数的实现依赖于sortByKey函数，示例： 123456789101112//升序scala&gt; rdd6.sortBy(x =&gt; x).collect()res13: Array[Int] = Array(1, 3, 3, 6, 23, 46)//降序scala&gt; rdd6.sortBy(x =&gt; x, false).collect()res14: Array[Int] = Array(46, 23, 6, 3, 3, 1)//修改分区数，默认的分区个数是2，而我们对它进行了修改，所以最后变成了1。scala&gt; val result = rdd6.sortBy(x =&gt; x, false, 1)result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[41] at sortBy at &lt;console&gt;:25scala&gt; result.partitions.sizeres16: Int = 1 2.sortByKey sortByKey函数作用于Key-Value形式的RDD，并对Key进行排序。它是在org.apache.spark.rdd.OrderedRDDFunctions中实现的，实现如下： 12345678def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size) : RDD[(K, V)] =&#123; val part = new RangePartitioner(numPartitions, self, ascending) new ShuffledRDD[K, V, V](self, part) .setKeyOrdering(if (ascending) ordering else ordering.reverse)&#125; 该函数返回的RDD一定是ShuffledRDD类型的，因为对源RDD进行排序，必须进行Shuffle操作，而Shuffle操作的结果RDD就是ShuffledRDD。其实这个函数的实现很优雅，里面用到了RangePartitioner，它可以使得相应的范围Key数据分到同一个partition中，然后内部用到了mapPartitions对每个partition中的数据进行排序，而每个partition中数据的排序用到了标准的sort机制，避免了大量数据的shuffle。 groupByKey、reduceByKey 示例： 1234567891011121314151617181920object WCApp &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster(\"local\").setAppName(this.getClass.getSimpleName) val sc = new SparkContext(sparkConf) val words = Array(\"one\", \"two\", \"two\", \"three\", \"three\", \"three\") val wordsRDD = sc.parallelize(words).map(word =&gt; (word, 1)) val wordsCountWithReduce = wordsRDD .reduceByKey(_ + _) .foreach(println) val wordsCountWithGroup = wordsRDD .groupByKey() .map(x =&gt; (x._1, x._2.sum)) .foreach(println) sc.stop() &#125;&#125; 运行结果： 123456(two,2)(one,1)(three,3)(two,2)(one,1)(three,3) 虽然两个函数都能得出正确的结果， 但reduceByKey函数更适合使用在大数据集上。 这是因为它可以在每个分区移动数据之前将输出数据与一个共用的key结合（先本地聚合,再shuffle）。 groupByKey这个出来的是(key,Interator),需要对iteration的内容进行进一步的处理，当调用 groupByKey时，所有的键值对(key-value pair) 都会被移动,在网络上传输这些数据非常没必要，因此应避免使用 groupByKey。 数据量很大时，在使用 reduceByKey 和 groupByKey 时他们shuffle后的文件大小差别会被放大更多倍。 join、leftOuterJoin、rightOuterJoin、fullOuterJoin 1.join join函数输出两个RDD中相同Key的所有项(类似于SQL中的inner join)。 12345678910111213scala&gt; val a = sc.parallelize(Array((\"A\",\"a1\"),(\"B\",\"b1\"),(\"C\",\"c1\"),(\"D\",\"d1\"),(\"E\",\"e1\"),(\"F\",\"f1\")))a: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24 scala&gt; val b = sc.parallelize(Array((\"A\",\"a2\"),(\"B\",\"b2\"),(\"C\",\"c1\"),(\"C\",\"c2\"),(\"C\",\"c3\"),(\"E\",\"e2\")))b: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:24 scala&gt; a.join(b).collect // 这里的join是inner join，只返回左右都匹配上的内容 res1: Array[(String, (String, String))] = Array((B,(b1,b2)), (A,(a1,a2)), (C,(c1,c1)), (C,(c1,c2)), (C,(c1,c3)), (E,(e1,e2))) scala&gt; b.join(a).collect res2: Array[(String, (String, String))] = Array((B,(b2,b1)), (A,(a2,a1)), (C,(c1,c1)), (C,(c2,c1)), (C,(c3,c1)), (E,(e2,e1))) 2.leftOuterJoin 是以左边为基准，会保留对象的所有key 左边（a）的记录一定会存在，右边（b）的记录有的返回Some(x)，没有的补None。 12345scala&gt; a.leftOuterJoin(b).collectres3: Array[(String, (String, Option[String]))] = Array((B,(b1,Some(b2))), (F,(f1,None)), (D,(d1,None)), (A,(a1,Some(a2))), (C,(c1,Some(c1))), (C,(c1,Some(c2))), (C,(c1,Some(c3))), (E,(e1,Some(e2)))) scala&gt; b.leftOuterJoin(a).collectres5: Array[(String, (String, Option[String]))] = Array((B,(b2,Some(b1))), (A,(a2,Some(a1))), (C,(c1,Some(c1))), (C,(c2,Some(c1))), (C,(c3,Some(c1))), (E,(e2,Some(e1)))) 3.rightOuterJoin 是以右边为基准，保留参数对象的所有key 右边（b）的记录一定会存在，左边（a）的记录有的返回Some(x)，没有的补None。 12345scala&gt; a.rightOuterJoin(b).collectres4: Array[(String, (Option[String], String))] = Array((B,(Some(b1),b2)), (A,(Some(a1),a2)), (C,(Some(c1),c1)), (C,(Some(c1),c2)), (C,(Some(c1),c3)), (E,(Some(e1),e2))) scala&gt; b.rightOuterJoin(a).collectres6: Array[(String, (Option[String], String))] = Array((B,(Some(b2),b1)), (F,(None,f1)), (D,(None,d1)), (A,(Some(a2),a1)), (C,(Some(c1),c1)), (C,(Some(c2),c1)), (C,(Some(c3),c1)), (E,(Some(e2),e1))) 4.fullOuterJoin 保留两个RDD中的所有key 所有的值列都可能出现缺失的情况，所有所有值列都转换为Option对象 123456789101112scala&gt; val a = sc.parallelize(Array((\"A\",\"a1\"),(\"B\",\"b1\"),(\"C\",\"c1\"),(\"D\",\"d1\"),(\"E\",\"e1\"),(\"F\",\"f1\")))a: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[49] at parallelize at &lt;console&gt;:24 scala&gt; val b = sc.parallelize(Array((\"A\",\"a2\"),(\"B\",\"b2\"),(\"C\",\"c1\"),(\"C\",\"c2\"),(\"C\",\"c3\"),(\"E\",\"e2\")))b: org.apache.spark.rdd.RDD[(String, String)] = ParallelCollectionRDD[50] at parallelize at &lt;console&gt;:24 scala&gt; a.fullOuterJoin(b).collectres15: Array[(String, (Option[String], Option[String]))] = Array((B,(Some(b1),Some(b2))), (F,(Some(f1),None)), (D,(Some(d1),None)), (A,(Some(a1),Some(a2))), (C,(Some(c1),Some(c1))), (C,(Some(c1),Some(c2))), (C,(Some(c1),Some(c3))), (E,(Some(e1),Some(e2)))) scala&gt; b.fullOuterJoin(a).collectres16: Array[(String, (Option[String], Option[String]))] = Array((B,(Some(b2),Some(b1))), (F,(None,Some(f1))), (D,(None,Some(d1))), (A,(Some(a2),Some(a1))), (C,(Some(c1),Some(c1))), (C,(Some(c2),Some(c1))), (C,(Some(c3),Some(c1))), (E,(Some(e2),Some(e1)))) coalesce、repartition repartition(numPartitions:Int):RDD[T]和coalesce(numPartitions:Int，shuffle:Boolean=false):RDD[T]， 他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区）： 如果N&lt;M，一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。 如果N&gt;M并且N和M相差不多，(假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false，在shuffl为false的情况下，如果M&gt;N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。 如果N&gt;M并且两者相差悬殊，这时如果将shuffle设置为false，父子RDD是窄依赖关系，他们同处在一个Stage中，就可能造成spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。 总之：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的分区数变多的。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"https://zzuuriel.github.io/tags/Spark/"}]},{"title":"Scala模式匹配","date":"2019-05-29T12:12:36.000Z","path":"Scala编程语言/Scala模式匹配/","text":"Scala模式匹配 Scala模式匹配 scala提供了一个非常强大的模式匹配机制,那什么是模式匹配呢?模式匹配是检查某个值（value）是否匹配某一个模式的机制，一个成功的匹配同时会将匹配值解构为其组成部分。它是Java中的switch语句的升级版，同样可以用于替代一系列的 if/else 语句。 字符串 123456789101112object CaseDemo01 &#123; def main(args: Array[String]): Unit = &#123; val arr = Array(\"hadoop\", \"zookeeper\", \"spark\", \"pk\") val name = arr(Random.nextInt(arr.length)) name match &#123; case \"hadoop\" =&gt; println(\" 大数据分布式存储和计算框架 ...\") case \"zookeeper\" =&gt; println(\" 大数据分布式协调服务框架 ...\") case \"spark\" =&gt; println(\" 大数据分布式内存计算框架 ...\") case _ =&gt; println(\" 我不认识你 ...\") &#125; &#125;&#125; 类型 123456789101112131415161718192021object CaseDemo02 extends MatchType &#123; def main(args: Array[String]): Unit = &#123; val arr = Array(\"xingxing\", 123, 4.23, true, CaseDemo01) val name = arr(Random.nextInt(arr.length)) println(\"name: \" + name) name match &#123; case str: String =&gt; println(s\"match String $str\") case int: Int =&gt; println(s\"match Int $int\") case double: Double =&gt; println(s\"match Double $double\") case boolean: Boolean =&gt; println(s\"match Boolean $boolean\") case matchType: MatchType =&gt; println(s\"match MatchType $matchType\") case _: Any =&gt; println(\"nothing\") &#125; &#125;&#125;class MatchType &#123;&#125; 数组、元组、集合 123456789101112131415161718192021222324252627282930313233343536373839object CaseDemo03 &#123; def main(args: Array[String]): Unit = &#123; /* * 匹配数组 */ val arr = Array(1, 2, 3, 4) arr match &#123; case Array(1, x, y, z) =&gt; println(s\"case: $x,$y,$z\") case Array(_, x, y) =&gt; println(s\"case: $x,$y\") case _ =&gt; println(\"nothing\") &#125; /* * 匹配元组 * 注意：元组匹配中的时候，元组内的个数是确定的 */ val tup = (1, 4, 6, 8) tup match &#123; case (1, x, y, z) =&gt; println(s\"case: $x,$y,$z\") case (4, w, s, t) =&gt; println(s\"case: $w,$s,$t\") case _ =&gt; println(\"nothing\") &#125; /* * 匹配集合 * 注意： :: 从右往左向队列的头部追加数据，创造新的列表。 * 用法为 x::list,其中x为加入到头部的元素； * Nil表示空的集合； * case1表示是一个空的list集合+0 就是只有一个0的list集合。 */ val list = List(0, 1, 3, 5, 6) list match &#123; case 0 :: Nil =&gt; println(\"case1:0\") case a :: b :: c :: d :: e :: Nil =&gt; println(s\"case2 : $a,$b,$c,$d,$e\") case 0 :: b :: Nil =&gt; println(s\"case3: $b\") case List(0, 1, 3, 5, 6) =&gt; println(\"这个也可以匹配到\") case _ =&gt; println(\"not found\") &#125; &#125;&#125; 样例类（case class） 123456789101112131415161718/** * 样例类 * 注意：在模式匹配case中，需要将参数设定好 */object CaseDemo04 &#123; def main(args: Array[String]): Unit = &#123; val arr = Array(HertBeat(2000L), SubmitTask(\"1000\", \"提交\"), CheckTimeOutTask) arr(Random.nextInt(arr.length)) match &#123; case HertBeat(time) =&gt; println(\"HertBeat\") case SubmitTask(id, task) =&gt; println(\"submitTask\") case CheckTimeOutTask =&gt; println(\"CheckTimeOutTask\") &#125; &#125;&#125;case class HertBeat(time: Long)case class SubmitTask(id: String, task: String)case class CheckTimeOutTask() Option 类型 在 scala 中 Option 类型用样例类来表示可能存在或者可能不存在的值(Option 的子类有Some 和 None)。Some 包装了某个值，None 表示没有值。 12345678910111213object CaseDemo05 &#123; def main(args: Array[String]) &#123; val map = Map(\"a\" -&gt; 1, \"b\" -&gt; 2) val v = map.get(\"b\") match &#123; case Some(i) =&gt; i case None =&gt; 0 &#125; println(v) //更好的方式 val v1 = map.getOrElse(\"c\", 0) println(v1) &#125;&#125; 偏函数 被包在花括号内没有 match 的一组 case 语句是一个偏函数，它是 PartialFunction[A, B]的一个实例，A 代表输入参数类型，B 代表返回结果类型，常用作输入模式匹配，偏函数最大的特点就是它只接受和处理其参数定义域的一个子集。 123456789101112131415161718object CaseDemo06 &#123; val func1: PartialFunction[String, Int] = &#123; case \"one\" =&gt; 1 case \"two\" =&gt; 2 case _ =&gt; -1 &#125; def func2(num: String): Int = num match &#123; case \"one\" =&gt; 1 case \"two\" =&gt; 2 case _ =&gt; -1 &#125; def main(args: Array[String]) &#123; println(func1(\"one\")) println(func2(\"one\")) &#125;&#125;","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala隐式转换、隐式参数与隐式类 ","date":"2019-05-27T02:15:18.000Z","path":"Scala编程语言/7.scala隐式转换/","text":"Scala隐式转换、隐式参数与隐式类 隐式转换 什么是隐式转换 隐式转换将某种类型的对象转换成其他类型的对象，其最核心的就是定义隐式转换函数，即implicit conversion function。定义的隐式转换函数，只要在编写的程序内引入，就会被Scala自动使用。Scala会根据隐式转换函数的签名，在程序中使用到隐式转换函数接收的参数类型定义的对象时，会自动将其传入隐式转换函数，转换为另外一种类型的对象并返回。这就是“隐式转换”。 隐式转换函数通常不会由用户手动调用，而是由Scala进行调用。因此通常建议将隐式转换函数的名称命名为“one2one”的形式。 举个例子：比如你去买票的时候，有正常的窗口和特殊人群窗口（学生，军人，残疾人…）,你定义了学生的这个类，创建学生的对象，但是没用啊！人家特殊窗口接收的参数只是特殊人群，你怎么证明你是特殊人群呢-----所以你就要拿着你学生证（名字），工作人员对照后发现你属于特殊人群，他就给你开了个证明，你就是特殊人群，这个时候你就可以去买票了 案例一：特殊售票窗口（只接受特殊人群，比如学生、老人等） 我们来看案例一：你会发现在隐式转换中先判断你的类是属于什么( if (obj.getClass == classOf[Student]))，然后证明（ val stu = obj.asInstanceOf[Student），最后把你列入特殊人群（new SpecialPerson(stu.name) ）。 123456789101112131415class SpecialPerson(val name: String)class Student(val name: String)class Older(val name: String)implicit def object2SpecialPerson (obj: Object): SpecialPerson = &#123; if (obj.getClass == classOf[Student]) &#123; val stu = obj.asInstanceOf[Student]; new SpecialPerson(stu.name) &#125; else if (obj.getClass == classOf[Older]) &#123; val older = obj.asInstanceOf[Older]; new SpecialPerson(older.name) &#125; else Nil&#125;var ticketNumber = 0def buySpecialTicket(p: SpecialPerson) = &#123; ticketNumber += 1 \"T-\" + ticketNumber&#125; 正常情况下，buySpecialTicket方法接受不了Student和Older类型的对象，如果想这样做，就要用隐式转换，将这两种类型的对象转换成SpecialPerson类型（内部创建SpecialPerson的对象） 案例二：让 File 类具备 RichFile 类中的 read 方法 123456789101112131415161718192021222324package cn.cheng.implic_demoimport java.io.Fileimport scala.io.Sourceobject Implic&#123;//定义隐式转换方法 implicit def file2RichFile(file: File)=new RichFile(file)&#125;class RichFile(val f:File) &#123; def read()=Source.fromFile(f).mkString&#125;//使用import导入隐式转换方法import Implic._object RichFile&#123; def main(args: Array[String]) &#123; val f=new File(\"E://text.txt\")//通过隐式转换，让File类具备了RichFile类中的方法 val content=f.read() println(content) &#125;&#125; 隐式转换作用域 Scala默认使用两种隐式转换，一种是源类型，或者目标类型的伴生对象内的隐式转换函数；一种是当前程序作用域内的可以用唯一标识符表示的隐式转换函数。 如果两种都没有还可以手动导入导入：使用import语法引入某个包下的隐式转换函数，比如import implic._ 隐转换发生时机 什么时候会进行隐式转换呢，分为三种情况 1、调用某个函数，但是给函数传入的参数的类型，与函数定义的接收参数类型不匹配（案例：特殊售票窗口） 2、使用某个类型的对象，调用某个方法，而这个方法并不存在于该类型时（案例：超人变身） 3、使用某个类型的对象，调用某个方法，虽然该类型有这个方法，但是给方法传入的参数类型，与方法定义的接收参数的类型不匹配 案例三：超人变身 123456789class Man(val name: String)class Superman(val name: String) &#123; def emitLaser = println(\"emit a laster!\")&#125;implicit def man2superman(man: Man): Superman = new Superman(man.name)val leo = new Man(\"leo\")leo.emitLaser 普通人没有办法用镭射眼（ emitLaser方法），但是进行隐式转换后，Man==&gt;Superman，就可以调用了 隐式参数 所谓的隐式参数，指的是在函数或者方法中，定义一个用implicit修饰的参数，此时Scala会尝试找到一个指定类型的，用implicit修饰的对象，即隐式值，并注入参数。 Scala会在两个范围内查找：一种是当前作用域内可见的val或var定义的隐式变量；一种是隐式参数类型的伴生对象内的隐式值 案例一：考试签到 考生到考场签到，所有人共用一支笔SignPen,这个时候就可以把&quot;笔&quot;变量，定义为隐式的 123456789101112131415object Text &#123; class SignPen&#123; def write(content:String) = println(content) &#125; implicit val pen = new SignPen def signForExam(name:String) (implicit pen: SignPen)&#123; pen.write(name + \" come to exam in time.\") &#125; def main(args: Array[String]): Unit = &#123; signForExam(\"pk\") &#125;&#125; 在调用的时候，你只需要==&gt;signForExam(“pk”)调用,scala就会自动去找隐式的SignPen对象，这样就可以实现调用了 案例二： 12345678910111213object ImplicitParameterDemo &#123; def testParam(implicit name: String) &#123; println(name) &#125; implicit val name = \"lujinhong\" def main(args: Array[String]) &#123; testParam(\"My name\") testParam &#125;&#125; 上述示例中将testParam中的参数name设为了implicit，因此当name未赋值时，它会寻找作用域内叫做name的隐式参数所定义的值。有几个注意事项： 函数及隐式参数都必须定义为implicit。 也可以显式定义函数，如本例中的”My name”，此时隐式参数不生效。 隐式类 案例一：作用于Int 12345678910object ImplicitClassDemo &#123; def main(args: Array[String]) &#123; println(2.addOne(1)) println(2.addOne(3)) &#125; implicit class Caculator(x: Int) &#123; def addOne(a: Int): Int = a + 1 &#125;&#125; 隐式类定义中的(x: Int) ，指定了当Int这种类型可以在这个隐式类中寻找方法，即这个类中定义的所有方法都会成为Int的隐式函数。 案例二：作用于String 12345678910object ImplicitClassDemo &#123; def main(args: Array[String]) &#123; println(\"d\".addOne(1)) println(\"kk\".addOne(3)) &#125; implicit class Caculator(x: String) &#123; def addOne(a: Int): Int = a + 1 &#125;&#125;","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala常用高阶函数(二）","date":"2019-05-22T01:15:46.000Z","path":"Scala编程语言/6.scala常用高阶函数（二）/","text":"Scala常用高阶函数（二） 柯里化 柯里化(Currying)指的是把原来接受多个参数的函数变换成接受一个参数的函数过程=&gt;返回值是一个函数，参数就是其余参数，而且第一个参数的值会累计在此函数中。 定义一个函数： 123456789101112//mutiplyBy这个函数的返回值是一个函数//该函数的输入是Doulbe，返回值也是Doublescala&gt; def multiplyBy(factor:Double)=(x:Double)=&gt;factor*xmultiplyBy: (factor: Double)Double =&gt; Double//返回的函数作为值函数赋值给变量xscala&gt; val x=multiplyBy(10)x: Double =&gt; Double = &lt;function1&gt; //变量x现在可以直接当函数使用scala&gt; x(50)res10: Double = 500.0 上述代码可以像这样使用： 12345scala&gt; def multiplyBy(factor:Double)=(x:Double)=&gt;factor*xmultiplyBy: (factor: Double)Double =&gt; Double //这是高阶函数调用的另外一种形式scala&gt; multiplyBy(10)(50) 那函数柯里化(curry）是怎么样的呢？其实就是将multiplyBy函数定义成如下形式 123scala&gt; def multiplyBy(factor:Double)(x:Double)=x*factormultiplyBy: (factor: Double)(x: Double)Double 即通过(factor:Double)(x:Double)定义函数参数，该函数的调用方式如下： 12345678910//柯里化的函数调用方式scala&gt; multiplyBy(10)(50)res11: Double = 500.0 //但此时它不能像def multiplyBy(factor:Double)=(x:Double)=&gt;factor*x函数一样，可以输入单个参数进行调用scala&gt; multiplyBy(10) &lt;console&gt;:10: error: missing arguments for method multiplyBy;follow this method with `_' if you want to treat it as a partially applied function multiplyBy(10) 错误提示函数multiplyBy缺少参数，如果要这么做的话，需要将其定义为偏函数 12scala&gt; multiplyBy(10)_res12: Double =&gt; Double = &lt;function1&gt; 那现在我们接着对偏函数进行介绍 偏函数 如果你想定义一个函数，而让它只接受和处理其参数定义域范围内的子集，对于这个参数范围外的参数则抛出异常，这样的函数就是偏函数（顾名思异就是这个函数只处理传入来的部分参数）。 需求： 给定List(1,2,3,4,“test”)，里面的每个Int元素+1 1234567891011121314val l = List(1,2,3,4,\"test\")//元素+1 ==&gt; 返回List(2,3,4,5)def f1(n:Any) = &#123; n.isInstanceOf[Int]&#125;def f2(n:Any)= &#123; n.asInstanceOf[Int]&#125;def f3(n:Int) = &#123; n + 1&#125;println(l.filter(f1).map(f2).map(f3)运行结果：List(2, 3, 4, 5) 使用偏函数实现： 123456789101112val addOne = new PartialFunction[Any,Int] &#123; override def isDefinedAt(x: Any): Boolean = if(x.isInstanceOf[Int])&#123; true &#125;else&#123; false &#125; override def apply(v1: Any): Int = v1.asInstanceOf[Int] + 1&#125;println(l.collect(addOne)运行结果：List(2, 3, 4, 5) 简化代码： 1234def f4:PartialFunction[Any,Int] = &#123;case i:Int =&gt; i + 1&#125; l.collect(f4) 直接调用： 1234567891011scala&gt; val l = List(1,2,3,4,\"test\")l: List[Any] = List(1, 2, 3, 4, test)scala&gt; l.collect(&#123; case i:Int =&gt; i + 1 &#125;)res13: List[Int] = List(2, 3, 4, 5)//最简化的版本：scala&gt; l.collect&#123; case i:Int =&gt; i + 1 &#125;res14: List[Int] = List(2, 3, 4, 5)","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala常用高阶函数 ","date":"2019-05-21T02:06:09.000Z","path":"Scala编程语言/5.scala常用高阶函数/","text":"Scala常用高阶函数 高阶函数介绍 MapReduce中仅仅只提供了两个函数map、reduce，而spark中提供非常多的函数供调用，更加人性化，开发效率更高。 map map：一一映射，对里面的每一个元素都作用一个function 123val l = List(1,2,3,4,5,6,7,8) println(l.map((x:Int) =&gt; x*2)) //每个元素都乘以2精简表示：println(l.map(_*2)) 1234scala&gt; val list = List(Array((\"hello\",3)),Array((\"world\",2)))list: List[Array[(String, Int)]] = List(Array((hello,3)), Array((world,2)))scala&gt; list.map(x =&gt; x.map(t =&gt; (t._1,t._2+2))) //每个tuple的第二个元素都加上2res1: List[Array[(String, Int)]] = List(Array((hello,5)), Array((world,4))) filter 123456scala&gt; val l = List(1,2,3,4,5,6,7,8)l: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8)scala&gt; l.filter(_ &gt; 6)res3: List[Int] = List(7, 8)scala&gt; l.map(_ * 2).filter(_ &gt; 8) //链式编程res4: List[Int] = List(10, 12, 14, 16) reduce 12345678//两两相加println(l.reduce((x:Int,y:Int) =&gt; x+y))简写如下：println(l.reduce( _ + _ ))//两两相减println(l.reduce((x:Int,y:Int) =&gt; x-y))简写如下:println(l.reduce(-)) reduceright 1234567891011121314151617//测试它的运行过程:val l = List(1,2,3,4,5,6,7,8)l.reduceRight((x:Int,y:Int) =&gt;&#123;println(x + “,” + y)x - y&#125;)结果：7,86,-15,74,-23,62,-31,5//运行结果scala&gt; l.reduceRight(_ - _)res2: Int = -4 fold 123456789101112131415161718//测试它的运行过程val l = List(1,2,3,4,5,6,7,8)l.fold(10)((x:Int,y:Int) =&gt;&#123;println(x + \":\" + y)x - y&#125;)结果：10:19:27:34:40:5-5:6-11:7-18:8运行结果：scala&gt; l.fold(10)(_ - _)res3: Int = -26 flatten flatten的作用是把输出拉平。 1234scala&gt; val a = List(List(1,2),List(3,4),List(5,6))a: List[List[Int]] = List(List(1, 2), List(3, 4), List(5, 6))scala&gt; a.flattenres4: List[Int] = List(1, 2, 3, 4, 5, 6) flatMap flatMap = map + flatten 123456789scala&gt; val a = List(List(1,2,3),List(3,4),List(4,5,6))a: List[List[Int]] = List(List(1, 2, 3), List(3, 4), List(4, 5, 6))scala&gt; val b = a.flattenb: List[Int] = List(1, 2, 3, 3, 4, 4, 5, 6)scala&gt; b.map(_*2)res5: List[Int] = List(2, 4, 6, 6, 8, 8, 10, 12)等价于==&gt;scala&gt; a.flatMap(_.map(_*2))res6: List[Int] = List(2, 4, 6, 6, 8, 8, 10, 12) groupBy 分组 123456scala&gt; val array = Array((\"a\",100),(\"b\",10),(\"a\",10),(\"d\",10))array: Array[(String, Int)] = Array((a,100), (b,10), (a,10), (d,10))scala&gt; array.groupBy(x =&gt; x._1)res7: scala.collection.immutable.Map[String,Array[(String, Int)]] = Map(b -&gt; Array((b,10)), d -&gt; Array((d,10)), a -&gt; Array((a,100), (a,10)))scala&gt; array.groupBy(x =&gt; x._2)res8: scala.collection.immutable.Map[Int,Array[(String, Int)]] = Map(100 -&gt; Array((a,100)), 10 -&gt; Array((b,10), (a,10), (d,10))) find 123456scala&gt; val array = Array(1,2,3,4,5)array: Array[Int] = Array(1, 2, 3, 4, 5)scala&gt; array.find(x =&gt; x &gt; 3)res9: Option[Int] = Some(4)scala&gt; array.find(x =&gt; x &gt; 30)res10: Option[Int] = None 词频统计实现 测试步骤： 1234567891011121314scala&gt; val list = List(\"hello world welcome\",\"hello world\")list: List[String] = List(hello world welcome, hello world)scala&gt; val words: List[String] = list.flatMap(_.split(\" \"))words: List[String] = List(hello, world, welcome, hello, world)scala&gt; val wordWithOne: List[(String, Int)] = words.map(x =&gt; (x, 1))wordWithOne: List[(String, Int)] = List((hello,1), (world,1), (welcome,1), (hello,1), (world,1))scala&gt; val groupByData: Map[String, List[(String, Int)]] = wordWithOne.groupBy(x =&gt; x._1)groupByData: Map[String,List[(String, Int)]] = Map(welcome -&gt; List((welcome,1)), world -&gt; List((world,1), (world,1)), hello -&gt; List((hello,1), (hello,1)))scala&gt; val result: Map[String, Int] = groupByData.mapValues(x =&gt; x.map(t =&gt; t._2).sum)result: Map[String,Int] = Map(welcome -&gt; 1, world -&gt; 2, hello -&gt; 2)scala&gt; result.toList.sortBy(_._2)res11: List[(String, Int)] = List((welcome,1), (world,2), (hello,2))scala&gt; result.toList.sortBy(-_._2)res12: List[(String, Int)] = List((world,2), (hello,2), (welcome,1)) 词频统计实现： 12345678910val list = List(\"hello world welcome\",\"hello world\")val words: List[String] = list.flatMap(_.split(\" \"))val wordWithOne: List[(String, Int)] = words.map(x =&gt; (x, 1))val groupByData: Map[String, List[(String, Int)]] = wordWithOne.groupBy(x =&gt; x._1)val result: Map[String, Int] = groupByData.mapValues(x =&gt; x.map(t =&gt; t._2).sum)println(result.toList.sortBy(_._2)) //map ==&gt;list,默认升序println(result.toList.sortBy(-_._2)) //加-改成降序 运行结果：List((welcome,1), (world,2), (hello,2))List((world,2), (hello,2), (welcome,1))","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala基础04","date":"2019-05-20T14:12:36.000Z","path":"Scala编程语言/Scala基础04/","text":"Scala基础04 集合 概述 scala提供了一套很好的集合实现，提供了一些集合类型的抽象。 scala 集合分为可变的和不可变的集合。可变集合可以在适当的地方被更新或扩展。这意味着你可以修改，添加，移除一个集合的元素。而不可变集合类永远不会改变。不过，你仍然可以模拟添加，移除或更新操作。但是这些操作将在每一种情况下都返回一个新的集合，同时使原来的集合不发生改变。 集合中基本结构： 根据scala集合体系图可以很清晰地看出scala中的集合类可以分为三大类： 1.Seq，是一组有序的元素。 2.Set，是一组没有重复元素的集合。 3.Map，是一组k-v对。 Seq Seq主要由两部分组成：IndexedSeq与LinearSeq。现在我们简单看下这两种类型。 首先看IndexedSeq，很容易看出来这种类型的主要访问方式是通过索引，默认的实现方式为vector。 12345678def test() = &#123; val x = IndexedSeq(1,2,3) println(x.getClass) println(x(0)) val y = Range(1, 5) println(y)&#125; 将以上函数运行起来以后，输出如下： 123class scala.collection.immutable.Vector1Range(1, 2, 3, 4) 而作为LinearSeq，主要的区别在于其被分为头与尾两部分。其中，头是容器内的第一个元素，尾是除了头元素以外剩余的其他所有元素。LinearSeq默认的实现是List。 12345678def test() = &#123; val x = collection.immutable.LinearSeq(\"a\", \"b\", \"c\") val head = x.head println(s\"head is: $head\") val y = x.tail println(s\"tail of y is: $y\")&#125; 将上面的代码运行起来以后，得到的结果如下： 12head is: atail of y is: List(b, c) Set 与其他任何一种编程语言一样，Scala中的Set集合类具有如下特点： 1.不存在有重复的元素。 2.集合中的元素是无序的。换句话说，不能以索引的方式访问集合中的元素。 3.判断某一个元素在集合中比Seq类型的集合要快。 Scala中的集合分为可变与不可变两种，对于Set类型自然也是如此。先来看看示例代码： 1234567891011121314def test() = &#123; def test() = &#123; val x = immutable.HashSet[String](\"a\",\"c\",\"b\") //x.add(\"d\")无法使用，因为是不可变集合，没有add方法。 val y = x + \"d\" + \"f\" // 增加新的元素，生成一个新的集合 val z = y - \"a\" // 删除一个元素，生成一个新的集合 val a = Set(1,2,3) val b = Set(1,4,5) val c = a ++ b // 生成一个新的集合，增加集合 val d = a -- b // 生成一个新的集合，去除集合 val e = a &amp; b // 与操作 val f = a | b // 或操作 &#125; 因为上面代码里的集合类型都是不可变类型，所以所有语句结果其实都是生成一个新的集合。 12345678910111213def test() = &#123; val x = new mutable.HashSet[String]() x += \"a\" // 添加一个新的元素。注意此时没有生成一个新的集合 x.add(\"d\") //因为是可变集合，所以有add方法 x ++= Set(\"b\", \"c\") // 添加一个新的集合 x.foreach(each =&gt; println(each)) x -= \"b\" // 删除一个元素 println() x.foreach(each =&gt; println(each)) println() val flag = x.contains(\"a\") // 是否包含元素 println(flag)&#125; 将上面这段代码运行起来以后，得到的结果如下： 12345678910cdab cda true Map Map这种数据结构是日常开发中使用非常频繁的一种数据结构。Map作为一个存储键值对的容器（key－value），其中key值必须是唯一的。 默认情况下，我们可以通过Map直接创建一个不可变的Map容器对象，这时候容器中的内容是不能改变的。示例代码如下。 12345678910111213def test() = &#123; val peoples = Map(\"john\" -&gt; 19, \"Tracy\" -&gt; 18, \"Lily\" -&gt; 20) //不可变 // people.put(\"lucy\",15) 会出错，因为是不可变集合。 //遍历方式1 for(p &lt;- peoples) &#123; print(p + \" \") // (john,19) (Tracy,18) (Lily,20) &#125; //遍历方式2 peoples.foreach(x =&gt; &#123;val (k, v) = x; print(k + \":\" + v + \" \")&#125;) //john:19 Tracy:18 Lily:20 //遍历方式3 peoples.foreach (&#123; case(k, v) =&gt; print(s\"key: $k, value: $v \")&#125;) //key: john, value: 19 key: Tracy, value: 18 key: Lily, value: 20&#125; 上面代码中的hashMap是不可变类型。 如果要使用可变类型的map，可以使用mutable包中的map相关类。 123456789101112131415def test() = &#123; val map = new mutable.HashMap[String, Int]() map.put(\"john\", 19) // 因为是可变集合，所以可以put map.put(\"Tracy\", 18) map.contains(\"Lily\") //false val res = getSome(map.get(\"john\")) println(res) //Some(19)&#125; def getSome(x:Option[Int]) : Any = &#123; x match &#123; case Some(s) =&gt; s case None =&gt; \"None\" &#125;&#125; 泛型 泛型 1.概述 泛型用于指定方法或类可以接受任意类型参数，参数在实际使用时才被确定，泛型可以有效地增强程序的适用性，使用泛型可以使得类或方法具有更强的通用性。泛型的典型应用场景是集合及集合中的方法参数，可以说同java一样，scala中泛型无处不在，具体可以查看scala的api。 2.泛型类、泛型方法 scala中的泛型主要包括泛型类和泛型方法 泛型类：指定类可以接受任意类型参数。 泛型方法：指定方法可以接受任意类型参数。 3.示例： 1）定义泛型类 1234567891011121314151617181920212223242526/** * 类上定义的泛型，只要是Comparable就可以传递 */class GenericTest1[T &lt;: Comparable[T]] &#123; def choose(one:T,two:T): T =&#123; //定义一个选择的方法 if(one.compareTo(two) &gt; 0) one else two &#125; &#125; class Boy(val name:String,var age:Int) extends Comparable[Boy]&#123; override def compareTo(o: Boy): Int = &#123; this.age - o.age &#125;&#125; object GenericTestOne&#123; def main(args: Array[String]): Unit = &#123; val gt = new GenericTest1[Boy] val pk = new Boy(\"pk\",60) val xingxing= new Boy(\"xingxing\",66) val boy = gt.choose(pk,xingxing) println(boy.name) &#125;&#125; 运行结果： 123xingxingProcess finished with exit code 0 2）定义泛型方法 123456789101112131415161718192021222324class GenericTest2&#123; //在方法上定义泛型 def choose[T &lt;: Comparable[T]](one:T,two:T): T =&#123; if(one.compareTo(two) &gt; 0) one else two &#125;&#125; class Boy(val name:String,var age:Int) extends Comparable[Boy]&#123; override def compareTo(o: Boy): Int = &#123; this.age - o.age &#125;&#125; object GenericTestTwo&#123; def main(args: Array[String]): Unit = &#123; val gt = new GenericTest2 val pk = new Boy(\"pk\",60) val xingxing = new Boy(\"xingxing\",66) val boy = gt.choose(pk,xingxing) println(boy) &#125;&#125; 运行结果: 123xingxingProcess finished with exit code 0 上界和下界 1.概述 在指定泛型类型时，有时需要界定泛型类型的范围，而不是接收任意类型。比如，要求某个泛型类型，必须是某个类的子类，这样在程序中就可以放心的调用父类的方法，程序才能正常的使用与运行。此时，就可以使用上下边界Bounds的特性。 Scala的上下边界特性允许泛型类型是某个类的子类，或者是某个类的父类。 1）S &lt;: T 这是类型上界的定义，也就是S必须是类型T的子类（或本身，自己也可以认为是自己的子类)。 2）U &gt;: T 这是类型下界的定义，也就是U必须是类型T的父类(或本身，自己也可以认为是自己的父类)。 2.示例 1）上界示例 参考上面的泛型方法 2）下界示例 123456789101112131415161718class GranderFatherclass Father extends GranderFatherclass Son extends Fatherclass Tongxue object Card&#123; def getIDCard[T &gt;: Son](person:T): Unit =&#123; println(\"OK,交给你了\") &#125; def main(args: Array[String]): Unit = &#123; getIDCard[GranderFather](new Father) getIDCard[GranderFather](new GranderFather) getIDCard[GranderFather](new Son) //getIDCard[GranderFather](new Tongxue) //Tongxue不是Son的父类，运行报错，注释掉 &#125;&#125; 运行结果： 12345OK,交给你了OK,交给你了OK,交给你了Process finished with exit code 0 协变和逆变 Scala中协变和逆变主要作用是用来解决参数化类型的泛化问题。由于参数化类型的参数（参数类型）是可变的，当两个参数化类型的参数是继承关系（可泛化），那被参数化的类型是否也可以泛化呢？在Java中这种情况下是不可泛化的，然而Scala提供了三个选择，即协变、逆变和非变，解决了参数化类型的泛化问题。 对于一个带类型参数的类型，比如 List[T]： 如果对A及其子类型B，满足 List[B]也符合 List[A]的子类型，那么就称为covariance(协变)； 如果 List[A]是 List[B]的子类型，即与原来的父子关系正相反，则称为contravariance(逆变)。 协变 12345678910____ _____________ | | | || A | | List[ A ] ||_____| |_____________| ^ ^ | | _____ _____________ | | | || B | | List[ B ] ||_____| |_____________| 逆变 12345678910___ _____________ | | | || A | | List[ B ] ||_____| |_____________| ^ ^ | | _____ _____________ | | | || B | | List[ A ] ||_____| |_____________| 协变和逆变使用“+”,“-”差异标记。当我们定义一个协变类型List[+A]时，List[Child]可以是List[Parent]的子类型，当我们定义一个逆变类型List[-A]时，List[Child]可以是List[Parent]的父类型 假设有参数化特质List，那么可以有三种定义。如下所示： (1) trait List [T]{} 非变。这种情况下，当类型S是类型A的子类型，则List [S]不可以认为是List [A]的子类型或父类型，这种情况和Java是一样的。 (2) trait List [+T]{} 协变。如果S extends A (S为子类型，A为父类型)，则List [S]为子类型，List [A]为父类型S &lt;: A =&gt; List [S] &lt;: List [A]。 (3) trait List [-T]{} 逆变。如果S extends A (S为子类型，A为父类型)，则List [S]为父类型，List [A]为子类型，和协变互逆S &lt;: A =&gt; Queue[S] &gt;: Queue[A]。 那么，在Scala中如何定义协变逆变类呢，举例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960object CovariantAndContravariantDemo &#123; def main(args: Array[String]): Unit = &#123; //不变 def inv1: Invariant[Tiger] = new Invariant[Tiger]() //不可以赋值，编译器编译不通过 def inv2: Invariant[Cat] = inv1 //协变 def cov1: Covariant[Tiger] = new Covariant[Tiger]() //可以直接赋值 def cov2: Covariant[Cat] = cov1 //逆变 def cont1: Contravariant[Cat] = new Contravariant[Cat]() //可以直接赋值 def cont2: Contravariant[Tiger] = cont1 &#125; /** * 定义一个不可变的类 */ class Invariant[T] &#123;&#125; /** * 定义一个协变的类 */ class Covariant[+T] &#123;&#125; /** * 定义一个逆变的类 */ class Contravariant[-T] &#123;&#125; /** * 定义一个动物的类，具有行为（吃） */ class Animal &#123; def eat() &#123; println(\"Animal likeeat botany.\") &#125; &#125; /** * 定义动物（猫）的类，具有行为（吃） */ class Cat extends Animal &#123; override def eat() &#123; println(\"Cat eatfish.\") &#125; &#125; /** * 定义动物（老虎）的类，具有行为（吃） */ class Tiger extends Cat &#123; override def eat() &#123; println(\"Tiger eatmeat.\") &#125; &#125;&#125;","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala基础03","date":"2019-05-18T14:25:36.000Z","path":"Scala编程语言/Scala基础03/","text":"Scala基础03 伴生类和伴生对象 Object 示例： 1234567object Timer&#123; var index = 0 def current()&#123; index += 1 index &#125;&#125; 调用： 123456789object TestApp &#123; def main(args: Array[String]): Unit = &#123; for(i &lt;- 1 to 3) &#123; Timer.current() println(Timer.index) &#125; &#125;&#125; 结果： 12345123Process finished with exit code 0 Object中的属性和方法，可以通过Object名称(Timer)直接调用。 伴生类和伴生对象 1.定义： 在一个源文件中，如果出现object与class名称相同的情况，那么就可以将该object成为该class的伴生对象，该class则可以成为该object的伴生类。 当一个源文件中，只有object而没有对应的伴生类时，object被称之为Standalone Object-独立对象。 2.使用伴生类或者伴生对象的apply方法： 示例： 1234567891011121314151617object ApplyDemo &#123; def main(args: Array[String]) &#123; ApplyTest() //调用object中的apply方法 val a = new ApplyT a() //调用class中的apply方法 &#125;&#125;class ApplyTest &#123; def apply() &#123; println(\"enter class apply\") &#125;&#125;object ApplyTest &#123; def apply() &#123; println(\"enter object apply\") &#125;&#125; 用类名()，会主动调用object中的apply方法（这是默认的，必须命名为apply），我们知道object关键字在scala中是用来表示单例对象的，也就是说object ApplyTest我们不需要new出来就可以直接使用，但是class与java中的class关键字是一致的，我们想要使用通过new关键字获取一个对象使用，调用伴生类中的apply方法，直接用对象()即可。 枚举 示例： 1234567891011121314151617object EnumApp &#123; def main(args: Array[String]): Unit = &#123; println(Season(1)) println(Season.withName(\"冬天\")) for(ele &lt;- Season.values)&#123; println(ele) &#125; &#125; object Season extends Enumeration &#123; val SPRING = Value(1,\"春天\") val SUMMER = Value(2,\"夏天\") val AUTUMN = Value(3,\"秋天\") val WINTER = Value(4,\"冬天\") &#125;&#125; 运行结果： 12345678春天冬天春天夏天秋天冬天Process finished with exit code 0 case class 在Scala中存在case class，它其实就是一个普通的class。但是它又和普通的class略有区别，如下： 1.初始化的时候可以不用new，当然你也可以加上，普通类一定需要加new。 12345678scala&gt; case class Iteblog(name:String)defined class Iteblogscala&gt; val iteblog = Iteblog(\"iteblog_hadoop\")iteblog: Iteblog = Iteblog(iteblog_hadoop)scala&gt; val iteblog = new Iteblog(\"iteblog_hadoop\")iteblog: Iteblog = Iteblog(iteblog_hadoop) 2.toString的实现更漂亮。 12scala&gt; iteblogres5: Iteblog = Iteblog(iteblog_hadoop) 3.默认实现了equals 和hashCode。 12345678scala&gt; val iteblog2 = Iteblog(\"iteblog_hadoop\")iteblog2: Iteblog = Iteblog(iteblog_hadoop)scala&gt; iteblog == iteblog2res6: Boolean = truescala&gt; iteblog.hashCoderes7: Int = 57880342 4.默认是可以序列化的，也就是实现了Serializable。 12345678910111213141516171819scala&gt; class Adefined class Ascala&gt; import java.io._import java.io._scala&gt; val bos = new ByteArrayOutputStreambos: java.io.ByteArrayOutputStream =scala&gt; val oos = new ObjectOutputStream(bos)oos: java.io.ObjectOutputStream = java.io.ObjectOutputStream@4c257aefscala&gt; oos.writeObject(iteblog)scala&gt; val a = new Aa: A = $iwC$$iwC$A@71687b10scala&gt; oos.writeObject(a)java.io.NotSerializableException: $iwC$$iwC$A 5.自动从scala.Product中继承一些函数。 6.case class构造函数的参数是public级别的，我们可以直接访问。 12scala&gt; iteblog.nameres11: String = iteblog_hadoop 7.支持模式匹配。 其实感觉case class最重要的特性应该就是支持模式匹配。这也是我们定义case class的唯一理由，难怪Scala官方也说：It makes only sense to define case classes if pattern matching is used to decompose data structures. 。来看看下面的例子： 1234567891011121314object caseTest &#123; def main(args: Array[String]): Unit = &#123; val p:Person = Student(\"Tom\",22,80) p match&#123; case Student(name,age,sno) =&gt; println(\"this is a student!\") case Teacher(name,age,tno) =&gt; println(\"this is a teacher!\") case Nobody(name) =&gt; println(\"unknown:\"+name) &#125; &#125; &#125; abstract class Person case class Student(name:String,age:Int,sno:Int) extends Person case class Teacher(name:String,age:Int,tno:Int) extends Person case class Nobody(name:String) extends Person 接口trait 示例： 1234567891011121314151617181920//声明一个 traittrait UserService &#123; def addUser()&#125;trait Logger &#123; def log()&#125;//对于多重继承，使用 with 关键字class UserServiceImpl extends UserService with Logger &#123;//重写父类的方法 override def addUser(): Unit = &#123; println(\"UserServiceImpl.addUser invoked...\") &#125; override def log(): Unit = &#123; println(\"UserServiceImpl.log invoked...\") &#125;&#125; 调用： 12345678object TraitApp &#123; def main(args: Array[String]): Unit = &#123; val userService = new UserServiceImpl userService.addUser() userService.log() &#125;&#125; 运行结果： 1234UserServiceImpl.addUser invoked...UserServiceImpl.log invoked...Process finished with exit code 0","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala基础02 ","date":"2019-05-16T13:25:36.000Z","path":"Scala编程语言/Scala基础02/","text":"Scala基础02 循环 to、Range、until 1.to 1 to 10 等同于1.to(10)，区间左闭右闭，包括10 12scala&gt; 1 to 10res0: scala.collection.immutable.Range.Inclusive = Range 1 to 10 2.Range 区间左闭右开，不包括10 12scala&gt; Range(1,10)res1: scala.collection.immutable.Range = Range 1 until 10 3.until 1 until 10 等同于1.until(10)，与Range相同，区间左闭右开，不包括10 12scala&gt; 1 until 10res2: scala.collection.immutable.Range = Range 1 until 10 4.by by是步长，用法如下： 1234scala&gt; 1 to 10 by 2res3: scala.collection.immutable.Range = inexact Range 1 to 10 by 2scala&gt; 1.to(10).by(2)res4: scala.collection.immutable.Range = inexact Range 1 to 10 by 2 遍历 1.foreach 1234scala&gt; Array(\"Hadoop\",\"Hive\",\"Spark\").foreach(x =&gt; println(x))HadoopHiveSpark 2.for 不需要通过下标来访问 1234val array = Array(\"Hadoop\",\"Hive\",\"Spark\")for(x &lt;- array ) &#123; println(x) &#125; 需要通过下标来访问 1234val array = Array(\"Hadoop\",\"Hive\",\"Spark\")for(i &lt;- 0 to array.length - 1) &#123; println(array(i))&#125; 带条件 123for (i &lt;- 1 to 10 if i%2 ==0)&#123; println(i)&#125; 3.while 求1到100的和 123456var(num, sum) = (100, 0)while (num &gt; 0)&#123; sum = sum + num num = num - 1&#125;println(sum) 终止循环 通过标志位flag终止循环 12345678var index = truevar flag = 1while(index) &#123; if(flag &gt; 10) &#123; index = false &#125; flag += 1&#125; 参数 默认参数 Scala 可以为函数参数指定默认参数值，使用了默认参数，你在调用函数的过程中可以不需要传递参数，这时函数就会调用它的默认参数值，如果传递了参数，则传递值会取代默认值。 123456789object Test &#123; def main(args: Array[String]) &#123; println( \"返回值 : \" + addInt() ); &#125; def addInt( a:Int=5, b:Int=7 ) : Int = &#123; var sum:Int = 0 sum = a + b &#125;&#125; 由于addInt未传入参数，函数调用它的默认参数，运行结果为12。 可变长参数 Scala 允许向函数传入可变长度参数列表。想要标注可变长度参数，在参数的类型之后加一个星号。 123456789101112object Test &#123; def main(args: Array[String]) &#123; println(sum(1 to 10 : _*) ) &#125; def sum( nums:Int*) : Int = &#123; var result = 0 for (num &lt;- nums) &#123; result += num &#125; result &#125;&#125; :_ *是类型归属的一个特殊实例，它告诉编译器将一个序列类型的单个参数当作可变参数序列。 最后运行结果为55。 命名参数 当调用方法时，可以使用参数名称标记参数。借此机制，可以乱序传参。 123def num(a:String, b:String, c:String, d:String) = println(s\"$a $b $c $d\")num(\"aaa\", \"bbb\", \"ccc\", \"ddd\")num(b=\"bbb\", a=\"aaa\", d=\"ddd\", c=\"ccc\") 当部分参数被命名，部分参数没有被命名时，未命名参数应当放在命名参数之前，并且传参的结果将会遵照方法签名中的参数的顺序。 1num(\"aaa\", d=\"ddd\",b=\"bbb\", c= \"ccc\") 注意，命名参数不能用于调用Java方法。命名参数了解即可。 面向对象 定义类 1.类 scala中定义类的方式：class 类名，例如我们定义个Person类 12class Person&#123;&#125; 2.属性/方法 在类中 1）添加属性的方式：val/var 名称[:类型]=值 2）添加方法的方式：def 方法名(参数列表):返回值类型={方法体} 12345678910111213141516class Person&#123; // 类型可以省略 var age = 18 // _为占位符，那么name的默认值为null val name:String = _ // 属性前面加上private表示私有的，外面就不可以直接调用 private var gender = \"M\" // 定义方法 def playbasketball(team:String)=&#123; println(name + \" is playing basketball for \" + team) &#125; //定义方法 def drinking() = &#123; println(this.name + \" is drinking...\") &#125;&#125; 调用上面的类： 1234567891011object SimpleClassApp &#123; def main(args: Array[String]): Unit = &#123; //创建对象 val person = new Person //给对象的属性赋值 person.name=\"zhang san\" //调用对象方法 person.playBasketball(\"china\") person.drink() &#125;&#125; 运行结果如下： 1234zhangsan is playing basketball for chinazhangsan is drinkingProcess finished with exit code 0 构造函数/继承 构造函数的方式： 1.类名（参数列表…）主构造器 2.def this (参数列表…) 附属构造器 1234567891011121314class Person2(val name: String, val age: Int) &#123; println(\"进入Person2构造器\") val job = \"解说员\" var term = \"\" /** * 1.附属构造起的名称this * 2.每个附属构造起的第一行要么调用主构造器，要么调用其他附属构造起 */ def this(name: String, age: Int, team: String) &#123; this(name, age) this.term = team &#125; println(\"执行完Person2构造器\")&#125; 写一个student 继承Person2，继承的子类注意点已经在注释中写清楚了 1234567891011121314151617181920/** * 继承： * 1.子类方法/构造器执行之前，会先执行父类的构造器 * 2.对于父类没有的字段，需要加上val/var，否则访问不到 * * @param name * @param age * @param stuId */class Student(name: String, age: Int, val stuId: Int) extends Person2(name, age) &#123; println(\"进入Student构造器\") //如果要重写父类的属性或方法，那么前面要加上override override val job: String = \"学生\" //重写toString override def toString: String = &#123; this.name + \"\\t\" + this.age + \"\\t\" + this.stuId &#125; println(\"执行完Student构造器\")&#125; 调用上面的类： 12345678object ConstructApp &#123; def main(args: Array[String]): Unit = &#123; val zhangsan = new Student(\"zhangsan\", 20, 1001) println(\"name:\" + zhangsan.name + \",age:\" + zhangsan.age + \",stuId:\" + zhangsan.stuId) println(zhangsan) &#125;&#125; 运行结果如下： 12345678进入Person2构造器执行完Person2构造器进入Student构造器执行完Student构造器name:zhangsan,age:20,stuId:1001zhangsan 20 1001Process finished with exit code 0 抽象类 Scala继承一个基类跟Java很相似，注意点： 1.类中有一个或者多个方法/属性，没有实现，那么为抽象方法 2.子类必须要实现父类的抽象方法或者属性 3.抽象类不能new 4.子类重写的抽象方法或者属性，不一定要加上override 5.重写一个非抽象方法必须使用override修饰符 12345678910111213141516171819202122232425262728abstract class Person3 &#123; //抽象方法 def speak //抽象属性 val name: String //非抽象方法 def sayHello(): Unit = &#123; println(\"abstract sayHello...\") &#125; //非抽象方法 def walk(): Unit = &#123; println(\"abstract walk...\") &#125;&#125;class Student3 extends Person3 &#123; //重写抽象方法 def speak: Unit = &#123; println(\"override speak...\") &#125; val name: String = \"zhangsan\" //重写非抽象方法 override def sayHello(): Unit = &#123; println(\"override sayHello...\") &#125;&#125; 调用上面的类： 123456789101112object AbstractApp &#123; def main(args: Array[String]): Unit = &#123; val student = new Student3 println(student.name) student.speak student.sayHello() //调用继承过来的方法 student.walk() &#125;&#125; 运行结果如下： 123456zhangsanoverride speak...override sayHello...abstract walk...Process finished with exit code 0","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Scala基础01 ","date":"2019-05-15T12:25:36.000Z","path":"Scala编程语言/Scala基础01/","text":"Scala基础01 编译 1.演示代码如下： 12345678object HelloWorld &#123; def main(args: Array[String]): Unit = &#123; println(\"Hello World...FROM IDEA....\") &#125;&#125; 2.编译 1$ scalac HelloWorld.scala 然后我们会看到多了一些.class文件： 123HelloWorld.scalaHelloWorld.classHelloWorld$.class 3.运行 1$ scala HelloWorld 运行结果： 1Hello World...FROM IDEA.... 4.对比Java 与Java不同，Scala无需编译就可直接执行： 1scala HelloWorld.scala 执行结果： 1Hello World...FROM IDEA.... 数据类型 下表列出了 Scala 支持的数据类型： 1）Scala 与 Java有着相同的数据类型，在Scala中数据类型都是对象，也就是说scala没有java中的原生类型。 2）Scala数据类型分为两大类 AnyVal(值类型) 和 AnyRef(引用类型)， 注意：不管是AnyVal还是AnyRef 都是对象。 数据类型转换 1.将Int类型转成Double类型 12345678scala&gt; val a = 10a: Int = 10方法一：scala&gt; 10.asInstanceOf[Double]res0: Double = 10.0方法二：scala&gt; 10.toDoubleres1: Double = 10.0 2.判断类型 12345scala&gt; 10.isInstanceOf[Int]res2: Boolean = truescala&gt; 10.isInstanceOf[Double]res3: Boolean = false 声明 1、val, 常量声明 12val name:String = \"uriel\"val name = \"uriel\" 2.var, 变量声明 12var name:String = \"uriel\"var name = \"uriel\" 3.def，方法声明 123def add(a:Int,b:Int) = &#123; a+b&#125; 方法&amp;&amp;函数的定义 定义方法 1.一个完整的方法如下： 123def add(a:Int, b:Int):Int = &#123; return a+b //方法体最后一行作为返回值，大部分场景不要return&#125; 2.方法的最后一句为返回值的话，可以将return 去掉, 如果一个方法体只有一句代码，大括号可以去掉。 1def add(a:Int, b:Int):Int = a + b 3.如果一个方法没有返回值，其返回类型为Unit , 并且“=”号可以去掉 1234//无参数传入，调用是可不带括号，sayHellodef sayHello()&#123; println(\"你好\")&#125; 4.方法可以带返回值，也可以不带返回值，带递归时必须带返回值 1234567def milit（i：Int）：Int = &#123; if（i &lt;= 1）&#123; 1 &#125;else&#123; i * milit(i-1) &#125; &#125; 5.总结 123def 方法名称（[参数名：参数类型]，.....）[:返回值类型 = ] &#123; 方法体&#125; []内的可有可不有 定义函数 1.函数的定义：val/var 函数名称=(函数的参数列表)=&gt;函数体 1234567891011//定义函数，有一个Int类型的参数val fun01 = (a: Int) =&gt; a + 10 //定义函数，有两个Int类型的参数val fun02 = (a: Int, b: Int) =&gt; &#123; if (a &gt; 10 || b &gt; 10)&#123; a + b &#125;else&#123; a - b &#125;&#125; 2.在函数使用过程中，还有另一个常见的操作，就是将函数作为另一个函数的参数来使用，即传名调用（区别于传值调用） 12345678910111213var money =100def calMoneyLeft():Int = &#123; println(\"函数参数调用中\") money&#125; def cost(x: =&gt; Int)&#123; money -=50 println(\"函数参数调用之前\") println(x)&#125; cost(calMoneyLeft()) 运行结果： 123函数参数调用之前函数参数调用中50 方法和函数 1.区别： 方法和函数的定义语法不同 方法一般定义在类，特质，或者object中 方法可以共享所在类，特质，或者object中的属性 可以调用函数，也可以存放到一个变量中，作为参数传递给其他的方法或者函数，也可以作为返回值 2.联系： 1）可以把函数作为参数传递给一个方法： 12345678scala&gt; def method(f:(Int,Int)=&gt;Int) = f(5,1)m: (f: (Int, Int) =&gt; Int)Intscala&gt; val function = (x:Int,y:Int)=&gt;x-yf: (Int, Int) =&gt; Int = $$Lambda$1125/809735384@615784a8scala&gt; method(function)res11: Int = 4 2）方法可以转换成函数 a. 把一个方法作为参数传递给其他的方法或者函数 b. 使用(方法名 _ )把一个方法显式的转换一个函数,注意方法名后面有个空格 12345678910111213141516scala&gt; def m(f:(Int,Int)=&gt;Int) = f(5,1)m: (f: (Int, Int) =&gt; Int)Intscala&gt; def m2(x:Int,y:Int) = x-ym2: (x: Int, y: Int)Int//方法可以转换成函数使用scala&gt; m(m2)res13: Int = 4//把一个方法显式的转换一个函数scala&gt; m2 _res15: (Int, Int) =&gt; Int = $$Lambda$1133/550115845@20d9f6b8scala&gt; m(m2 _)res14: Int = 4","categories":[{"name":"Scala编程语言","slug":"Scala编程语言","permalink":"https://zzuuriel.github.io/categories/Scala编程语言/"}],"tags":[{"name":"Scala","slug":"Scala","permalink":"https://zzuuriel.github.io/tags/Scala/"}]},{"title":" Hadoop架构详解 ","date":"2019-01-08T07:18:25.000Z","path":"大数据技术/3.Hadoop架构详解/","text":"Hadoop架构详解 Hadoop架构概述 Hadoop是一个能够对大量数据进行分布式处理的软件框架，以一种可靠、高效、可拓展的方式进行数据处理。Hadoop在设计之初就考虑了各种问题，包括，容错、处理大数据集、数据本地化、不同硬件和软件平台间的可移植性等等。 下面是Hadoop架构图： Hadoop具有主从（master-slave）拓扑结构。在这样的拓扑结构中，有一个主节点和多个从节点。主节点负责为各种从节点分配任务并管理资源，从节点完成实际的计算工作。从节点存储实际数据而主节点仅持有元数据，也就是存储关于数据的数据。 Hadoop包含3个主要的层，它们是： HDFS（Hadoop Distributed File System，Hadoop分布式文件系统） MapReduce Yarn HDFS HDFS表示Hadooop分布式文件系统，它为Hadoop提供数据存储能力。HDFS将数据切分成更小的单元（称为块），并且以分布式的方式进行存储。它有两个进程运行，一个在主节点上叫NameNode，另外一个在从节点上叫DataNode。 NameNode和DataNode HDFS具有主从架构，以管理者-工作者的模式运行。NameNode进程运行在master服务器上，它负责命名空间的管理以及控制客户端的文件访问。Namenode管理文件系统命名空间的修改，一些类似打开、关闭和重命名文件和目录的行为，NameNode也跟踪数据块到DataNode的映射。 DataNode进程运行在slave节点，它负责存储实际的业务数据。在系统内部，文件被分割成一定数量的数据块，并且存储到一组slave节点机器中。Datanode是文件系统的工作者，它们存储并提供定位块的服务（被用户或namenode调用时），并且定时的向namenode发送它们存储的块的列表。DataNode完成文件系统客户端请求的读写服务，DataNode也响应NameNode的需求完成数据块的创建、删除和复制工作。 HDFS的数据块 数据块是计算机上的最小存储单元，在Hadoop中，默认的数据块大小是128MB或者256MB。关于Hadoop的误解之一是认为较小的块(小于块大小)在文件系统中仍然会占用整个块。事实并非如此。较小的块只占用它们所需要大小的磁盘空间。 但这并不意味着大量小文件可以有效地利用 HDFS。无论块大小是多大，其元数据在 NameNode 中所占的内存完全相同。其结果是，数目众多的 HDFS 小文件(小于块大小)会占用大量的 NameNode 内存，从而给 HDFS 的可扩展性和性能带来负面影响。 在现实系统中，几乎不可能避免出现较小的HDFS块。比较大的可能性是某个给定的 HDFS文件会占用一些完整的块和一些较小的块。这会成为一个问题吗？考虑到大多数 HDFS文件会相当庞大，整个系统中此类较小块的数量将相对较少，因此通常没有问题。 复制管理 为了提供容错性，HDFS使用了复制技术。它建立几份数据块拷贝，并在不同的DataNode间进行存储。复制因子决定了数据块的拷贝数量。复制因子的默认值是3，当然你也可以配置成其他值。 假设我们有一个1GB的文件，由于复制因子是3，那么它实际需要3GB的存储空间。为了维持复制因子，NameNode会从每个DataNode中收集数据块报告。只要一个数据块的数量低于或者高于复制因子，NameNode就会添加或者删除相应的拷贝。 机架感知 HDFS 数据复制的最重要特性叫做机架感知，机架感知算法提供了低延迟和容错性。运行在计算机集群上的大型 HDFS 实例通常跨越许多个机架。通常情况下，相同机架上机器之间的网络带宽(以及与之相关联的网 络性能)远大于不同机架上机器之间的网络带宽。 NameNode 通过 Hadoop 机架感知进程确定各个 DataNode 所属的机架 ID。一种简单的策略是将各个副本分别放置于不同的机架上。这种策略能够在整个机架失效时防止数据丢失，且将副本均匀地分布到集群中。它也允许在读取数据时使用源自多个机架的带宽。但由于在这种情况下，写操作必须将块传输到多个机架上，因此写入性能会受影响。 机架感知策略的一个优化方案是让占用的机架数少于副本数，以减少跨机架写入流量 (进而提高写入性能)。例如，当复制因子为3时，将两个副本置于同一个机架上，并将第三个副本放在另一个不同的机架上。如果可能，在同一个机架上不会存储超过2个数据块。 MapReduce 同HDFS一样，Hadoop MapReduce也采用了Master/Slave(M/S)架构，具体如下图所示： 它主要由以下几个组件组成：Client、JobTracker、TaskTracker和Task。下面分别对这几个组件进行介绍。 Client 用户编写的MapReduce程序通过Client提交到JobTracker端；同时，用户可通过Client提供的一些接口查看作业运行状态。在Hadoop内部用作业（Job）表示MapReduce程序。一个MapReduce程序可对应若干个作业，而每个作业会被分解成若干个Map/Reduce(Task)。 JobTracker JobTracker主要负责资源监控和作业调度。JobTraker监控所有TaskTracker与作业的健康状况，一旦发现失败情况后，其会将相应的任务转移到其他节点；同时，JobTracker会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在Hadoop中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。 TaskTracker TaskTracker会周期性地通过Heartbeat将本节点上的资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker发送的过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker使用&quot;slot&quot;等量划分本节点上的资源量。&quot;slot&quot;代表计算资源（CPU、内存等）。一个Task获取到一个slot后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用。slot分为Map slot和Reduce slot两种，分别供Map Task和Reduce Task使用。TaskTracker通过slot数目（可配置参数）限定Task的并发度。 Task Task分为Map Task和Reduce Task两种，均由TaskTracker启动。HDFS以固定大小的block为基本单位存储数据，而对于MapReduce而言，其处理单位是split。split是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。split的多少决定了Map Task的数目，因为每个split会交由一个Map Task处理。split与block的对应关系如下图所示： Map Task先将对应的split迭代解析成一个个key/value对，依次调用用户自定义的map()函数进行处理，最终将临时结果存放到本地磁盘上，其中临时数据被分成若干个partition,每个partition将被一个Reduce Task处理。Map Task执行过程如下图所示： Reduce Task执行过程分为三个阶段： 1.从远程节点读取Map Task中间结果（称为&quot;Shuffle阶段&quot;）； 2.按照key对key/value进行排序(称为&quot;Sort阶段&quot;)； 3.依次读取&lt;key,value list&gt;,调用用户自定义的reduce()函数处理，并将最终结果存到HDFS上（称为&quot;Reduce阶段&quot;）。 Reduce Task执行过程如下图所示： YARN YARN（Yet Another Resource Negotiator）是Hadoop的资源管理层。YARN背后的基本原则是将资源管理和任务调度/监控功能分离成独立进程。YARN通过两类长期运行的守护进程提供核心服务：管理集群上资源使用的资源管理器Resource manager（RM），能够启动和监控容器（container）的节点管理器Node Manager（NM），container用于执行特定的应用程序进程。ResourceManager为系统中发生竞争的应用程序间仲裁资源，NodeManager的任务是监控容器对资源的占用情况，并且向ResourceManager进行汇报，这些资源指的是：CPU，内存，磁盘，网络等等。与HDFS一样，YARN也是master/slave结构，一个RM管理着多个NM。 YARN应用运行机制 YARN运行机制： 1.client首先联系Resource Manager，要求它运行一个Applcation Master（AM），Applcation Master负责和Resource Manager协调资源，并且和NodeManager一起执行作业，并且监控作业。； 2.Resource Manager找到一个能够在container中启动Applcation Master的Node Manager； 3.Applcation Master运行起来之后做什么依赖于应用本身，有可能在所处的container中简单地执行一个计算并将结果返回给客户端，或是像Resource Manager请求更多的container，用于运行一个分布式计算，例如MapReduce。 ResourceManager有两个重要的组件：Scheduler(调度器)和ApplicationManager（应用程序管理器）。 Scheduler Scheduler负责为各种应用程序指派资源。它是一个纯调度器，因为它并不对应用程序状态进行跟踪。对于那些发生软硬件故障的任务，它也不会再次为其调度资源。调度器根据应用程序的需求来分配资源。 ApplicationManager 应用程序管理器有如下功能： 1)接受作业提交 2)为执行中的ApplicationMaster协调第一个容器。容器由CPU，内存，磁盘及网络等元素构成。 3)失败时重启ApplicationMaster容器 通过YARN联盟（YARN Federation）特性，我们可以在几千个节点上扩展YARN。这个特性可以将多个YARN集群附属到一个更大型的集群，这样我们通过这样独立的集群拼凑起来完成更大型的作业。 YARN资源请求 YARN有一个灵活的资源请求模型，请求多个container时可以指定每个container的资源，还可以治container的本地限制要求。 本地化对于确保分布式数据处理算法高效使用集群带宽非常重要，本地限制有时也存在无法被满足的情况，例如一个Node Manager节点已经运行了别的container而无法再启动新的container，此时若有应用请求该节点，则YARN会尝试在同一机架的其他节点上启动一个container，如果还不行则会尝试集群中的任意节点。 YARN应用可以在运行中的任意时刻提出资源申请，可以在最开始提出所有的请求，或者为满足不断变化的应用需要，采取更为动态的方式在需要更多资源时提出请求。Spark采用的就是第一种方式，MapReduce采用的就是第二种方式，在最开始时申请map任务的container，后期再申请reduce任务的container，如果任何任务出现失败，会另外申请容器重新运行失败的任务。 YARN特性 YARN具有如下特性： 多租户模式（Multi-tenancy） YARN允许多种（开源或商业）访问引擎工作在同一Hadoop数据集上面。这些访问引擎可以以分批、实时或者迭代等方式处理数据。 充分利用集群 通过动态分配资源，YARN能够充分利用集群。相对于Hadoop以前版本的静态MapReduce规则，其集群利用度更高。 可缩放 任何处理功能强大的数据中心都要具备可扩展。YARN的ResourceManager关注调度并且应当每个扩展的集群，从容处理PB级的数据。 兼容性 为Hadoop1.x开发的MapReduce程序仍旧可以运行在YARN上面。而且不会影响已有作业的执行。 总结 Hadoop具有自我恢复能力，冗余的存储使其具备容错性，从而更加健壮。MapReduce遵从数据本地化处理原则，使计算向数据靠近，从而降低网络带宽。Hadoop的整个架构都是经济，可缩放而且高效的。","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://zzuuriel.github.io/tags/Hadoop/"}]},{"title":" hadoop官网翻译:Setting up a Single Node Cluster","date":"2018-12-11T04:00:00.000Z","path":"大数据技术/1.single-noge-cluster/","text":"hadoop官网翻译:Setting up a Single Node Cluster. Hadoop: Setting up a Single Node Cluster. 设置单节点集群。 ▪ Purpose ▪ Prerequisites ▪ Supported Platforms ▪ Required Software ▪ Installing Software ▪ Download ▪ Prepare to Start the Hadoop Cluster ▪ Standalone Operation ▪ Pseudo-Distributed Operation ▪ Configuration ▪ Setup passphraseless ssh ▪ Execution ▪ YARN on a Single Node ▪ Fully-Distributed Operation ▪ 目的 先决条件 ▪ 支持平台 ▪ 所需软件 ▪ 安装软件 ▪ 下载 ▪ 准备开始安装Hadoop集群 ▪ 独立操作 ▪ 伪分布式操作 ▪ 配置 ▪ 设置无密码ssh ▪ 执行 ▪ YARN在单节点上运行 ▪ 全分布式操作 Purpose This document describes how to set up and configure a single-node Hadoop installation so that you can quickly perform simple operations using Hadoop MapReduce and the Hadoop Distributed File System (HDFS) 本文档描述了如何去建立并且配置一个单节点hadoop的安装，这样你就可以使用MR和HDFS快速执行简单的操作 Prerequisites Supported Platforms GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes. Windows is also a supported platform but the followings steps are for Linux only. To set up Hadoop on Windows, see wiki page． ▪ 支持GNU/Linux作为开发和生产平台。Hadoop已经在具有2000个节点的GNU/Linux集群上进行了演示。 ▪ Windows也是一个受支持的平台，但是下面的步骤只适用于Linux。要在Windows上配置Hadoop，请参阅wiki页面。 Required Software Required software for Linux include: 1.Java™ must be installed. Recommended Java versions are described at HadoopJavaVersions. 2.ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used. Additionally, it is recommmended that pdsh also be installed for better ssh resource management. linux所需的软件 1.必须安装java，在HadoopJavaVersions中查看适合的java版本 2.如果要使用可选的启动和停止脚本，则必须安装 ssh 并运行 sshd 以使用管理远程 hadoop 守护进程的 hadoop 脚本。 此外，还建议安装 pdsh，以便更好地管理 ssh 资源。 Installing Software If your cluster doesn’t have the requisite software you will need to install it. For example on Ubuntu Linux: 如果你的集群没有安装所需的软件，你需要安装它 例如在 Ubuntu Linux: 12 $ sudo apt-get install ssh $ sudo apt-get install pdsh Download To get a Hadoop distribution, download a recent stable release from one of the Apache Download Mirrors. 要获得Hadoop发行版，请从Apache下载镜像下载一个最新的稳定版本 Prepare to Start the Hadoop Cluster Unpack the downloaded Hadoop distribution. In the distribution, edit the file etc/hadoop/hadoop-env.sh to define some parameters as follows: 12 # set to the root of your Java installation export JAVA_HOME=/usr/java/latest Try the following command: 1 $ bin/hadoop This will display the usage documentation for the hadoop script. Now you are ready to start your Hadoop cluster in one of the three supported modes: Local (Standalone) Mode Pseudo-Distributed Mode Fully-Distributed Mode 解压缩下载的Hadoop发行版。在发行版中，编辑文件etc/hadoop/hadoop-env.sh以定义以下一些参数： 12 # set to the root of your Java installation export JAVA_HOME=/usr/java/latest 尝试以下命令： 1 $ bin/hadoop 这个会显示hadoop脚本的使用文档 现在，您已经准备好在三种支持的模式之一中启动Hadoop集群 ▪ 单机模式：默认情况下运行在一个单独机器上的独立Java进程，主要用于调试环境 ▪ 伪分布模式：在单个机器上模拟成分布式多节点环境，每一个Hadoop守护进程都作为一个独立的Java进程运行 ▪ 完全分布式模式：真实的生产环境，搭建在完全分布式的集群环境 Standalone Operation By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging. The following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory. 1234 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos; $ cat output/* 默认情况下，hadoop 被配置为以非分布式模式(作为单个 java 进程)运行。 这对调试很有用。 下面的示例复制未打包的conf目录作为输入，然后查找并显示给定正则表达式的每个匹配。输出被写入给定的输出目录。 1234 $ mkdir input $ cp etc/hadoop/*.xml input $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos; $ cat output/* Pseudo-Distributed Operation Hadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process. Hadoop还可以在一个伪分布式模式的单个节点上运行，其中每个Hadoop守护进程在单独的Java进程中运行。 Configuration Use the following: 使用以下方法： etc/hadoop/core-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; etc/hadoop/hdfs-site.xml: 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 常用配置项说明： ▪ fs.defaultFS这是默认的HDFS路径。当有多个HDFS集群同时工作时，用户在这里指定默认HDFS集群，该值来自于hdfs-site.xml中的配置。 ▪ fs.default.name这是一个描述集群中NameNode结点的URI(包括协议、主机名称、端口号)，集群里面的每一台机器都需要知道NameNode的地址。DataNode结点会先在NameNode上注册，这样它们的数据才可以被使用。独立的客户端程序通过这个URI跟DataNode交互，以取得文件的块列表。 ▪ hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它。如果hdfs-site.xml中不配置namenode和datanode的存放位置，默认就放在/tmp/hadoop-${user.name}这个路径中。 更多说明请参考core-default.xml ，包含配置文件所有配置项的说明和默认值。 Setup passphraseless ssh Now check that you can ssh to the localhost without a passphrase: 现在检查是否可以在没有密码的情况下ssh到本地主机： 1 $ ssh localhost If you cannot ssh to localhost without a passphrase, execute the following commands: 如果没有密码无法ssh到localhost，请执行以下命令： 123 $ ssh-keygen -t rsa -P &apos;&apos; -f ~/.ssh/id_rsa $ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys $ chmod 0600 ~/.ssh/authorized_keys Execution The following instructions are to run a MapReduce job locally. If you want to execute a job on YARN, see YARN on Single Node. 以下说明将在本地运行MapReduce作业。如果您想在YARN上执行作业，请参阅YARN on Single Node。 在使用hadoop前，必须格式化一个全新的HDFS安装，通过创建存储目录和NameNode持久化数据结构的初始版本，格式化过程创建了一个空的文件系统。由于NameNode管理文件系统的元数据，而DataNode可以动态的加入或离开集群，因此这个格式化过程并不涉及DataNode。同理，用户也无需关注文件系统的规模。集群中DataNode的数量决定着文件系统的规模。DataNode可以在文件系统格式化之后的很长一段时间内按需增加。 Format the filesystem: 格式化文件系统: 1 $ bin/hdfs namenode -format Start NameNode daemon and DataNode daemon: 启动NameNode和DataNode的守护进程: 1 $ sbin/start-dfs.sh The hadoop daemon log output is written to the $HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs). hadoop守护进程日志输出被写入$HADOOP_LOG_DIR目录（默认为$HADOOP_HOME/logs）。 Browse the web interface for the NameNode; by default it is available at: ▪ NameNode - http://localhost:9870/ 浏览用于NameNode的网络接口；默认情况下，可以在：NameNode - http://localhost:9870/ Make the HDFS directories required to execute MapReduce jobs: 创建执行MapReduce作业所需的HDFS目录： 12 $ bin/hdfs dfs -mkdir /user $ bin/hdfs dfs -mkdir /user/&lt;username&gt; Copy the input files into the distributed filesystem: 将输入文件复制到分布式文件系统中： 12 $ bin/hdfs dfs -mkdir input $ bin/hdfs dfs -put etc/hadoop/*.xml input Run some of the examples provided: 运行所提供的一些示例： 1 $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output &apos;dfs[a-z.]+&apos; Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them: 检查输出文件：将输出文件从分布式文件系统复制到本地文件系统，并检查它们： 12 $ bin/hdfs dfs -get output output $ cat output/* or View the output files on the distributed filesystem: 或者直接在分布式文件系统中查看： 1 $ bin/hdfs dfs -cat output/* When you’re done, stop the daemons with: 完成后，使用以下命令停止守护进程： 1 $ sbin/stop-dfs.sh YARN on a Single Node You can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition. The following instructions assume that 1. ~ 4. steps of the above instructions are already executed. 您可以通过设置一些参数并运行ResourceManager守护进程和NodeManager守护进程，在伪分布式模式下在YARN上运行MapReduce作业。 下面的指令假设已经执行上述指令1. ~ 4. 的步骤。 Configure parameters as follows: etc/hadoop/mapred-site.xml: 1234567 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;更多说明请参考mapred-default.xml，包含配置文件所有配置项的说明和默认值 etc/hadoop/yarn-site.xml: 12345678&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;注：yarn.nodemanager.aux-services通过该配置，用户可以自定义一些服务更多说明请参考yarn-default.xml，包含配置文件所有配置项的说明和默认值 Start ResourceManager daemon and NodeManager daemon: 启动ResourceManager守护进程和NodeManager守护进程： 1 $ sbin/start-yarn.sh Browse the web interface for the ResourceManager; by default it is available at: 浏览ResourceManager的网页界面；默认情况下，可以在： ResourceManager - http://localhost:8088/ Run a MapReduce job. When you’re done, stop the daemons with: 完成后，使用以下命令停止守护进程： 1 $ sbin/stop-yarn.sh Fully-Distributed Operation For information on setting up fully-distributed, non-trivial clusters see Cluster Setup. 有关全分布集群设置的信息，请参见Cluster Setup.","categories":[{"name":"大数据技术","slug":"大数据技术","permalink":"https://zzuuriel.github.io/categories/大数据技术/"}],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://zzuuriel.github.io/tags/Hadoop/"}]},{"title":"Linux source命令、sh和./区别","date":"2018-08-15T12:12:36.000Z","path":"Linux基础/Linux source命令、sh和.区别/","text":"Linux source命令、sh和./区别 转载自： https://www.cnblogs.com/pkufork/p/linux_source.html Linux source命令 通常用法：source filepath 或 . filepath 功能：使当前shell读入路径为filepath的shell文件并依次执行文件中的所有语句，通常用于重新执行刚修改的初始化文件，使之立即生效，而不必注销并重新登录。例如，当我们修改了/etc/profile文件，并想让它立刻生效，而不用重新登录，就可以使用source命令，如source /etc/profile。 source命令(从 C Shell 而来)是bash shell的内置命令；点命令(.)，就是个点符号(从Bourne Shell而来)是source的另一名称。这从用法中也能看出来。 source filepath 与 sh filepath 、./filepath的区别 当shell脚本具有可执行权限时，用sh filepath与./filepath是没有区别的。./filepath是因为当前目录没有在PATH中，所有&quot;.&quot;是用来表示当前目录的。 sh filepath 会重新建立一个子shell，在子shell中执行脚本里面的语句，该子shell继承父shell的环境变量，但子shell是新建的，其改变的变量不会被带回父shell，除非使用export。 source filename其实只是简单地读取脚本里面的语句依次在当前shell里面执行，没有建立新的子shell。那么脚本里面所有新建、改变变量的语句都会保存在当前shell里面。 举例说明： 新建一个test.sh脚本，内容为:A=1； 修改其可执行权限：chmod +x test.sh； 运行sh test.sh后，echo $A，显示为空，因为A=1并未传回给当前shell； 运行./test.sh后，也是一样的效果； 运行source test.sh 或者 . test.sh，然后echo $A，则会显示1，说明A=1的变量在当前shell中；","categories":[{"name":"Linux基础","slug":"Linux基础","permalink":"https://zzuuriel.github.io/categories/Linux基础/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://zzuuriel.github.io/tags/Linux/"}]}]