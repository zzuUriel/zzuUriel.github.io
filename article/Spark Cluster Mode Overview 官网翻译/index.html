<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>Spark Cluster Mode Overview 官网翻译 | 暗也橙子Blog</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark Cluster Mode Overview 官网翻译</h1><a id="logo" href="/.">暗也橙子Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Spark Cluster Mode Overview 官网翻译</h1><div class="post-meta"><a href="/article/Spark Cluster Mode Overview 官网翻译/#comments" class="comment-count"></a><p><span class="date">Jun 10, 2019</span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="spark-cluster-mode-overview-官网翻译">Spark Cluster Mode Overview 官网翻译</h3>
<hr>
<p>文档地址：<a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/cluster-overview.html</a></p>
<h4 id="cluster-mode-overview集群模式概述">Cluster Mode Overview(集群模式概述)</h4>
<p>This document gives a short overview of how Spark runs on clusters, to make it easier to understand the components involved. Read through the <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">application submission guide</a> to learn about launching applications on a cluster.</p>
<p>这篇文档介绍了Spark在集群上运行的大概情况，让我们更容易理解其各个组件是如何交互的。我们可以通读 <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">application submission guide</a> 来学习如何在集群上部署应用程序。</p>
<h4 id="components组件">Components(组件)</h4>
<p>Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).</p>
<p>Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.</p>
<p>Spark应用程序在一个集群上运行着一组独立的进程，包括一个driver和多个executors，在你应用程序的main函数里通过SparkContext对象来协调组织，我们也把Spark applications称之为driver program。</p>
<p>具体来说，集群模式下，SparkContext能够连接不同类型的cluster managers，比如说Spark自己的standalone cluster manager， Mesos或者YARN，而这些cluster managers所扮演的角色是在各个应用程序application之间分配资源。一旦Spark连接上这些cluster managers，Spark就获得了分布在集群各个节点上的executors，这些executors其实是一系列的进程，这些进程执行我们的应用程序application中的计算并存储相关的数据。接着，SparkContext将我们的应用程序application代码发送给executors，这些应用程序application代码是由JAR或者Python文件所定义并且传给SparkContext。最后，SparkContext把tasks发送给executors去执行。<br>
<img src="/img/spark/cluster-overview.png" alt=" Cluster Mode架构图"><br>
There are several useful things to note about this architecture:</p>
<ol>
<li>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.</li>
<li>Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).</li>
<li>The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see <a href="http://spark.apache.org/docs/latest/configuration.html#networking" target="_blank" rel="noopener">spark.driver.port in the network config section</a>). As such, the driver program must be network addressable from the worker nodes.</li>
<li>Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.</li>
</ol>
<p>关于这个架构，有以下几个有用的地方需要注意：</p>
<ol>
<li>每个应用程序application都有属于它自己本身的executor进程，这些进程横跨这个application的整个生命周期并且以多线程的方式来执行内部的多个tasks。这有个好处，每个application之间无论是在调度层面scheduling side还是在执行层面<br>
executor side都是相互隔离的。也就是说从调度层面来看，每个driver调度属于它自身的tasks，从执行层面上来看，属于不同applications的tasks运行在不同的JVM上。然而，这也意味着不同的Spark applications（也可以说是SparkContext的实例）是不能共享各自所属的数据，除非，你把数据写到外部存储系统，比如说<a href="https://www.alluxio.io/" target="_blank" rel="noopener">Alluxio</a>。</li>
<li>Spark不关心底层的cluster manager是哪种类型。只要Spark可以获取得到executor进程，并且这些executor进程能够互相通信，那么对于同样支持其他applications的cluster manager来说，比如Mesos/YARN，都是能去运行Spark程序的。</li>
<li>在driver program的整个生命周期中，它一直在监听并且接收来自属于它本身的executors的连接。可以查看 <a href="http://spark.apache.org/docs/latest/configuration.html#networking" target="_blank" rel="noopener">spark.driver.port in the network config section</a>。因此，driver program必须跟各个worker nodes节点网络互通。</li>
<li>由于driver是在集群上调度各个任务的，按理来说，它应该运行在靠近worker nodes的节点上，最好是在同一个局域网里。如果你想发送请求给远端的集群，最佳的方式是你给driver开一个RPC，并且让driver在靠近worker nodes的节点上提交作业，而不是在远离worker nodes的节点上运行driver。</li>
</ol>
<h4 id="cluster-manager-typescluster-manager类型">Cluster Manager Types(Cluster Manager类型)</h4>
<p>The system currently supports several cluster managers:</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Standalone</a> – a simple cluster manager included with Spark that makes it easy to set up a cluster.</li>
<li><a href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="noopener">Apache Mesos</a> – a general cluster manager that can also run Hadoop MapReduce and service applications.</li>
<li><a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">Hadoop YARN</a> – the resource manager in Hadoop 2.</li>
<li><a href="http://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">Kubernetes</a> – an open-source system for automating deployment, scaling, and management of containerized applications.</li>
</ul>
<p>A third-party project (not supported by the Spark project) exists to add support for <a href="https://github.com/hashicorp/nomad-spark" target="_blank" rel="noopener">Nomad</a> as a cluster manager.</p>
<p>如今Spark生态目前支持以下几个cluster managers:</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/spark-standalone.html" target="_blank" rel="noopener">Standalone</a> – 一个简单的cluster manager，它内置了Spark，使得我们能够快速的启动一个集群。</li>
<li><a href="http://spark.apache.org/docs/latest/running-on-mesos.html" target="_blank" rel="noopener">Apache Mesos</a> – 一个通用的cluster manager，它能够运行Hadoop MapReduce以及service applications。</li>
<li><a href="http://spark.apache.org/docs/latest/running-on-yarn.html" target="_blank" rel="noopener">Hadoop YARN</a> – Hadoop 2的resource manager。</li>
<li><a href="http://spark.apache.org/docs/latest/running-on-kubernetes.html" target="_blank" rel="noopener">Kubernetes</a> – 一个开源的项目，用于自动部署，扩容以及容器内部应用的管理。</li>
</ul>
<h4 id="submitting-applications提交applications">Submitting Applications（提交Applications）</h4>
<p>Applications can be submitted to a cluster of any type using the spark-submit script. The <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">application submission guide</a> describes how to do this.</p>
<p>通过spark-submit脚本，我们可以把应用Applications提交到任何类型的集群上。这篇 <a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="noopener">application submission guide</a> 文章描述了具体的实现方式。</p>
<h4 id="monitoring监控">Monitoring（监控）</h4>
<p>Each driver program has a web UI, typically on port 4040, that displays information about running tasks, executors, and storage usage. Simply go to http://<driver-node>:4040 in a web browser to access this UI. The <a href="http://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">monitoring guide</a> also describes other monitoring options.</driver-node></p>
<p>每个driver program都有它自己的一套web UI界面，通常运行在4040端口，它详细展示了当前运行的tasks，executors，和存储使用情况等相关信息。我们可以通过浏览器访问http://<driver-node>:4040来浏览这个UI界面。这篇 <a href="http://spark.apache.org/docs/latest/monitoring.html" target="_blank" rel="noopener">monitoring guide</a> 文章详细介绍了其他监控选项。</driver-node></p>
<h4 id="job-scheduling作业调度">Job Scheduling(作业调度)</h4>
<p>Spark gives control over resource allocation both across applications (at the level of the cluster manager) and within applications (if multiple computations are happening on the same SparkContext). The <a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="noopener">job scheduling overview</a> describes this in more detail.</p>
<p>Spark不仅仅通过cluster manager在各个应用applications之间来控制资源的分配，而且在应用applications内部，当同一个SparkContext里面同时有多个计算同时运行的时候，Spark同样会去控制资源如何分配。详情请看 <a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="noopener">job scheduling overview</a>。</p>
<h4 id="glossary术语">Glossary(术语)</h4>
<p>The following table summarizes terms you’ll see used to refer to cluster concepts:</p>
<table>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application</td>
<td>User program built on Spark. Consists of a driver program and executors on the cluster.</td>
</tr>
<tr>
<td>Application jar</td>
<td>A jar containing the user’s Spark application. In some cases users will want to create an “uber jar” containing their application along with its dependencies. The user’s jar should never include Hadoop or Spark libraries, however, these will be added at runtime.</td>
</tr>
<tr>
<td>Driver program</td>
<td>The process running the main() function of the application and creating the SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)</td>
</tr>
<tr>
<td>Deploy mode</td>
<td>Distinguishes where the driver process runs. In “cluster” mode, the framework launches the driver inside of the cluster. In “client” mode, the submitter launches the driver outside of the cluster.</td>
</tr>
<tr>
<td>Worker node</td>
<td>Any node that can run application code in the cluster</td>
</tr>
<tr>
<td>Executor</td>
<td>A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.</td>
</tr>
<tr>
<td>Task</td>
<td>A unit of work that will be sent to one executor</td>
</tr>
<tr>
<td>Job</td>
<td>A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs.</td>
</tr>
<tr>
<td>Stage</td>
<td>Each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce); you’ll see this term used in the driver’s logs.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application</td>
<td>通过Spark构建的用户应用程序。包括集群中的一个driver program以及多个executors。</td>
</tr>
<tr>
<td>Application jar</td>
<td>一个jar包包括用户的Spark应用程序。在某些情况之下，一些用户想要创建一个&quot;uber jar&quot;，这个&quot;uber jar&quot;包含了自身的应用程序以及自身所有的依赖。用户的jar包里面不能包括Hadoop or Spark的库，但是，这个功能很快就会被添加中</td>
</tr>
<tr>
<td>Driver program</td>
<td>Driver program是一个进程，它里面运行着应用程序application里面的main()函数，并在main函数里面创建SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>一个外部服务，获取standalone manager，Mesos,，YARN等集群上面的资源</td>
</tr>
<tr>
<td>Deploy mode</td>
<td>决定了driver进程运行在哪里。如果使用cluster模式，Spark会把driver运行在集群里面。如果使用client模式，submitter会将driver部署到集群外面。</td>
</tr>
<tr>
<td>Worker node</td>
<td>一个集群里面能够运行应用程序代码的工作节点</td>
</tr>
<tr>
<td>Executor</td>
<td>一个worker node节点上面运行应用程序application的进程，它运行tasks，并且把数据存到内存或者持久化到存储设备。每个应用程序application有它自身相应的executors。</td>
</tr>
<tr>
<td>Task</td>
<td>一个发送给executor作业的工作单元。</td>
</tr>
<tr>
<td>Job</td>
<td>当一个Spark action（如save, collect）被触发，一个包含很多个tasks的并行计算的job将会生成。你可以在driver’s logs看到这个术语。</td>
</tr>
<tr>
<td>Stage</td>
<td>每个job将会被分成更小的tasks的集合，我们称之为stages。每个stage之间相互依赖，类似于MapReduce的map和reduce的stages。你可以在driver’s logs看到这个术语。</td>
</tr>
</tbody>
</table>
</div><div class="post-copyright"><blockquote><p>原文作者: 暗也橙子</p><p>原文链接: <a href="https://zzuuriel.github.io/article/Spark Cluster Mode Overview 官网翻译/">https://zzuuriel.github.io/article/Spark Cluster Mode Overview 官网翻译/</a></p><p>版权声明: 转载请注明出处(必须保留作者署名及链接)</p></blockquote></div><div class="tags"><a href="/tags/spark/">spark</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/article/4.spark RDD定义与特性/" class="pre"> Spark RDD定义与特性 </a><a href="/article/Spark Core之读写数据/" class="next">Spark Core之读写数据</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-cluster-mode-overview-官网翻译"><span class="toc-text">Spark Cluster Mode Overview &#x5B98;&#x7F51;&#x7FFB;&#x8BD1;</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#cluster-mode-overview集群模式概述"><span class="toc-text">Cluster Mode Overview(&#x96C6;&#x7FA4;&#x6A21;&#x5F0F;&#x6982;&#x8FF0;)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#components组件"><span class="toc-text">Components(&#x7EC4;&#x4EF6;)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cluster-manager-typescluster-manager类型"><span class="toc-text">Cluster Manager Types(Cluster Manager&#x7C7B;&#x578B;)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#submitting-applications提交applications"><span class="toc-text">Submitting Applications&#xFF08;&#x63D0;&#x4EA4;Applications&#xFF09;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#monitoring监控"><span class="toc-text">Monitoring&#xFF08;&#x76D1;&#x63A7;&#xFF09;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#job-scheduling作业调度"><span class="toc-text">Job Scheduling(&#x4F5C;&#x4E1A;&#x8C03;&#x5EA6;)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#glossary术语"><span class="toc-text">Glossary(&#x672F;&#x8BED;)</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/article/修改Flume源码使taildir source支持递归（可配置）/">修改Flume源码使taildir source支持递归（可配置）</a></li><li class="post-list-item"><a class="post-list-link" href="/article/解决Spark on YARN时jar包乱飞的问题/">解决Spark on YARN时jar包乱飞的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/article/Flink之Watermark理解/">Flink之Watermark理解</a></li><li class="post-list-item"><a class="post-list-link" href="/article/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/"> Spark-2.4.5源码编译</a></li><li class="post-list-item"><a class="post-list-link" href="/article/Flink自定义触发器/">Flink自定义触发器</a></li><li class="post-list-item"><a class="post-list-link" href="/article/60.深入理解Spark算子aggregate/">深入理解Spark算子aggregate</a></li><li class="post-list-item"><a class="post-list-link" href="/article/60.Kudu常用Api(java)/">Kudu常用Api(java) </a></li><li class="post-list-item"><a class="post-list-link" href="/article/60.CentOS7安装单机版Kudu/">CentOS7安装单机版Kudu </a></li><li class="post-list-item"><a class="post-list-link" href="/article/HBase架构和读写流程/">HBase架构和读写流程</a></li><li class="post-list-item"><a class="post-list-link" href="/article/HBase的Rowkey设计/">HBase的Rowkey设计</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/kudu/" style="font-size: 15px;">kudu</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/flume/" style="font-size: 15px;">flume</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">23</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">2</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"> <div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">暗也橙子.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/zzuUriel/zzuUriel.github.io"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>