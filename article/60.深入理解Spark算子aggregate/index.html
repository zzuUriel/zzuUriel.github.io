<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>深入理解Spark算子aggregate | 暗也橙子Blog</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">深入理解Spark算子aggregate</h1><a id="logo" href="/.">暗也橙子Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">深入理解Spark算子aggregate</h1><div class="post-meta"><a href="/article/60.深入理解Spark算子aggregate/#comments" class="comment-count"></a><p><span class="date">Dec 15, 2019</span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="深入理解spark算子aggregate">深入理解Spark算子aggregate</h3>
<hr>
<h4 id="源码解读">源码解读</h4>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">* Aggregate the elements of each partition, and then the results for all the partitions, using</span></span><br><span class="line"><span class="comment">* given combine functions and a neutral "zero value". This function can return a different result</span></span><br><span class="line"><span class="comment">* type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U</span></span><br><span class="line"><span class="comment">* and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are</span></span><br><span class="line"><span class="comment">* allowed to modify and return their first argument instead of creating a new U to avoid memory</span></span><br><span class="line"><span class="comment">* allocation.</span></span><br><span class="line"><span class="comment">*  * <span class="doctag">@param </span>zeroValue the initial value for the accumulated result of each partition for the</span></span><br><span class="line"><span class="comment">*                  `seqOp` operator, and also the initial value for the combine results from</span></span><br><span class="line"><span class="comment">*                  different partitions for the `combOp` operator - this will typically be the</span></span><br><span class="line"><span class="comment">*                  neutral element (e.g. `Nil` for list concatenation or `0` for summation)</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param </span>seqOp an operator used to accumulate results within a partition</span></span><br><span class="line"><span class="comment">* <span class="doctag">@param </span>combOp an associative operator used to combine results from different partitions</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> def aggregate[U: ClassTag](zeroValue: U)(seqOp: <span class="function">(<span class="params">U, T</span>) =&gt;</span> U, <span class="attr">combOp</span>: <span class="function">(<span class="params">U, U</span>) =&gt;</span> U): U = withScope &#123;</span><br><span class="line">   <span class="comment">// Clone the zero value since we will also be serializing it as part of tasks</span></span><br><span class="line">   <span class="keyword">var</span> jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance())</span><br><span class="line">   val cleanSeqOp = sc.clean(seqOp)</span><br><span class="line">   val cleanCombOp = sc.clean(combOp)</span><br><span class="line">   val aggregatePartition = <span class="function">(<span class="params">it: Iterator[T]</span>) =&gt;</span> it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)</span><br><span class="line">   val mergeResult = <span class="function">(<span class="params">index: Int, taskResult: U</span>) =&gt;</span> jobResult = combOp(jobResult, taskResult)</span><br><span class="line">   sc.runJob(<span class="keyword">this</span>, aggregatePartition, mergeResult)</span><br><span class="line">   jobResult</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>翻译：</p>
<ul>
<li>aggregate先对每个分区的元素做聚集，然后对所有分区的结果做聚集，聚集过程中，使用的是给定的聚集函数以及初始值”zero value”。这个函数能返回一个与原始RDD不同的类型U，因此，需要一个合并RDD类型T到结果类型U的函数，还需要一个合并类型U的函数。这两个函数都可以修改和返回他们的第一个参数，而不是重新新建一个U类型的参数以避免重新分配内存。</li>
<li>参数zeroValue：seqOp运算符的每个分区的累积结果的初始值以及combOp运算符的不同分区的组合结果的初始值 - 这通常将是初始元素（例如“Nil”表的列表 连接或“0”表示求和）</li>
<li>参数seqOp： 每个分区累积结果的聚集函数。</li>
<li>参数combOp： 一个关联运算符用于组合不同分区的结果</li>
</ul>
<p>解读：</p>
<ul>
<li>这个算子一共有两个参数列表，第一个参数列表中传递 zeroValue （第零个值）第二个参数列表中传递两个函数，传入的第一个函数seqOp函数会作用于每个分区,第二个函数combOp函数在第一个函数执行完之后汇总所有分区结果。</li>
<li>这两个函数的第一个参数都是累加器，第一次执行时，会把zeroValue赋给累加器。第一次j计算之后会把返回值赋给累加器，作为下一次运算的第一个参数。seqOP函数每个分区都有一个累加器，combOp函数只有一个累加器。</li>
</ul>
<h4 id="示例">示例</h4>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">object test &#123;</span><br><span class="line">  def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">    val rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">    def func1(a:Int, <span class="attr">b</span>:Int) = a * b</span><br><span class="line">    def func2(a:Int, <span class="attr">b</span>:Int) = a + b</span><br><span class="line">    println(rdd1.aggregate(<span class="number">3</span>)(func1, func2))</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析：<br>
1）seqOp<br>
seqOp对分区内的所有元素遍历计算<br>
这个分区有几个元素执行几次这个方法<br>
def func1(a:Int, b:Int) = a * b<br>
当第一个元素1传进来时<br>
a是seqOp累加器(第一次执行时，会把zeroValue赋给累加器，zeroValue=3），y是第一个元素1，之后将返回值赋回给累加器。<br>
当第二个元素2传进来时<br>
a：上一次运算之后赋了新值的累加器，3 * 1 =3，y：传入的第二个元素2，之后将返回值赋回给累加器。<br>
当第三个元素3传进来时<br>
a：上一次运算之后赋了新值的累加器，3 * 2 =6，y：传入的第二个元素2，之后将返回值赋回给累加器。<br>
…<br>
最后seqOp累加计算得到360</p>
<p>2）combOp<br>
之后combOp会合并所有分区的结果。<br>
def func2(a:Int, b:Int) = a + b<br>
这个函数遍历所有中间结果（累加器：一个分区一个）<br>
第一次执行时，<br>
a是combOp累加器（第一次执行时，会把zeroValue赋给累加器，zeroValue=3），b是第一个分区的累加器（360)，之后将返回值赋回给累加器。。<br>
第二次执行时，<br>
a是combOp累加器，b是第二个分区的累加器，之后将返回值赋回给累加器。<br>
…<br>
有几个分区，就执行几次combOp函数。<br>
本例只有一个分区，最后计算结果为363</p>
<h4 id="应用">应用</h4>
<p>1.两两求和</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">object test &#123;</span><br><span class="line">  def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">    val rdd = sc.parallelize(List(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>), <span class="number">3</span>)</span><br><span class="line">    println(rdd.sum)</span><br><span class="line">    <span class="comment">//println（rdd.aggregate(0)(_+_,_+_)）</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>rdd.sum可以用rdd.aggregate(0)(<em>+</em>,<em>+</em>)替代，最后运行结果都是18</p>
<p>2.求每个分区的最大值之和</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">object test &#123;</span><br><span class="line">  def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">    val rdd2 = sc.parallelize(List(List(<span class="number">1</span>,<span class="number">3</span>),List(<span class="number">2</span>,<span class="number">4</span>),List(<span class="number">3</span>,<span class="number">5</span>)), <span class="number">3</span>)</span><br><span class="line">    def fun01(x:Int,<span class="attr">y</span>:List[Int]) = &#123;</span><br><span class="line">      x.max(y.max)</span><br><span class="line">    &#125;</span><br><span class="line">    def fun02(a:Int, <span class="attr">b</span>:Int) = a + b</span><br><span class="line">    println(rdd2.aggregate(<span class="number">0</span>)(fun01,fun02))</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>初始值为0，最后运行结果为12，<br>
初始值为10，运行结果为40。</p>
</div><div class="post-copyright"><blockquote><p>原文作者: 暗也橙子</p><p>原文链接: <a href="https://zzuuriel.github.io/article/60.深入理解Spark算子aggregate/">https://zzuuriel.github.io/article/60.深入理解Spark算子aggregate/</a></p><p>版权声明: 转载请注明出处(必须保留作者署名及链接)</p></blockquote></div><div class="tags"><a href="/tags/spark/">spark</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/article/Flink自定义触发器/" class="pre">Flink自定义触发器</a><a href="/article/60.Kudu常用Api(java)/" class="next">Kudu常用Api(java) </a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#深入理解spark算子aggregate"><span class="toc-text">&#x6DF1;&#x5165;&#x7406;&#x89E3;Spark&#x7B97;&#x5B50;aggregate</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#源码解读"><span class="toc-text">&#x6E90;&#x7801;&#x89E3;&#x8BFB;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#示例"><span class="toc-text">&#x793A;&#x4F8B;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#应用"><span class="toc-text">&#x5E94;&#x7528;</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/article/修改Flume源码使taildir source支持递归（可配置）/">修改Flume源码使taildir source支持递归（可配置）</a></li><li class="post-list-item"><a class="post-list-link" href="/article/解决Spark on YARN时jar包乱飞的问题/">解决Spark on YARN时jar包乱飞的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/article/Flink之Watermark理解/">Flink之Watermark理解</a></li><li class="post-list-item"><a class="post-list-link" href="/article/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/"> Spark-2.4.5源码编译</a></li><li class="post-list-item"><a class="post-list-link" href="/article/Flink自定义触发器/">Flink自定义触发器</a></li><li class="post-list-item"><a class="post-list-link" href="/article/60.深入理解Spark算子aggregate/">深入理解Spark算子aggregate</a></li><li class="post-list-item"><a class="post-list-link" href="/article/60.Kudu常用Api(java)/">Kudu常用Api(java) </a></li><li class="post-list-item"><a class="post-list-link" href="/article/60.CentOS7安装单机版Kudu/">CentOS7安装单机版Kudu </a></li><li class="post-list-item"><a class="post-list-link" href="/article/HBase架构和读写流程/">HBase架构和读写流程</a></li><li class="post-list-item"><a class="post-list-link" href="/article/HBase的Rowkey设计/">HBase的Rowkey设计</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/spark/" style="font-size: 15px;">spark</a> <a href="/tags/scala/" style="font-size: 15px;">scala</a> <a href="/tags/kudu/" style="font-size: 15px;">kudu</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/flume/" style="font-size: 15px;">flume</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">1</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"> <div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">暗也橙子.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/zzuUriel/zzuUriel.github.io"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>