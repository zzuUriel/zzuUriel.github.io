<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka | 暗也橙子Blog</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka</h1><a id="logo" href="/.">暗也橙子Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka</h1><div class="post-meta"><a href="/大数据技术/Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka/#comments" class="comment-count"></a><p><span class="date">Aug 03, 2019</span><span><a href="/categories/大数据技术/" class="category">大数据技术</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="spark-streaming-transform的使用amp闭包ampspark-streaming对接kafka">Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka</h3>
<hr>
<h4 id="spark-streaming-transform的使用">Spark Streaming transform的使用</h4>
<p>官网：<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams</a></p>
<p>transform 操作(及其变体如 transformWith)允许在 DStream 上应用任意的 RDD-to-RDD 函数。 它可以用于应用未在 DStream API 中公开的任何 RDD操作。 例如，DStream API没有将数据流中的每个批处理与另一个数据集联接的功能。 然而，你可以很容易地使用transform 来做到这一点，并提供了非常强大的可能性。 例如，可以通过将输入数据流与预先计算的垃圾信息(也可能是由 spark 生成的)联接起来，然后根据这些信息进行过滤，从而实现实时数据清理。</p>
<p>示例：黑名单过滤</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">  val checkpoint = <span class="string">"./transform-checkpoint"</span></span><br><span class="line">  def functionToCreateContext(): StreamingContext = &#123;</span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(sparkConf, Seconds(<span class="number">5</span>))</span><br><span class="line">    ssc.checkpoint(checkpoint)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//黑名单用户：xingxing，转换成RDD</span></span><br><span class="line">    val blacks = List(<span class="string">"xingxing"</span>)</span><br><span class="line">    val blacksRDD = ssc.sparkContext.parallelize(blacks).map((_,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从socket拿到流式数据：xingxing,60</span></span><br><span class="line">    val stream = ssc.socketTextStream(<span class="string">"hadoop000"</span>, <span class="number">9999</span>)</span><br><span class="line">    stream.map(<span class="function"><span class="params">x</span> =&gt;</span> (x.split(<span class="string">","</span>)(<span class="number">0</span>), x))    <span class="comment">// (xingxing, (xingxing,60))</span></span><br><span class="line">        .transform(<span class="function"><span class="params">rdd</span> =&gt;</span> &#123;</span><br><span class="line">          rdd.leftOuterJoin(blacksRDD)   <span class="comment">//(xingxing, (xingxing,60,true))</span></span><br><span class="line">            .filter(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">            x._2._2.getOrElse(<span class="literal">false</span>) != <span class="literal">true</span></span><br><span class="line">          &#125;).map(_._2._1)</span><br><span class="line">        &#125;).print()</span><br><span class="line">    ssc</span><br><span class="line">  &#125;</span><br><span class="line">  val ssc = StreamingContext.getOrCreate(checkpoint, functionToCreateContext _)</span><br><span class="line">  ssc.start()             <span class="comment">// Start the computation</span></span><br><span class="line">  ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="闭包">闭包</h4>
<p>官网：<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-</a><br>
1.Understanding closures<br>
Spark 的难点之一是理解跨集群执行代码时变量和方法的作用域和生命周期。 在作用域外修改变量的 rdd 操作可能经常引起混淆。 在下面的示例中，我们将查看使用 foreach ()增加计数器的代码，但其他操作也可能出现类似的问题。<br>
2.Example<br>
考虑一下下面那个天真的 rdd 元素 sum，它的行为可能会因为是否在同一个 jvm 中执行而有所不同。 一个常见的例子是在本地模式下运行 spark (-- master local [ n ])和在集群中部署 spark 应用程序(例如通过 spark-submit to YARN) :</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">var</span> rdd = sc.parallelize(data)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Wrong: Don't do this!!</span></span><br><span class="line">rdd.foreach(<span class="function"><span class="params">x</span> =&gt;</span> counter += x)</span><br><span class="line"></span><br><span class="line">println(<span class="string">"Counter value: "</span> + counter)</span><br></pre></td></tr></table></figure>
<p>3.Local vs. cluster modes</p>
<ul>
<li>上述代码的行为是未定义的，可能无法按预期工作。为了执行作业，Spark将RDD操作的处理分解为tasks，每个任务由executor执行。在执行之前，Spark计算task的闭包。闭包是那些executor在RDD上执行其计算时必须可见的变量和方法(在本例中为foreach())。这个闭包被序列化并发送给每个executor 。</li>
<li>闭包中发送给每个executor 的变量现在都是副本，因此，当在foreach函数中引用counter时，它不再是driver 上的计数器。在executors的内存中仍然有一个计数器，但它对executor不再可见!executor只看到来自序列化闭包的副本。因此，counter的最终值仍然是零，因为counter上的所有操作都引用了序列化闭包中的值。</li>
<li>在本地模式下，在某些情况下，foreach 函数实际上将在与驱动程序相同的 jvm 中执行，并引用相同的原始计数器，并可能实际更新它。</li>
<li>为了确保在这些场景中定义良好的行为，应该使用累加器。 Spark 中的累加器专门用于提供一种机制，当执行在集群中的工作节点之间分离时，可以安全地更新变量。</li>
<li>一般来说，像循环或局部定义方法这样的闭包结构不应该用来改变全局状态。Spark不保证闭包外部引用的对象的突变行为。一些这样做的代码可能在本地模式下工作，但那只是偶然的，而且这样的代码在分布式模式下不会像预期的那样工作。如果需要全局聚合，则使用Accumulator。</li>
</ul>
<h4 id="spark-streaming对接kafka">Spark Streaming对接Kafka</h4>
<p>官网：<a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#spark-streaming-kafka-integration-guide-kafka-broker-version-010" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#spark-streaming-kafka-integration-guide-kafka-broker-version-010</a><br>
Kafka 0.10的Spark Streaming集成在设计上与0.8 Direct Stream方法类似。 它提供简单的并行性，Kafka分区和Spark分区之间的1：1对应关系，以及对偏移和元数据的访问。 但是，由于较新的集成使用新的Kafka消费者API而不是简单的API，因此使用方法存在显着差异。 这个版本的集成被标记为实验性的，因此 api 可能会发生变化。<br>
1.Linking<br>
对于使用 SBT/Maven 项目定义的 Scala/Java 应用程序，将您的流式应用程序链接到以下工件。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">groupId = org.apache.spark</span><br><span class="line">artifactId = spark-streaming-kafka<span class="number">-0</span><span class="number">-10</span>_2<span class="number">.12</span></span><br><span class="line">version = <span class="number">2.4</span><span class="number">.5</span></span><br></pre></td></tr></table></figure>
<p>不要手动添加对 org.apache.kafka 工件的依赖(例如 kafka-clients)。  kafka-0-10工件已经具有适当的传递依赖性，不同的版本可能难以诊断的方式是不兼容的。<br>
2.Creating a Direct Stream</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe</span><br><span class="line"></span><br><span class="line">val kafkaParams = <span class="built_in">Map</span>[<span class="built_in">String</span>, <span class="built_in">Object</span>](</span><br><span class="line">  <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"localhost:9092,anotherhost:9092"</span>,</span><br><span class="line">  <span class="string">"key.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">  <span class="string">"value.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">  <span class="string">"group.id"</span> -&gt; <span class="string">"use_a_separate_group_id_for_each_stream"</span>,</span><br><span class="line">  <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"latest"</span>,</span><br><span class="line">  <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.Boolean)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val topics = <span class="built_in">Array</span>(<span class="string">"topicA"</span>, <span class="string">"topicB"</span>)</span><br><span class="line">val stream = KafkaUtils.createDirectStream[<span class="built_in">String</span>, <span class="built_in">String</span>](</span><br><span class="line">  streamingContext,</span><br><span class="line">  PreferConsistent,</span><br><span class="line">  Subscribe[<span class="built_in">String</span>, <span class="built_in">String</span>](topics, kafkaParams)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">stream.map(<span class="function"><span class="params">record</span> =&gt;</span> (record.key, record.value))</span><br></pre></td></tr></table></figure>
<p>3.示例<br>
1）Kafka Product API</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">   <span class="comment">// 设置配置文件</span></span><br><span class="line">   val props = <span class="keyword">new</span>  Properties()</span><br><span class="line">   props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop000:9092,hadoop000:9093,hadoop000:9094"</span>);</span><br><span class="line">   props.put(<span class="string">"acks"</span>, <span class="string">"all"</span>);</span><br><span class="line">   props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">   props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">   <span class="comment">// 创建producer</span></span><br><span class="line">   val producer = <span class="keyword">new</span> KafkaProducer[<span class="built_in">String</span>, <span class="built_in">String</span>](props)</span><br><span class="line">   <span class="comment">// 循环发送数据</span></span><br><span class="line">   <span class="keyword">for</span>(i&lt;<span class="number">-1</span> to <span class="number">100</span>) &#123;</span><br><span class="line">     Thread.sleep(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">     val part = i%<span class="number">3</span>    <span class="comment">// 分区</span></span><br><span class="line"></span><br><span class="line">     val word = <span class="built_in">String</span>.valueOf((<span class="keyword">new</span> Random().nextInt(<span class="number">6</span>) + <span class="string">'a'</span>).toChar)</span><br><span class="line">     logger.error(word)</span><br><span class="line">     val record = <span class="keyword">new</span> ProducerRecord[<span class="built_in">String</span>,<span class="built_in">String</span>](<span class="string">"ruozedata3partstopic"</span>, part, <span class="string">""</span>, word)</span><br><span class="line">     producer.send(record)</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">// 关闭producer	</span></span><br><span class="line">producer.close();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>2）Spark Streaming Consumer</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(sparkConf, Seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    val kafkaParams = <span class="built_in">Map</span>[<span class="built_in">String</span>, <span class="built_in">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop000:9092,hadoop000:9093,hadoop000:9094"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; <span class="string">"use_a_separate_group_id_for_each_stream"</span>,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>,</span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.Boolean)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 可以设置多个topic</span></span><br><span class="line">    val topics = <span class="built_in">Array</span>(<span class="string">"ruozedata3partstopic"</span>)</span><br><span class="line">    <span class="comment">// 创建DirectStream</span></span><br><span class="line">    val stream: InputDStream[ConsumerRecord[<span class="built_in">String</span>, <span class="built_in">String</span>]] = KafkaUtils.createDirectStream[<span class="built_in">String</span>, <span class="built_in">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      PreferConsistent,</span><br><span class="line">      Subscribe[<span class="built_in">String</span>, <span class="built_in">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 业务逻辑</span></span><br><span class="line">    val result = stream.map(<span class="function"><span class="params">record</span> =&gt;</span> (record.key, record.value))</span><br><span class="line">      .map(_._2)</span><br><span class="line">      .flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">      .map((_, <span class="number">1</span>))</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">      .foreachRDD(<span class="function"><span class="params">rdd</span> =&gt;</span> &#123;</span><br><span class="line">      rdd.foreachPartition(<span class="function"><span class="params">partition</span> =&gt;</span> &#123;</span><br><span class="line">        val connection = MySQLUtils.getConnection()</span><br><span class="line">        partition.foreach(<span class="function"><span class="params">pair</span> =&gt;</span> &#123;</span><br><span class="line">          val sql = s<span class="string">"insert into wc(word,cnt) values('$&#123;pair._1&#125;', $&#123;pair._2&#125;)"</span></span><br><span class="line">          connection.createStatement().execute(sql)</span><br><span class="line">        &#125;)</span><br><span class="line">        MySQLUtils.closeConnection(connection)</span><br><span class="line">      &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">// 启动程序</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>3）MySQLUtils API</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">object MySQLUtils &#123;</span><br><span class="line"></span><br><span class="line">  def getConnection() = &#123;</span><br><span class="line">    Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">    DriverManager.getConnection(<span class="string">"jdbc:mysql://hadoop000:3306/test"</span>, <span class="string">"root"</span>, <span class="string">"123456"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def closeConnection(connection: Connection): Unit =&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="literal">null</span> != connection) &#123;</span><br><span class="line">      connection.close()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样，从Kafka Product API制造的数据就可以写进Kafka，通过Spark Streaming Consumer消费，最终写到Mysql中。<br>
4）拿到Kafka的Offset</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    val kafkaParams = <span class="built_in">Map</span>[<span class="built_in">String</span>, <span class="built_in">Object</span>](</span><br><span class="line">        <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop000:9092,hadoop000:9093,hadoop000:9094"</span>,</span><br><span class="line">        <span class="string">"key.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">        <span class="string">"value.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">        <span class="string">"group.id"</span> -&gt; <span class="string">"use_a_separate_group_id_for_each_stream"</span>,</span><br><span class="line">        <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>, <span class="comment">//latest</span></span><br><span class="line">        <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.Boolean)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    val topics = <span class="built_in">Array</span>(<span class="string">"ruozedata3partstopic"</span>)</span><br><span class="line">    <span class="comment">// stream不能做任何操作，否则得到的不是一个KafkaRDD</span></span><br><span class="line">    val stream = KafkaUtils.createDirectStream[<span class="built_in">String</span>, <span class="built_in">String</span>](</span><br><span class="line">        ssc,</span><br><span class="line">        PreferConsistent,</span><br><span class="line">        Subscribe[<span class="built_in">String</span>, <span class="built_in">String</span>](topics, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 必须先拿到HasOffsetRanges，才能开始业务逻辑</span></span><br><span class="line">    stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">        <span class="comment">// 通过rdd.asInstanceOf[HasOffsetRanges]拿到KafkaRDD，它保存了每个分区的offset</span></span><br><span class="line">        val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">        <span class="comment">// KafkaRDD维护了topic、partition、fromOffset、untilOffset</span></span><br><span class="line">        offsetRanges.foreach &#123; o =&gt;</span><br><span class="line">            println(s<span class="string">"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 启动程序</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><blockquote><p>原文作者: 暗也橙子</p><p>原文链接: <a href="https://zzuuriel.github.io/大数据技术/Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka/">https://zzuuriel.github.io/大数据技术/Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka/</a></p><p>版权声明: 转载请注明出处(必须保留作者署名及链接)</p></blockquote></div><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/大数据技术/9.Spark Streaming中foreachRDD的使用/" class="pre"> Spark Streaming中foreachRDD的使用 </a><a href="/大数据技术/Spark Streaming基础/" class="next">Spark Streaming基础</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-streaming-transform的使用amp闭包ampspark-streaming对接kafka"><span class="toc-text">Spark Streaming transform&#x7684;&#x4F7F;&#x7528;&amp;&#x95ED;&#x5305;&amp;Spark Streaming&#x5BF9;&#x63A5;Kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-streaming-transform的使用"><span class="toc-text">Spark Streaming transform&#x7684;&#x4F7F;&#x7528;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#闭包"><span class="toc-text">&#x95ED;&#x5305;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-streaming对接kafka"><span class="toc-text">Spark Streaming&#x5BF9;&#x63A5;Kafka</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/大数据技术/修改Flume源码使taildir source支持递归（可配置）/">修改Flume源码使taildir source支持递归（可配置）</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/解决Spark on YARN时jar包乱飞的问题/">解决Spark on YARN时jar包乱飞的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink之Watermark理解/">Flink之Watermark理解</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/"> Spark-2.4.5源码编译</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink自定义触发器/">Flink自定义触发器</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.深入理解Spark算子aggregate/">深入理解Spark算子aggregate</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.Kudu常用Api(java)/">Kudu常用Api(java) </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.CentOS7安装单机版Kudu/">CentOS7安装单机版Kudu </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/CDH5.16.1安装Spark2.x，简称CDS安装/">CDH5.16.1安装Spark2.x，简称CDS安装</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/CDH安装Kafka,简称CDK安装/">CDH安装Kafka</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux基础/">Linux基础</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala编程语言/">Scala编程语言</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据技术/">大数据技术</a><span class="category-list-count">41</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据结构与算法/">数据结构与算法</a><span class="category-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Kudu/" style="font-size: 15px;">Kudu</a> <a href="/tags/CDH/" style="font-size: 15px;">CDH</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Flume/" style="font-size: 15px;">Flume</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">51</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">6</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"> <div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">暗也橙子.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/zzuUriel/zzuUriel.github.io"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>