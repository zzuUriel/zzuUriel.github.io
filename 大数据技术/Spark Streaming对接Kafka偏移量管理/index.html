<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>Spark Streaming对接Kafka偏移量管理 | 暗也橙子Blog</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark Streaming对接Kafka偏移量管理</h1><a id="logo" href="/.">暗也橙子Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Spark Streaming对接Kafka偏移量管理</h1><div class="post-meta"><a href="/大数据技术/Spark Streaming对接Kafka偏移量管理/#comments" class="comment-count"></a><p><span class="date">Aug 06, 2019</span><span><a href="/categories/大数据技术/" class="category">大数据技术</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="spark-streaming对接kafka偏移量管理">Spark Streaming对接Kafka偏移量管理</h3>
<hr>
<p>官网：<a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets</a></p>
<h4 id="获取-offsets">获取 Offsets</h4>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line">  rdd.foreachPartition &#123; iter =&gt;</span><br><span class="line">    val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)</span><br><span class="line">    println(s<span class="string">"$&#123;o.topic&#125; $&#123;o.partition&#125; $&#123;o.fromOffset&#125; $&#123;o.untilOffset&#125;"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>请注意，hasoffsetranges 的类型转换只有在第一个方法中执行时才会成功，这个方法是在创建直接流的结果上调用的，而不是在以后的一系列方法中。 请注意，rdd 分区和 kafka 分区之间的一对一映射在任何进行洗牌或重新分区的方法(例如 reducebykey ()或 window ())之后不会保留。</p>
<h4 id="存储-offsets">存储 Offsets</h4>
<p>首先我们了解一下Kafka消息的三种语义：</p>
<ul>
<li>At most once: 每个记录要么处理一次，要么根本不处理。==&gt;提交偏移量在业务处理之前,偏移量提交了，但是业务未处理</li>
<li>At least once: 每个记录将被处理一次或多次。这比最多一次强，因为它确保不会丢失任何数据。但是可能有重复的。==&gt;业务处理在提交偏移量之前，业务处理了，但是偏移量没有提交</li>
<li>Exactly once: 每条记录将被精确处理一次——没有数据会丢失，也没有数据会被多次处理。这显然是三者中最有力的保证。==&gt;pipeline，保证提交偏移量和业务处理同时成功</li>
</ul>
<p>下面介绍3种方法存储Offsets的方法：<br>
1.Checkpoints<br>
使用Spark Streaming的checkpoint是最简单的存储方式，并且在Spark 框架中很容易实现。Spark Streaming checkpoints就是为保存应用状态而设计的，我们将路径这在HDFS上，所以能够从失败中恢复数据。<br>
如果是应用挂掉的话，那么Spark Streamig应用功能可以从保存的offset中开始读取消息。但是，如果是对Spark Streaming应用进行升级的话，那么很抱歉，checkpoint的数据没法使用，所以这种机制并不可靠，特别是在严格的生产环境中，我们不推荐这种方式。</p>
<p>2.Kafka itself<br>
这种方式支持大吞吐量的 offset 更新，又不需要手动编写 offset 管理程序或者维护一套额外的集群，但是Kafka不支持事物，不能保证输出的幂等性(Exactly once)。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line"></span><br><span class="line">  <span class="comment">// some time later, after outputs have completed</span></span><br><span class="line">  stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.手动维护Offsets<br>
对于支持事务的数据存储，在相同的事务中保存偏移量可以使两者保持同步，即使在故障情况下也是如此。 如果小心地检测重复或跳过的偏移量范围，回滚事务可以防止重复或丢失的消息影响结果。 这就等同于一次性语义。即使是聚合产生的结果也可以使用这种策略，因为聚合产生的结果通常很难产生幂等性(Exactly once)。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The details depend on your data store, but the general idea looks like this</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// begin from the offsets committed to the database</span></span><br><span class="line">val fromOffsets = selectOffsetsFromYourDatabase.map &#123; resultSet =&gt;</span><br><span class="line">  <span class="keyword">new</span> TopicPartition(resultSet.string(<span class="string">"topic"</span>), resultSet.int(<span class="string">"partition"</span>)) -&gt; resultSet.long(<span class="string">"offset"</span>)</span><br><span class="line">&#125;.toMap</span><br><span class="line"></span><br><span class="line">val stream = KafkaUtils.createDirectStream[<span class="built_in">String</span>, <span class="built_in">String</span>](</span><br><span class="line">  streamingContext,</span><br><span class="line">  PreferConsistent,</span><br><span class="line">  Assign[<span class="built_in">String</span>, <span class="built_in">String</span>](fromOffsets.keys.toList, kafkaParams, fromOffsets)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">stream.foreachRDD &#123; rdd =&gt;</span><br><span class="line">  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line"></span><br><span class="line">  val results = yourCalculation(rdd)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// begin your transaction</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// update results</span></span><br><span class="line">  <span class="comment">// update offsets where the end of existing offsets matches the beginning of this batch of offsets</span></span><br><span class="line">  <span class="comment">// assert that offsets were updated correctly</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// end your transaction</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="redis实现offsets幂等性消费">Redis实现Offsets幂等性消费</h4>
<p>下面我们用Redis实现Offsets幂等性消费，手动维护Offsets<br>
1.OffsetsManager Trait<br>
定义一个接口，用来获取和存储Offsets</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">trait OffsetsManager &#123;</span><br><span class="line"></span><br><span class="line">  def obtainOffsets(topic: <span class="built_in">String</span>, <span class="attr">groupId</span>: <span class="built_in">String</span>): <span class="built_in">Map</span>[TopicPartition, Long]</span><br><span class="line"></span><br><span class="line">  def storeOffsets(pipeline: Pipeline, <span class="attr">offsetRanges</span>: <span class="built_in">Array</span>[OffsetRange], <span class="attr">groupId</span>: <span class="built_in">String</span>): Unit</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>2.RedisOffsetsManager Object<br>
使用Redis实现Offsets的获取和存储，定义一个RedisOffsetsManager Object继承OffsetsManager Trait</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * offsetsManager实现类</span></span><br><span class="line"><span class="comment"> * 1) 获取offset</span></span><br><span class="line"><span class="comment"> * 2) 保存offset</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line">object RedisOffsetsManager extends OffsetsManager &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> isReset = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 从redis、kafka获取offset</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * if(redisOffset&gt;kafkaOffset)</span></span><br><span class="line"><span class="comment">   * return allPartition.offset)=0</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param topic</span></span><br><span class="line"><span class="comment">   * @param groupId</span></span><br><span class="line"><span class="comment">   * @return</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  override def obtainOffsets(topic: <span class="built_in">String</span>, <span class="attr">groupId</span>: <span class="built_in">String</span>): <span class="built_in">Map</span>[TopicPartition, Long] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> offsets = <span class="built_in">Map</span>[TopicPartition, Long]() <span class="comment">//返回值</span></span><br><span class="line">  val jedis = RedisUtils.getJedis</span><br><span class="line">  <span class="comment">//1.1 从redis获取offset</span></span><br><span class="line">  val redisMap: util.Map[<span class="built_in">String</span>, <span class="built_in">String</span>] = jedis.hgetAll(topic + <span class="string">"_"</span> + groupId)</span><br><span class="line">  <span class="comment">//1.2 从kafka获取到的offset</span></span><br><span class="line">  val kafkaMap: mutable.Map[TopicPartition, lang.Long] = KafkaOffsetsTool.getLastOffset(topic)</span><br><span class="line">  <span class="comment">//2、进行比较判断，如果fromOffset&gt;untilOffset isReset=true</span></span><br><span class="line">  kafkaMap.foreach(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">    val redisOffset = redisMap.getOrDefault(x._1.partition() + <span class="string">""</span>, <span class="string">"0"</span>)</span><br><span class="line">    <span class="keyword">if</span> (redisOffset.toInt &gt; x._2) &#123;</span><br><span class="line">      isReset = <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">//redisOffset转换为fromOffset</span></span><br><span class="line">  <span class="comment">//if isReset==true 设置所有partition的offset为0</span></span><br><span class="line">    <span class="keyword">import</span> scala.collection.JavaConversions._</span><br><span class="line">    redisMap.foreach(<span class="function"><span class="params">pair</span> =&gt;</span> &#123;</span><br><span class="line"><span class="comment">//      val redisMap: util.Map[String, String] = jedis.hgetAll(topic + "_" + groupId)</span></span><br><span class="line">      val topicPartition = <span class="keyword">new</span> TopicPartition(topic, pair._1.toInt)</span><br><span class="line">      <span class="keyword">var</span> offset = pair._2</span><br><span class="line">      <span class="keyword">if</span> (isReset == <span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">//重置offset</span></span><br><span class="line">        offset = <span class="string">"0"</span></span><br><span class="line">      &#125;</span><br><span class="line">      offsets += topicPartition -&gt; offset.toLong</span><br><span class="line">    &#125;)</span><br><span class="line">    offsets</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 保存offset到Redis</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param offsetRanges</span></span><br><span class="line"><span class="comment">   * @param groupId</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  override def storeOffsets(pipeline: Pipeline, <span class="attr">offsetRanges</span>: <span class="built_in">Array</span>[OffsetRange], <span class="attr">groupId</span>: <span class="built_in">String</span>): Unit = &#123;</span><br><span class="line">  offsetRanges.foreach(<span class="function"><span class="params">o</span> =&gt;</span> &#123;</span><br><span class="line">    pipeline.hset(o.topic + <span class="string">"_"</span> + groupId, o.partition + <span class="string">""</span>, o.untilOffset + <span class="string">""</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.KafkaOffsetsTool Object<br>
当Kafka的数据丢失时，消费数据的偏移量可能会大于Kafka数据的最大偏移量，所以我们需要定义一个KafkaOffsetsTool Object将Kafka数据的最大偏移量取出来，与我们存储在Redis的偏移量作比较，具体处理方式见RedisOffsetsManager Object。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">object KafkaOffsetsTool &#123;</span><br><span class="line"></span><br><span class="line">  def getLastOffset(topic: <span class="built_in">String</span>): mutable.Map[TopicPartition, lang.Long] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> partitionToLongMap: mutable.Map[TopicPartition, lang.Long] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">    val props = <span class="keyword">new</span> Properties</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"hadoop000:9092,hadoop000:9093,hadoop000:9094"</span>)</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, <span class="string">"asd"</span>)</span><br><span class="line">    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>)</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> kafkaConsumer: KafkaConsumer[<span class="built_in">String</span>, <span class="built_in">String</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> topicPartitions: ListBuffer[TopicPartition] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">//1、初始化</span></span><br><span class="line">      kafkaConsumer = <span class="keyword">new</span> KafkaConsumer[<span class="built_in">String</span>, <span class="built_in">String</span>](props)</span><br><span class="line">      topicPartitions = ListBuffer[TopicPartition]()</span><br><span class="line">      <span class="comment">//2、获取partition，构建 topicPartition</span></span><br><span class="line">      val partitionInfos: util.List[PartitionInfo] = kafkaConsumer.partitionsFor(topic)</span><br><span class="line">      <span class="keyword">import</span> scala.collection.JavaConversions._</span><br><span class="line">      partitionInfos.foreach(<span class="function"><span class="params">partitionInfo</span> =&gt;</span> &#123;</span><br><span class="line">        topicPartitions.append(<span class="keyword">new</span> TopicPartition(topic, partitionInfo.partition()))</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="comment">//3、根据topicPartition获取endoffset</span></span><br><span class="line">      val partitionToLongJavaMap: util.Map[TopicPartition, lang.Long] = kafkaConsumer.endOffsets(topicPartitions)</span><br><span class="line">      partitionToLongMap = JavaConverters.mapAsScalaMap(partitionToLongJavaMap)</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="function"><span class="params">Exception</span> =&gt;</span></span><br><span class="line">        e.printStackTrace()</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (kafkaConsumer != <span class="literal">null</span>) kafkaConsumer.close()</span><br><span class="line">    &#125;</span><br><span class="line">    partitionToLongMap</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>4.最终实现</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * SS消费Kafka数据，使用Redis维护offset</span></span><br><span class="line"><span class="comment"> * offset存储格式 topic_groupid partition offset</span></span><br><span class="line"><span class="comment"> * 1) 添加offset矫正,与kafka offset对比</span></span><br><span class="line"><span class="comment"> * 2) 使用pipeline保证业务数据和offset的提交的一致性</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">object StreamingKafkaApp05 &#123;</span><br><span class="line">  def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//1、获取StreamingContext、以及设置KafkaParams和topics</span></span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(sparkConf, Seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    val groupId = <span class="string">"asd"</span></span><br><span class="line">    val wcRedisKeyName = <span class="string">"wc_redis"</span></span><br><span class="line">    val kafkaParams = <span class="built_in">Map</span>[<span class="built_in">String</span>, <span class="built_in">Object</span>](</span><br><span class="line">      <span class="string">"bootstrap.servers"</span> -&gt; <span class="string">"hadoop000:9092,hadoop000:9093,hadoop000:9094"</span>,</span><br><span class="line">      <span class="string">"key.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">      <span class="string">"value.deserializer"</span> -&gt; classOf[StringDeserializer],</span><br><span class="line">      <span class="string">"group.id"</span> -&gt; <span class="string">"asd"</span>,</span><br><span class="line">      <span class="string">"auto.offset.reset"</span> -&gt; <span class="string">"earliest"</span>,</span><br><span class="line">      <span class="string">"enable.auto.commit"</span> -&gt; (<span class="literal">false</span>: java.lang.Boolean)</span><br><span class="line">    )</span><br><span class="line">    val topics = <span class="built_in">Array</span>(<span class="string">"ruozedata3partstopic"</span>)</span><br><span class="line">    <span class="comment">//2、获取offset</span></span><br><span class="line">    val fromOffset: collection.Map[TopicPartition, Long] = RedisOffsetsManager.obtainOffsets(topics(<span class="number">0</span>), groupId)</span><br><span class="line">    <span class="comment">//3、业务处理、offset提交</span></span><br><span class="line">    val stream = KafkaUtils.createDirectStream(</span><br><span class="line">      ssc,</span><br><span class="line">      LocationStrategies.PreferConsistent,</span><br><span class="line">      Subscribe[<span class="built_in">String</span>, <span class="built_in">String</span>](topics, kafkaParams, fromOffset))</span><br><span class="line"></span><br><span class="line">    stream.foreachRDD(<span class="function"><span class="params">rdd</span> =&gt;</span> &#123;</span><br><span class="line">      println(<span class="string">"接收数据条数: "</span> + rdd.count())</span><br><span class="line">      <span class="keyword">if</span> (!rdd.isEmpty()) &#123;</span><br><span class="line">        val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges <span class="comment">//获取最新的offset信息</span></span><br><span class="line">        val result = rdd.map(<span class="function"><span class="params">x</span> =&gt;</span> (x.value(), <span class="number">1</span>)).reduceByKey(_ + _).collect() <span class="comment">//业务逻辑处理：词频统计</span></span><br><span class="line">        <span class="keyword">var</span> jedis: Jedis = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">var</span> pipeline: Pipeline = <span class="literal">null</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">//4、开启jedis pipelined</span></span><br><span class="line">          jedis = RedisUtils.getJedis</span><br><span class="line">          pipeline = jedis.pipelined()</span><br><span class="line">          pipeline.multi() <span class="comment">//JedisDataException: DISCARD without MULTI</span></span><br><span class="line">          <span class="comment">//if isReset 把旧数据放到tmp中</span></span><br><span class="line">          <span class="keyword">if</span> (RedisOffsetsManager.isReset) &#123;</span><br><span class="line">            pipeline.rename(wcRedisKeyName, wcRedisKeyName + <span class="string">"_tmp"</span>)</span><br><span class="line">            RedisOffsetsManager.isReset = <span class="literal">false</span></span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">//4.1 保存计算结果</span></span><br><span class="line">          result.map(<span class="function"><span class="params">pair</span> =&gt;</span> &#123;</span><br><span class="line">            pipeline.hincrBy(wcRedisKeyName, pair._1, pair._2)</span><br><span class="line">          &#125;)</span><br><span class="line">          <span class="comment">//4.2 保存offset</span></span><br><span class="line">          RedisOffsetsManager.storeOffsets(pipeline, offsetRanges, groupId)</span><br><span class="line">          <span class="comment">//4.3 pipe提交</span></span><br><span class="line">          pipeline.exec()</span><br><span class="line">          pipeline.sync()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="function"><span class="params">Exception</span> =&gt;</span></span><br><span class="line">            pipeline.discard() <span class="comment">//不成功，清除</span></span><br><span class="line">            e.printStackTrace()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">finally</span> &#123;</span><br><span class="line">          pipeline.close()</span><br><span class="line">          jedis.close()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">        println(<span class="string">"当前批次无数据"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="comment">//5、开始作业</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><blockquote><p>原文作者: 暗也橙子</p><p>原文链接: <a href="https://zzuuriel.github.io/大数据技术/Spark Streaming对接Kafka偏移量管理/">https://zzuuriel.github.io/大数据技术/Spark Streaming对接Kafka偏移量管理/</a></p><p>版权声明: 转载请注明出处(必须保留作者署名及链接)</p></blockquote></div><div class="tags"><a href="/tags/Spark/">Spark</a><a href="/tags/Kafka/">Kafka</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/大数据技术/Spark Streaming读取kafka数据的两种方式(Receiver和Direct)/" class="pre">Spark Streaming读取kafka数据的两种方式(Receiver和Direct)</a><a href="/大数据技术/9.Spark Streaming中foreachRDD的使用/" class="next"> Spark Streaming中foreachRDD的使用 </a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-streaming对接kafka偏移量管理"><span class="toc-text">Spark Streaming&#x5BF9;&#x63A5;Kafka&#x504F;&#x79FB;&#x91CF;&#x7BA1;&#x7406;</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#获取-offsets"><span class="toc-text">&#x83B7;&#x53D6; Offsets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#存储-offsets"><span class="toc-text">&#x5B58;&#x50A8; Offsets</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#redis实现offsets幂等性消费"><span class="toc-text">Redis&#x5B9E;&#x73B0;Offsets&#x5E42;&#x7B49;&#x6027;&#x6D88;&#x8D39;</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/大数据技术/修改Flume源码使taildir source支持递归（可配置）/">修改Flume源码使taildir source支持递归（可配置）</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/解决Spark on YARN时jar包乱飞的问题/">解决Spark on YARN时jar包乱飞的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink之Watermark理解/">Flink之Watermark理解</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/"> Spark-2.4.5源码编译</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink自定义触发器/">Flink自定义触发器</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.深入理解Spark算子aggregate/">深入理解Spark算子aggregate</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.Kudu常用Api(java)/">Kudu常用Api(java) </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.CentOS7安装单机版Kudu/">CentOS7安装单机版Kudu </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/CDH5.16.1安装Spark2.x，简称CDS安装/">CDH5.16.1安装Spark2.x，简称CDS安装</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/CDH安装Kafka,简称CDK安装/">CDH安装Kafka</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux基础/">Linux基础</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala编程语言/">Scala编程语言</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据技术/">大数据技术</a><span class="category-list-count">41</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据结构与算法/">数据结构与算法</a><span class="category-list-count">4</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Kudu/" style="font-size: 15px;">Kudu</a> <a href="/tags/CDH/" style="font-size: 15px;">CDH</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Flume/" style="font-size: 15px;">Flume</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">50</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">6</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"> <div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">暗也橙子.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/zzuUriel/zzuUriel.github.io"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>