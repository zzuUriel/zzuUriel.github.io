<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>累加器和广播变量&amp;&amp;持久化 | 暗也橙子Blog</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">累加器和广播变量&amp;&amp;持久化</h1><a id="logo" href="/.">暗也橙子Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">累加器和广播变量&amp;&amp;持久化</h1><div class="post-meta"><a href="/大数据技术/累加器和广播变量&amp;&amp;持久化/#comments" class="comment-count"></a><p><span class="date">Jun 28, 2019</span><span><a href="/categories/大数据技术/" class="category">大数据技术</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="累加器和广播变量ampamp持久化">累加器和广播变量&amp;&amp;持久化</h3>
<hr>
<h4 id="累加器和广播变量">累加器和广播变量</h4>
<p>Spark有两种共享变量：广播变量（broadcast variable）与累加器（accumulator）<br>
累加器用来对信息进行聚合，而广播变量用来高效分发较大的对象。<br>
共享变量出现的原因：<br>
通常情况下，当向Spark操作(如map,reduce)传递一个函数时，它会在一个远程集群节点上执行，它会使用函数中所有变量的副本。这些变量被复制到所有的机器上，远程机器上并没有被更新的变量向驱动程序回传。通常跨任务的读写变量是低效的，但是，Spark还是提供了两种有限的共享变量：广播变量（broadcast variable）和累加器</p>
<h5 id="累加器">累加器</h5>
<p>在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到累加器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会在driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。<br>
<img src="/img/spark/%E7%B4%AF%E5%8A%A0%E5%99%A8.png" alt=" stage划分图"><br>
累加器的用法如下所示：<br>
(1)通过在Driver中调用 SparkContext.accumulator(initialValue) 方法，创建出存有初始值的累加器。返回值为 org.apache.spark.Accumulator[T] 对象，其中 T 是初始值initialValue 的类型。<br>
(2)Spark闭包（函数序列化）里的excutor代码可以使用累加器的 += 方法（在Java中是 add ）增加累加器的值。<br>
(3)Driver程序可以调用累加器的 value 属性（在 Java 中使用 value() 或 setValue() ）来访问累加器的值。</p>
<p>计数器种类很多，但是经常使用的就是两种，longAccumulator和collectionAccumulator<br>
需要注意的是计数器是lazy的，只有触发action才会进行计数，在不持久的情况下重复触发action,计数器会重复累加</p>
<p>1、LongAccumulator</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"MyLongAccumulator"</span>)</span><br><span class="line">   val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">   val acc = sc.longAccumulator(<span class="string">"计数"</span>)</span><br><span class="line">   val rdd = sc.parallelize(List(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>))</span><br><span class="line">   val numberRDD = rdd.map(<span class="function"><span class="params">x</span>=&gt;</span>&#123;</span><br><span class="line">     acc.add(<span class="number">1</span>L)</span><br><span class="line">   &#125;)</span><br><span class="line">   numberRDD.count()</span><br><span class="line">   println(acc.value) <span class="comment">//9</span></span><br><span class="line">   numberRDD.count()</span><br><span class="line">   println(acc.value) <span class="comment">//18</span></span><br><span class="line">   numberRDD.count()</span><br><span class="line">   println(acc.value) <span class="comment">//27</span></span><br><span class="line">   sc.stop()</span><br></pre></td></tr></table></figure>
<p>使用longAccumulator做计数的时候要小心重复执行action导致的acc.value的变化,这是因为重复执行了count,累加器的数量成倍增长，解决方法，在action操作之前调用rdd的cache方法（或persist）,这样在count后数据集就会被缓存下来，而无需从头开始计算</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">numberRDD.cache()</span><br><span class="line">   numberRDD.count()</span><br><span class="line">   println(acc.value)  <span class="comment">//9</span></span><br><span class="line">   numberRDD.count()</span><br><span class="line">   println(acc.value)  <span class="comment">//9</span></span><br><span class="line">   numberRDD.count()</span><br><span class="line">   println(acc.value)  <span class="comment">//9</span></span><br></pre></td></tr></table></figure>
<p>2、CollectionAccumulator<br>
CollectionAccumulator,集合计数器，计数器中保存的是集合元素，通过泛型指定</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 需求:id后三位相同的加入计数器</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"MyLongAccumulator"</span>)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">    <span class="comment">//生成集合计数器</span></span><br><span class="line">    val acc = sc.collectionAccumulator[People](<span class="string">"集合计数器"</span>)</span><br><span class="line">    <span class="comment">//生成RDD</span></span><br><span class="line">    val rdd = sc.parallelize(<span class="built_in">Array</span>(People(<span class="string">"p1"</span>, <span class="number">100000</span>), People(<span class="string">"p2"</span>, <span class="number">100001</span>),</span><br><span class="line">      People(<span class="string">"p3"</span>, <span class="number">100222</span>), People(<span class="string">"p4"</span>, <span class="number">100003</span>)))</span><br><span class="line">    rdd.map(<span class="function"><span class="params">x</span>=&gt;</span>&#123;</span><br><span class="line">      val id = x.id.toString.reverse</span><br><span class="line">      <span class="comment">//满足条件就加入计数器</span></span><br><span class="line">      <span class="keyword">if</span>(id(<span class="number">0</span>) == id(<span class="number">1</span>) &amp;&amp; id(<span class="number">0</span>) == id(<span class="number">2</span>))&#123;</span><br><span class="line">        acc.add(x)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;).count()</span><br><span class="line">    println(acc.value) <span class="comment">//[People(p1,100000), People(p3,100222)]</span></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">  case class People(name:String,id:Long);</span><br></pre></td></tr></table></figure>
<h5 id="广播变量">广播变量</h5>
<p>如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么只是每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。<br>
<img src="/img/spark/%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F.png" alt=" stage划分图"><br>
小表广播案例<br>
spark有一种常见的优化方式就是小表广播，使用map join来代替reduce join,我们通过把小表的数据集广播到各个节点上，节省了shuffle操作。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line">      .setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Fact table  航线(起点机场, 终点机场, 航空公司, 起飞时间)</span></span><br><span class="line">    val flights = sc.parallelize(List(</span><br><span class="line">      (<span class="string">"SEA"</span>, <span class="string">"JFK"</span>, <span class="string">"DL"</span>,  <span class="string">"7:00"</span>),</span><br><span class="line">      (<span class="string">"SFO"</span>, <span class="string">"LAX"</span>, <span class="string">"AA"</span>,  <span class="string">"7:05"</span>),</span><br><span class="line">      (<span class="string">"SFO"</span>, <span class="string">"JFK"</span>, <span class="string">"VX"</span>, <span class="string">"7:05"</span>),</span><br><span class="line">      (<span class="string">"JFK"</span>, <span class="string">"LAX"</span>, <span class="string">"DL"</span>, <span class="string">"7:10"</span>),</span><br><span class="line">      (<span class="string">"LAX"</span>, <span class="string">"SEA"</span>, <span class="string">"DL"</span>,  <span class="string">"7:10"</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Dimension table 机场(简称, 全称, 城市, 所处城市简称)</span></span><br><span class="line">    val airports = sc.parallelize(List(</span><br><span class="line">      (<span class="string">"JFK"</span>, <span class="string">"John F. Kennedy International Airport"</span>, <span class="string">"New York"</span>, <span class="string">"NY"</span>),</span><br><span class="line">      (<span class="string">"LAX"</span>, <span class="string">"Los Angeles International Airport"</span>, <span class="string">"Los Angeles"</span>, <span class="string">"CA"</span>),</span><br><span class="line">      (<span class="string">"SEA"</span>, <span class="string">"Seattle-Tacoma International Airport"</span>, <span class="string">"Seattle"</span>, <span class="string">"WA"</span>),</span><br><span class="line">      (<span class="string">"SFO"</span>, <span class="string">"San Francisco International Airport"</span>, <span class="string">"San Francisco"</span>, <span class="string">"CA"</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Dimension table  航空公司(简称,全称)</span></span><br><span class="line">    val airlines = sc.parallelize(List(</span><br><span class="line">      (<span class="string">"AA"</span>, <span class="string">"American Airlines"</span>),</span><br><span class="line">      (<span class="string">"DL"</span>, <span class="string">"Delta Airlines"</span>),</span><br><span class="line">      (<span class="string">"VX"</span>, <span class="string">"Virgin America"</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//最终统计结果：</span></span><br><span class="line">    <span class="comment">//出发城市           终点城市           航空公司名称         起飞时间</span></span><br><span class="line">    <span class="comment">//Seattle           New York       Delta Airlines           7:00</span></span><br><span class="line">    <span class="comment">//San Francisco     Los Angeles    American Airlines       7:05</span></span><br><span class="line">    <span class="comment">//San Francisco     New York       Virgin America            7:05</span></span><br><span class="line">    <span class="comment">//New York          Los Angeles    Delta Airlines           7:10</span></span><br><span class="line">    <span class="comment">//Los Angeles       Seattle        Delta Airlines          7:10</span></span><br><span class="line"></span><br><span class="line">    val airportsBc = sc.broadcast(airports.map(<span class="function"><span class="params">x</span> =&gt;</span> (x._1, x._3)).collectAsMap())</span><br><span class="line">    val airlinesBc = sc.broadcast(airlines.collectAsMap())</span><br><span class="line"></span><br><span class="line">    flights.map&#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="function">(<span class="params">a,b,c,d</span>) =&gt;</span> (airportsBc.value.get(a).get,</span><br><span class="line">        airportsBc.value.get(b).get,</span><br><span class="line">        airlinesBc.value.get(c).get,</span><br><span class="line">        d</span><br><span class="line">      )</span><br><span class="line">    &#125;.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>为什么只能 broadcast 只读的变量<br>
这就涉及一致性的问题，如果变量可以被更新，那么变量被某个节点更新，其他节点需要一块更新，这涉及了事务一致性。<br>
注意事项：</p>
<ul>
<li>变量一旦被定义为一个广播变量，那么这个变量只能读，不能修改</li>
<li>能不能将一个RDD使用广播变量广播出去？因为RDD是不存储数据的。可以将RDD的结果广播出去。</li>
<li>广播变量只能在Driver端定义，不能在Executor端定义。</li>
<li>在Driver端可以修改广播变量的值，在Executor端无法修改广播变量的值。</li>
<li>如果Executor端用到了Driver的变量，不使用广播变量在Executor有多少task就有多少Driver端的变量副本。</li>
<li>如果Executor端用到了Driver的变量，使用广播变量在每个Executor中只有一份Driver端的变量副本。</li>
</ul>
<h4 id="持久化">持久化</h4>
<p>Spark非常重要的一个功能特性就是可以将RDD持久化在内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，直接使用内存缓存的partition。这样的话，对于针对一个RDD反复执行多个操作的场景，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要反复计算多次该RDD。</p>
<p>巧妙使用RDD持久化，甚至在某些场景下，可以将spark应用程序的性能提升10倍。对于迭代式算法和快速交互式应用来说，RDD持久化，是非常重要的。<br>
持久化的存储级别很多，常用的是MEMORY_ONLY、MEMORY_ONLY_SER、MEMORY_AND_DISK<br>
<img src="/img/spark/%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8%E7%BA%A7%E5%88%AB.png" alt=" stage划分图"></p>
<p>如何选择存储级别？<br>
Storage Level的选择是内存和CPU的权衡</p>
<p>1.如果内存足够，默认的存储级别（MEMORY_ONLY (不进行序列化)）是性能最优的，最高效的。<br>
2.如果内存不够且CPU跟的上，可以尝试MEMORY_ONLY_SER 再加上一个序列化框架(kyro），这样内存的空间更好。<br>
3.不要把数据写到磁盘，这样成本是非常高的，当数据太大的时候，可以过滤一部分的数据再存，这样的话可能会更快。<br>
4.可以使用副本的存储级别能更快的容错，所有的storage level都提供了副本机制（从另外的节点拿）。<br>
首选第1种方式，如果满足不了再使用第2种。后两种不推荐。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; numberRDD.cache</span><br><span class="line">res18: forRDD.type = MapPartitionsRDD[<span class="number">9</span>] at map at &lt;<span class="built_in">console</span>&gt;:<span class="number">27</span></span><br><span class="line"></span><br><span class="line">scala&gt; numberRDD.count</span><br><span class="line">res19: Long = <span class="number">8</span></span><br></pre></td></tr></table></figure>
<p>结果可以在Web UI的Storage中查看</p>
<p>Spark自动监视每个节点上的缓存使用情况，并以最近最少使用(LRU)的方式删除旧的数据分区。如果想要手动删除一个RDD，而不是等待它从缓存中消失，那么可以使用RDD.unpersist()方法,清除缓存数据是立即执行的</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; numberRDD.unpersist()</span><br><span class="line">res8: numberRDD.type = MapPartitionsRDD[<span class="number">3</span>] at map at &lt;<span class="built_in">console</span>&gt;:<span class="number">28</span></span><br></pre></td></tr></table></figure>
<p>修改存储级别</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val forRDD = rdd.map(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">    <span class="comment">//计数器做累加</span></span><br><span class="line">    acc.add(<span class="number">1</span>L)</span><br><span class="line">&#125;).persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br></pre></td></tr></table></figure>
<p>cache和persist有什么区别和联系？</p>
<ul>
<li>使用cache()和persist()进行持久化操作，它们都是lazy的，需要action才能触发，默认使用MEMORY_ONLY</li>
<li>cache调用的persist，persist调用的persist(storage level)</li>
</ul>
<p>序列化和非序列化有什么区别？</p>
<ul>
<li>序列化将对象转换成字节数组了，节省空间，占CPU</li>
</ul>
<p>测试：开启kyro序列化（需要注册）</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">val sparkConf = <span class="keyword">new</span> SparkConf()</span><br><span class="line">      .setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"SerApp"</span>)</span><br><span class="line">      ---------开启序列化------</span><br><span class="line">      .set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">      ------注册-----</span><br><span class="line">      .registerKryoClasses(<span class="built_in">Array</span>(classOf[Student]))</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 10w条数据 原大小2908KB</span></span><br><span class="line"><span class="comment">      * 1:Kryo serialization 注册 2029.1KB</span></span><br><span class="line"><span class="comment">      * 2:Java serialization 3.1 MB</span></span><br><span class="line"><span class="comment">      * 3:Kryo serializationkryo 不注册 4.8 MB</span></span><br><span class="line"><span class="comment">      * </span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    rdd.persist(StorageLevel.MEMORY_ONLY_SER).count()</span><br><span class="line">    </span><br><span class="line">    case class Student(id: String, name: String, age: Int)</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><blockquote><p>原文作者: 暗也橙子</p><p>原文链接: <a href="https://zzuuriel.github.io/大数据技术/累加器和广播变量&amp;&amp;持久化/">https://zzuuriel.github.io/大数据技术/累加器和广播变量&amp;&amp;持久化/</a></p><p>版权声明: 转载请注明出处(必须保留作者署名及链接)</p></blockquote></div><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/大数据技术/Spark Core-分组求topn/" class="pre">Spark Core-分组求topn</a><a href="/大数据技术/Spark Core基础-WordCount增强/" class="next"> Spark Core基础-WordCount增强</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#累加器和广播变量ampamp持久化"><span class="toc-text">&#x7D2F;&#x52A0;&#x5668;&#x548C;&#x5E7F;&#x64AD;&#x53D8;&#x91CF;&amp;&amp;&#x6301;&#x4E45;&#x5316;</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#累加器和广播变量"><span class="toc-text">&#x7D2F;&#x52A0;&#x5668;&#x548C;&#x5E7F;&#x64AD;&#x53D8;&#x91CF;</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#累加器"><span class="toc-text">&#x7D2F;&#x52A0;&#x5668;</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#广播变量"><span class="toc-text">&#x5E7F;&#x64AD;&#x53D8;&#x91CF;</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#持久化"><span class="toc-text">&#x6301;&#x4E45;&#x5316;</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/大数据技术/修改Flume源码使taildir source支持递归（可配置）/">修改Flume源码使taildir source支持递归（可配置）</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/解决Spark on YARN时jar包乱飞的问题/">解决Spark on YARN时jar包乱飞的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink之Watermark理解/">Flink之Watermark理解</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/"> Spark-2.4.5源码编译</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink自定义触发器/">Flink自定义触发器</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.深入理解Spark算子aggregate/">深入理解Spark算子aggregate</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.Kudu常用Api(java)/">Kudu常用Api(java) </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.CentOS7安装单机版Kudu/">CentOS7安装单机版Kudu </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/CDH5.16.1安装Spark2.x，简称CDS安装/">CDH5.16.1安装Spark2.x，简称CDS安装</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/CDH安装Kafka,简称CDK安装/">CDH安装Kafka</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux基础/">Linux基础</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala编程语言/">Scala编程语言</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据技术/">大数据技术</a><span class="category-list-count">41</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据结构与算法/">数据结构与算法</a><span class="category-list-count">8</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Kudu/" style="font-size: 15px;">Kudu</a> <a href="/tags/CDH/" style="font-size: 15px;">CDH</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Flume/" style="font-size: 15px;">Flume</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">54</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">6</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"> <div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">暗也橙子.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/zzuUriel/zzuUriel.github.io"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>