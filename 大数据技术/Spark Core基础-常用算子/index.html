<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>Spark Core基础-常用算子 | 暗也橙子Blog</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark Core基础-常用算子</h1><a id="logo" href="/.">暗也橙子Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Spark Core基础-常用算子</h1><div class="post-meta"><a href="/大数据技术/Spark Core基础-常用算子/#comments" class="comment-count"></a><p><span class="date">Jun 04, 2019</span><span><a href="/categories/大数据技术/" class="category">大数据技术</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="spark-core基础-常用算子">Spark Core基础-常用算子</h3>
<hr>
<h4 id="常用算子">常用算子</h4>
<h5 id="map-mappartition-mappartitionwithindex">map、mapPartition、mapPartitionWithIndex</h5>
<p>1.map<br>
map对rdd中的每一个元素进行操作。<br>
2.mapPartition<br>
mapPartitions是对rdd中的每个分区的迭代器进行操作。<br>
mapPartitions的优点：<br>
如果是普通的map，比如一个partition中有1万条数据。那么你的function要执行和计算1万次。使用mapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。如果在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库,map需要为每个元素创建一个链接而mapPartition为每个partition创建一个链接),则mapPartitions效率比map高的多。SparkSql或DataFrame默认会对程序进行mapPartition的优化。<br>
mapPartitions的缺点：<br>
如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下， 比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。<br>
所以说普通的map操作通常不会导致内存的OOM异常。 但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。<br>
3.mapPartitionWithIndex<br>
mapPartitions则是对rdd中的每个分区的迭代器进行操作，并且可以将分区的编号取出来。<br>
示例：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">object mapPartitionsWithIndexApp &#123;</span><br><span class="line">  def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">    val rdd1: RDD[<span class="built_in">String</span>] = sc.parallelize(List(</span><br><span class="line">      <span class="string">"spark1"</span>, <span class="string">"spark2"</span>, <span class="string">"spark3"</span>,</span><br><span class="line">      <span class="string">"spark4"</span>, <span class="string">"spark5"</span>, <span class="string">"spark6"</span>,</span><br><span class="line">      <span class="string">"spark7"</span>, <span class="string">"spark8"</span>, <span class="string">"spark9"</span>),</span><br><span class="line">      <span class="number">3</span>)</span><br><span class="line">    val rdd2: RDD[<span class="built_in">String</span>] = rdd1.mapPartitionsWithIndex &#123;</span><br><span class="line">      (index, iter) =&gt; &#123;</span><br><span class="line">        println()</span><br><span class="line">        <span class="keyword">var</span> result = List[<span class="built_in">String</span>]()</span><br><span class="line">        <span class="keyword">var</span> subfix = <span class="string">""</span></span><br><span class="line">        <span class="keyword">if</span> (index == <span class="number">0</span>) subfix = <span class="string">"【北京区】"</span></span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (index == <span class="number">1</span>) subfix = <span class="string">"【上海区】"</span></span><br><span class="line">        <span class="keyword">else</span> subfix = <span class="string">"【广州区】"</span></span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">          val str: <span class="built_in">String</span> = iter.next()</span><br><span class="line">          result = result :+ subfix + str</span><br><span class="line">        &#125;</span><br><span class="line">        result.iterator</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    rdd2.foreach(println(_))</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">【北京区】spark1</span><br><span class="line">【北京区】spark2</span><br><span class="line">【北京区】spark3</span><br><span class="line"></span><br><span class="line">【上海区】spark4</span><br><span class="line">【上海区】spark5</span><br><span class="line">【上海区】spark6</span><br><span class="line"></span><br><span class="line">【广州区】spark7</span><br><span class="line">【广州区】spark8</span><br><span class="line">【广州区】spark9</span><br><span class="line"></span><br><span class="line">Process finished <span class="keyword">with</span> exit code <span class="number">0</span></span><br></pre></td></tr></table></figure>
<h5 id="zip-zipwithindex">zip、zipWithIndex</h5>
<p>1.zip<br>
zip把两个集合按元素顺序合并成元组：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(List(<span class="string">"pk"</span>, <span class="string">"J"</span>, <span class="string">"xingxing"</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[<span class="built_in">String</span>] = ParallelCollectionRDD[<span class="number">0</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 = sc.parallelize(List(<span class="string">"1"</span>, <span class="string">"2"</span>, <span class="string">"3"</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[<span class="built_in">String</span>] = ParallelCollectionRDD[<span class="number">1</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.zip(rdd2).collect</span><br><span class="line">res4: <span class="built_in">Array</span>[(<span class="built_in">String</span>, <span class="built_in">String</span>)] = <span class="built_in">Array</span>((pk,<span class="number">1</span>), (J,<span class="number">2</span>), (xingxing,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>注意：<br>
和scala不同,spark中必须保证:</p>
<ul>
<li>元素数目相同</li>
<li>分区数相同</li>
</ul>
<p>2.zipWithIndex<br>
该函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键/值对。<br>
示例：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd3 = sc.parallelize(List(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"flink"</span>))</span><br><span class="line">rdd3: org.apache.spark.rdd.RDD[<span class="built_in">String</span>] = ParallelCollectionRDD[<span class="number">6</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.zipWithIndex().collect</span><br><span class="line">res6: <span class="built_in">Array</span>[(<span class="built_in">String</span>, Long)] = <span class="built_in">Array</span>((hadoop,<span class="number">0</span>), (spark,<span class="number">1</span>), (flink,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<h5 id="union-distinct-intersection-substract-cartesian">union、distinct、intersection、substract、cartesian</h5>
<p>创建两个rdd：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd4 = sc.parallelize(List(<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"b"</span>,<span class="string">"c"</span>,<span class="string">"c"</span>))</span><br><span class="line">rdd4: org.apache.spark.rdd.RDD[<span class="built_in">String</span>] = ParallelCollectionRDD[<span class="number">8</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; val rdd5 = sc.parallelize(List(<span class="string">"c"</span>, <span class="string">"d"</span>, <span class="string">"e"</span>))</span><br><span class="line">rdd5: org.apache.spark.rdd.RDD[<span class="built_in">String</span>] = ParallelCollectionRDD[<span class="number">9</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure>
<p>1.union<br>
返回一个新的数据集，该数据集包含源数据集和参数中的元素的联合。（不去重）</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd4.union(rdd5).collect</span><br><span class="line">res7: <span class="built_in">Array</span>[<span class="built_in">String</span>] = <span class="built_in">Array</span>(a, b, b, c, c, c, d, e)</span><br></pre></td></tr></table></figure>
<p>2.distinct<br>
返回包含源数据集去重后元素的新数据集。<br>
源码：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def distinct(numPartitions: Int)(implicit ord: Ordering[T] = <span class="literal">null</span>): RDD[T] = withScope &#123;</span><br><span class="line">  map(<span class="function"><span class="params">x</span> =&gt;</span> (x, <span class="literal">null</span>)).reduceByKey(<span class="function">(<span class="params">x, y</span>) =&gt;</span> x, numPartitions).map(_._1)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd4.distinct().collect</span><br><span class="line">res8: <span class="built_in">Array</span>[<span class="built_in">String</span>] = <span class="built_in">Array</span>(b, a, c)</span><br></pre></td></tr></table></figure>
<p>3.intersection 交集<br>
intersection(otherDataset) 返回一个新的RDD，它包含源数据集中元素和参数的交集。（去重）</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd4.intersection(rdd5).collect</span><br><span class="line">res9: <span class="built_in">Array</span>[<span class="built_in">String</span>] = <span class="built_in">Array</span>(c)</span><br></pre></td></tr></table></figure>
<p>4.substract 差集<br>
rdd1.subtract (rdd2) 返回在rdd1中出现，但是不在rdd2中出现的元素。（不去重）</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd4.subtract(rdd5).collect</span><br><span class="line">res10: <span class="built_in">Array</span>[<span class="built_in">String</span>] = <span class="built_in">Array</span>(b, b, a)</span><br></pre></td></tr></table></figure>
<p>5.cartesian 笛卡尔<br>
cartesian(otherDataset) 当调用类型T和U的数据集时，返回(T,U)对的数据集(所有对元素)。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd4.cartesian(rdd5).collect</span><br><span class="line">res11: <span class="built_in">Array</span>[(<span class="built_in">String</span>, <span class="built_in">String</span>)] = <span class="built_in">Array</span>((a,c), (b,c), (a,d), (a,e), (b,d), (b,e), (b,c), (c,c), (c,c), (b,d), (b,e), (c,d), (c,e), (c,d), (c,e))</span><br></pre></td></tr></table></figure>
<h5 id="sortby-sortbykey">sortBy、sortByKey</h5>
<p>在Spark中存在两种对RDD进行排序的函数，分别是 sortBy和sortByKey函数。sortBy是对标准的RDD进行排序，它是从Spark 0.9.0之后才引入的（可以参见SPARK-1063）。而sortByKey函数是对PairRDD进行排序，也就是有Key和Value的RDD。<br>
1.sortBy<br>
sortBy函数函数的实现依赖于sortByKey函数，示例：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//升序</span></span><br><span class="line">scala&gt; rdd6.sortBy(<span class="function"><span class="params">x</span> =&gt;</span> x).collect()</span><br><span class="line">res13: <span class="built_in">Array</span>[Int] = <span class="built_in">Array</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">23</span>, <span class="number">46</span>)</span><br><span class="line"><span class="comment">//降序</span></span><br><span class="line">scala&gt; rdd6.sortBy(<span class="function"><span class="params">x</span> =&gt;</span> x, <span class="literal">false</span>).collect()</span><br><span class="line">res14: <span class="built_in">Array</span>[Int] = <span class="built_in">Array</span>(<span class="number">46</span>, <span class="number">23</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment">//修改分区数，默认的分区个数是2，而我们对它进行了修改，所以最后变成了1。</span></span><br><span class="line">scala&gt; val result = rdd6.sortBy(<span class="function"><span class="params">x</span> =&gt;</span> x, <span class="literal">false</span>, <span class="number">1</span>)</span><br><span class="line">result: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[<span class="number">41</span>] at sortBy at &lt;<span class="built_in">console</span>&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; result.partitions.size</span><br><span class="line">res16: Int = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>2.sortByKey<br>
sortByKey函数作用于Key-Value形式的RDD，并对Key进行排序。它是在org.apache.spark.rdd.OrderedRDDFunctions中实现的，实现如下：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def sortByKey(ascending: <span class="built_in">Boolean</span> = <span class="literal">true</span>, <span class="attr">numPartitions</span>: Int = self.partitions.size)</span><br><span class="line"></span><br><span class="line">    : RDD[(K, V)] =</span><br><span class="line">&#123;</span><br><span class="line">  val part = <span class="keyword">new</span> RangePartitioner(numPartitions, self, ascending)</span><br><span class="line">  <span class="keyword">new</span> ShuffledRDD[K, V, V](self, part)</span><br><span class="line">    .setKeyOrdering(<span class="keyword">if</span> (ascending) ordering <span class="keyword">else</span> ordering.reverse)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>该函数返回的RDD一定是ShuffledRDD类型的，因为对源RDD进行排序，必须进行Shuffle操作，而Shuffle操作的结果RDD就是ShuffledRDD。其实这个函数的实现很优雅，里面用到了RangePartitioner，它可以使得相应的范围Key数据分到同一个partition中，然后内部用到了mapPartitions对每个partition中的数据进行排序，而每个partition中数据的排序用到了标准的sort机制，避免了大量数据的shuffle。</p>
<h5 id="groupbykey-reducebykey">groupByKey、reduceByKey</h5>
<p>示例：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">object WCApp &#123;</span><br><span class="line">  def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    </span><br><span class="line">    val sparkConf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val sc = <span class="keyword">new</span> SparkContext(sparkConf)</span><br><span class="line">    val words = <span class="built_in">Array</span>(<span class="string">"one"</span>, <span class="string">"two"</span>, <span class="string">"two"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>, <span class="string">"three"</span>)</span><br><span class="line">    val wordsRDD = sc.parallelize(words).map(<span class="function"><span class="params">word</span> =&gt;</span> (word, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    val wordsCountWithReduce = wordsRDD</span><br><span class="line">      .reduceByKey(_ + _)</span><br><span class="line">      .foreach(println)</span><br><span class="line">    </span><br><span class="line">    val wordsCountWithGroup = wordsRDD</span><br><span class="line">      .groupByKey()</span><br><span class="line">      .map(<span class="function"><span class="params">x</span> =&gt;</span> (x._1, x._2.sum))</span><br><span class="line">      .foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(two,<span class="number">2</span>)</span><br><span class="line">(one,<span class="number">1</span>)</span><br><span class="line">(three,<span class="number">3</span>)</span><br><span class="line">(two,<span class="number">2</span>)</span><br><span class="line">(one,<span class="number">1</span>)</span><br><span class="line">(three,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>虽然两个函数都能得出正确的结果， 但reduceByKey函数更适合使用在大数据集上。 这是因为它可以在每个分区移动数据之前将输出数据与一个共用的key结合（先本地聚合,再shuffle）。</li>
<li>groupByKey这个出来的是(key,Interator),需要对iteration的内容进行进一步的处理，当调用 groupByKey时，所有的键值对(key-value pair) 都会被移动,在网络上传输这些数据非常没必要，因此应避免使用 groupByKey。</li>
<li>数据量很大时，在使用 reduceByKey 和 groupByKey 时他们shuffle后的文件大小差别会被放大更多倍。</li>
</ul>
<h5 id="join-leftouterjoin-rightouterjoin-fullouterjoin">join、leftOuterJoin、rightOuterJoin、fullOuterJoin</h5>
<p>1.join<br>
join函数输出两个RDD中相同Key的所有项(类似于SQL中的inner join)。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; val a = sc.parallelize(<span class="built_in">Array</span>((<span class="string">"A"</span>,<span class="string">"a1"</span>),(<span class="string">"B"</span>,<span class="string">"b1"</span>),(<span class="string">"C"</span>,<span class="string">"c1"</span>),(<span class="string">"D"</span>,<span class="string">"d1"</span>),(<span class="string">"E"</span>,<span class="string">"e1"</span>),(<span class="string">"F"</span>,<span class="string">"f1"</span>)))</span><br><span class="line">a: org.apache.spark.rdd.RDD[(<span class="built_in">String</span>, <span class="built_in">String</span>)] = ParallelCollectionRDD[<span class="number">0</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br><span class="line"> </span><br><span class="line">scala&gt; val b = sc.parallelize(<span class="built_in">Array</span>((<span class="string">"A"</span>,<span class="string">"a2"</span>),(<span class="string">"B"</span>,<span class="string">"b2"</span>),(<span class="string">"C"</span>,<span class="string">"c1"</span>),(<span class="string">"C"</span>,<span class="string">"c2"</span>),(<span class="string">"C"</span>,<span class="string">"c3"</span>),(<span class="string">"E"</span>,<span class="string">"e2"</span>)))</span><br><span class="line">b: org.apache.spark.rdd.RDD[(<span class="built_in">String</span>, <span class="built_in">String</span>)] = ParallelCollectionRDD[<span class="number">1</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br><span class="line"> </span><br><span class="line">scala&gt; a.join(b).collect    <span class="comment">// 这里的join是inner join，只返回左右都匹配上的内容</span></span><br><span class="line"> </span><br><span class="line">res1: <span class="built_in">Array</span>[(<span class="built_in">String</span>, (<span class="built_in">String</span>, <span class="built_in">String</span>))] = <span class="built_in">Array</span>((B,(b1,b2)), (A,(a1,a2)), (C,(c1,c1)), (C,(c1,c2)), (C,(c1,c3)), (E,(e1,e2)))</span><br><span class="line"> </span><br><span class="line">scala&gt; b.join(a).collect    </span><br><span class="line">res2: <span class="built_in">Array</span>[(<span class="built_in">String</span>, (<span class="built_in">String</span>, <span class="built_in">String</span>))] = <span class="built_in">Array</span>((B,(b2,b1)), (A,(a2,a1)), (C,(c1,c1)), (C,(c2,c1)), (C,(c3,c1)), (E,(e2,e1)))</span><br></pre></td></tr></table></figure>
<p>2.leftOuterJoin</p>
<ul>
<li>是以左边为基准，会保留对象的所有key</li>
<li>左边（a）的记录一定会存在，右边（b）的记录有的返回Some(x)，没有的补None。</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; a.leftOuterJoin(b).collect</span><br><span class="line">res3: <span class="built_in">Array</span>[(<span class="built_in">String</span>, (<span class="built_in">String</span>, Option[<span class="built_in">String</span>]))] = <span class="built_in">Array</span>((B,(b1,Some(b2))), (F,(f1,None)), (D,(d1,None)), (A,(a1,Some(a2))), (C,(c1,Some(c1))), (C,(c1,Some(c2))), (C,(c1,Some(c3))), (E,(e1,Some(e2))))</span><br><span class="line"> </span><br><span class="line">scala&gt; b.leftOuterJoin(a).collect</span><br><span class="line">res5: <span class="built_in">Array</span>[(<span class="built_in">String</span>, (<span class="built_in">String</span>, Option[<span class="built_in">String</span>]))] = <span class="built_in">Array</span>((B,(b2,Some(b1))), (A,(a2,Some(a1))), (C,(c1,Some(c1))), (C,(c2,Some(c1))), (C,(c3,Some(c1))), (E,(e2,Some(e1))))</span><br></pre></td></tr></table></figure>
<p>3.rightOuterJoin</p>
<ul>
<li>是以右边为基准，保留参数对象的所有key</li>
<li>右边（b）的记录一定会存在，左边（a）的记录有的返回Some(x)，没有的补None。</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; a.rightOuterJoin(b).collect</span><br><span class="line">res4: <span class="built_in">Array</span>[(<span class="built_in">String</span>, (Option[<span class="built_in">String</span>], <span class="built_in">String</span>))] = <span class="built_in">Array</span>((B,(Some(b1),b2)), (A,(Some(a1),a2)), (C,(Some(c1),c1)), (C,(Some(c1),c2)), (C,(Some(c1),c3)), (E,(Some(e1),e2)))</span><br><span class="line"> </span><br><span class="line">scala&gt; b.rightOuterJoin(a).collect</span><br><span class="line">res6: <span class="built_in">Array</span>[(<span class="built_in">String</span>, (Option[<span class="built_in">String</span>], <span class="built_in">String</span>))] = <span class="built_in">Array</span>((B,(Some(b2),b1)), (F,(None,f1)), (D,(None,d1)), (A,(Some(a2),a1)), (C,(Some(c1),c1)), (C,(Some(c2),c1)), (C,(Some(c3),c1)), (E,(Some(e2),e1)))</span><br></pre></td></tr></table></figure>
<p>4.fullOuterJoin</p>
<ul>
<li>保留两个RDD中的所有key</li>
<li>所有的值列都可能出现缺失的情况，所有所有值列都转换为Option对象</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">scala&gt; val a = sc.parallelize(<span class="built_in">Array</span>((<span class="string">"A"</span>,<span class="string">"a1"</span>),(<span class="string">"B"</span>,<span class="string">"b1"</span>),(<span class="string">"C"</span>,<span class="string">"c1"</span>),(<span class="string">"D"</span>,<span class="string">"d1"</span>),(<span class="string">"E"</span>,<span class="string">"e1"</span>),(<span class="string">"F"</span>,<span class="string">"f1"</span>)))</span><br><span class="line">a: org.apache.spark.rdd.RDD[(<span class="built_in">String</span>, <span class="built_in">String</span>)] = ParallelCollectionRDD[<span class="number">49</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br><span class="line"> </span><br><span class="line">scala&gt; val b = sc.parallelize(<span class="built_in">Array</span>((<span class="string">"A"</span>,<span class="string">"a2"</span>),(<span class="string">"B"</span>,<span class="string">"b2"</span>),(<span class="string">"C"</span>,<span class="string">"c1"</span>),(<span class="string">"C"</span>,<span class="string">"c2"</span>),(<span class="string">"C"</span>,<span class="string">"c3"</span>),(<span class="string">"E"</span>,<span class="string">"e2"</span>)))</span><br><span class="line">b: org.apache.spark.rdd.RDD[(<span class="built_in">String</span>, <span class="built_in">String</span>)] = ParallelCollectionRDD[<span class="number">50</span>] at parallelize at &lt;<span class="built_in">console</span>&gt;:<span class="number">24</span></span><br><span class="line"> </span><br><span class="line">scala&gt; a.fullOuterJoin(b).collect</span><br><span class="line">res15: <span class="built_in">Array</span>[(<span class="built_in">String</span>, (Option[<span class="built_in">String</span>], Option[<span class="built_in">String</span>]))] = <span class="built_in">Array</span>((B,(Some(b1),Some(b2))), (F,(Some(f1),None)), (D,(Some(d1),None)), (A,(Some(a1),Some(a2))), (C,(Some(c1),Some(c1))), (C,(Some(c1),Some(c2))), (C,(Some(c1),Some(c3))), (E,(Some(e1),Some(e2))))</span><br><span class="line"> </span><br><span class="line">scala&gt; b.fullOuterJoin(a).collect</span><br><span class="line">res16: <span class="built_in">Array</span>[(<span class="built_in">String</span>, (Option[<span class="built_in">String</span>], Option[<span class="built_in">String</span>]))] = <span class="built_in">Array</span>((B,(Some(b2),Some(b1))), (F,(None,Some(f1))), (D,(None,Some(d1))), (A,(Some(a2),Some(a1))), (C,(Some(c1),Some(c1))), (C,(Some(c2),Some(c1))), (C,(Some(c3),Some(c1))), (E,(Some(e2),Some(e1))))</span><br></pre></td></tr></table></figure>
<h5 id="coalesce-repartition">coalesce、repartition</h5>
<p>repartition(numPartitions:Int):RDD[T]和coalesce(numPartitions:Int，shuffle:Boolean=false):RDD[T]，<br>
他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的简易实现，（假设RDD有N个分区，需要重新划分成M个分区）：</p>
<ul>
<li>如果N&lt;M，一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true。</li>
<li>如果N&gt;M并且N和M相差不多，(假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false，在shuffl为false的情况下，如果M&gt;N时，coalesce为无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系。</li>
<li>如果N&gt;M并且两者相差悬殊，这时如果将shuffle设置为false，父子RDD是窄依赖关系，他们同处在一个Stage中，就可能造成spark程序的并行度不够，从而影响性能，如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以讲shuffle设置为true。</li>
</ul>
<p>总之：如果shuff为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的分区数变多的。</p>
</div><div class="post-copyright"><blockquote><p>原文作者: 暗也橙子</p><p>原文链接: <a href="https://zzuuriel.github.io/大数据技术/Spark Core基础-常用算子/">https://zzuuriel.github.io/大数据技术/Spark Core基础-常用算子/</a></p><p>版权声明: 转载请注明出处(必须保留作者署名及链接)</p></blockquote></div><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/大数据技术/Spark Core之读写数据/" class="pre">Spark Core之读写数据</a><a href="/Scala编程语言/Scala模式匹配/" class="next">Scala模式匹配</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-core基础-常用算子"><span class="toc-text">Spark Core&#x57FA;&#x7840;-&#x5E38;&#x7528;&#x7B97;&#x5B50;</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#常用算子"><span class="toc-text">&#x5E38;&#x7528;&#x7B97;&#x5B50;</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#map-mappartition-mappartitionwithindex"><span class="toc-text">map&#x3001;mapPartition&#x3001;mapPartitionWithIndex</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#zip-zipwithindex"><span class="toc-text">zip&#x3001;zipWithIndex</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#union-distinct-intersection-substract-cartesian"><span class="toc-text">union&#x3001;distinct&#x3001;intersection&#x3001;substract&#x3001;cartesian</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#sortby-sortbykey"><span class="toc-text">sortBy&#x3001;sortByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#groupbykey-reducebykey"><span class="toc-text">groupByKey&#x3001;reduceByKey</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#join-leftouterjoin-rightouterjoin-fullouterjoin"><span class="toc-text">join&#x3001;leftOuterJoin&#x3001;rightOuterJoin&#x3001;fullOuterJoin</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#coalesce-repartition"><span class="toc-text">coalesce&#x3001;repartition</span></a></li></ol></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/大数据技术/修改Flume源码使taildir source支持递归（可配置）/">修改Flume源码使taildir source支持递归（可配置）</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/解决Spark on YARN时jar包乱飞的问题/">解决Spark on YARN时jar包乱飞的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink之Watermark理解/">Flink之Watermark理解</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/"> Spark-2.4.5源码编译</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink自定义触发器/">Flink自定义触发器</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.深入理解Spark算子aggregate/">深入理解Spark算子aggregate</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.Kudu常用Api(java)/">Kudu常用Api(java) </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.CentOS7安装单机版Kudu/">CentOS7安装单机版Kudu </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/HBase架构和读写流程/">HBase架构和读写流程</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/HBase的Rowkey设计/">HBase的Rowkey设计</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux基础/">Linux基础</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala编程语言/">Scala编程语言</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据技术/">大数据技术</a><span class="category-list-count">37</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Kudu/" style="font-size: 15px;">Kudu</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/Flume/" style="font-size: 15px;">Flume</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">42</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">5</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"> <div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">暗也橙子.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/zzuUriel/zzuUriel.github.io"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>