<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划 | 暗也橙子Blog</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划</h1><a id="logo" href="/.">暗也橙子Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划</h1><div class="post-meta"><a href="/大数据技术/数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划/#comments" class="comment-count"></a><p><span class="date">Jul 22, 2019</span><span><a href="/categories/大数据技术/" class="category">大数据技术</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="数据列自动推导amp数据错误执行模式ampudafampudtfamp解读spark-sql执行计划">数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划</h3>
<hr>
<h4 id="数据列自动推导">数据列自动推导</h4>
<p>1.准备一份数据</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a|b|c</span><br><span class="line"><span class="number">1</span>|<span class="number">2</span>|<span class="number">3</span></span><br><span class="line"><span class="number">4</span>|uriel|<span class="number">6</span></span><br><span class="line"><span class="number">7</span>|<span class="number">8</span>|<span class="number">9.0</span></span><br></pre></td></tr></table></figure>
<p>2.代码测试</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">  val spark = SparkSession</span><br><span class="line">    .builder()</span><br><span class="line">    .master(<span class="string">"local[2]"</span>)</span><br><span class="line">    .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    .getOrCreate()</span><br><span class="line">   </span><br><span class="line">  val csvDF: DataFrame = spark.read</span><br><span class="line">    .format(<span class="string">"csv"</span>)</span><br><span class="line">    .option(<span class="string">"header"</span>,<span class="string">"true"</span>)</span><br><span class="line">    .option(<span class="string">"sep"</span>,<span class="string">"|"</span>)</span><br><span class="line">    .option(<span class="string">"interSchema"</span>,<span class="string">"true"</span>)</span><br><span class="line">    .load(<span class="string">"ruozedata-spark-sql/data/test.csv"</span>)</span><br><span class="line">   </span><br><span class="line">  csvDF.printSchema()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.打印数据Schema信息</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- a: integer (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- b: string (nullable = <span class="literal">true</span>)</span><br><span class="line"> |-- c: double (nullable = <span class="literal">true</span>)</span><br></pre></td></tr></table></figure>
<h4 id="数据错误执行模式">数据错误执行模式</h4>
<p>在Spark SQL中，读取数据时，遇到错误数据或者脏数据时，我们可以使用option设置mode，处理些数据的模式有3种，分别为PERMISSIVE（默认处理方式）、DROPMALFORMED（丢弃数据）、FAILFAST（快速失败），这些模式可以在ParseMode.scala源码中查看。<br>
1.准备一份数据test.json</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"a"</span>:<span class="number">1</span>,<span class="string">"b"</span>:<span class="number">2</span>,<span class="string">"c"</span>:<span class="number">3</span>&#125;</span><br><span class="line">&#123;<span class="string">"a"</span>:<span class="number">4</span>,:<span class="number">5</span>,<span class="string">"c"</span>:<span class="number">6</span>&#125;</span><br><span class="line">&#123;<span class="string">"a"</span>:<span class="number">7</span>,<span class="string">"b"</span>:<span class="number">8</span>,<span class="string">"c"</span>:<span class="number">9</span>&#125;</span><br></pre></td></tr></table></figure>
<p>2.默认模式<br>
1）读取数据</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val jsonDF: DataFrame = spark.read.json(<span class="string">"ruozedata-spark-sql/data/test.json"</span>)</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure>
<p>2）运行结果：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">+----------------+----+----+----+</span><br><span class="line">| _corrupt_record|   a|   b|   c|</span><br><span class="line">+----------------+----+----+----+</span><br><span class="line">|            <span class="literal">null</span>|   <span class="number">1</span>|   <span class="number">2</span>|   <span class="number">3</span>|</span><br><span class="line">|&#123;<span class="string">"a"</span>:<span class="number">4</span>,:<span class="number">5</span>,<span class="string">"c"</span>:<span class="number">6</span>&#125;|<span class="literal">null</span>|<span class="literal">null</span>|<span class="literal">null</span>|</span><br><span class="line">|            <span class="literal">null</span>|   <span class="number">7</span>|   <span class="number">8</span>|   <span class="number">9</span>|</span><br><span class="line">+----------------+----+----+----+</span><br></pre></td></tr></table></figure>
<p>如果没有在option中设置mode选项，默认为PERMISSIVE，通过_corrupt_record列打印出错误信息<br>
3.使用option设置mode为DROPMALFORMED，如果碰到错误的数据，则自动丢弃</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val jsonDF: DataFrame = spark.read.option(<span class="string">"mode"</span>,<span class="string">"DROPMALFORMED"</span>).json(<span class="string">"ruozedata-spark-sql/data/test.json"</span>)</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+---+---+---+</span><br><span class="line">|  a|  b|  c|</span><br><span class="line">+---+---+---+</span><br><span class="line">|  <span class="number">1</span>|  <span class="number">2</span>|  <span class="number">3</span>|</span><br><span class="line">|  <span class="number">7</span>|  <span class="number">8</span>|  <span class="number">9</span>|</span><br><span class="line">+---+---+---+</span><br></pre></td></tr></table></figure>
<h4 id="自定义udaf函数">自定义UDAF函数</h4>
<p>1.自定义一个UDAF的class或者object，作为具体的逻辑实现，需要继承UserDefinedAggregateFunction</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">object AgeAvgUDAF extends UserDefinedAggregateFunction&#123;</span><br><span class="line">    <span class="comment">//输入类型</span></span><br><span class="line">    override def inputSchema: StructType = StructType(</span><br><span class="line">      StructField(<span class="string">"input"</span>,DoubleType,<span class="literal">true</span>)::Nil</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//聚合内部中的buffer类型</span></span><br><span class="line">    override def bufferSchema: StructType = StructType(</span><br><span class="line">      StructField(<span class="string">"sums"</span>,DoubleType,<span class="literal">true</span>)::        <span class="comment">//年龄和</span></span><br><span class="line">      StructField(<span class="string">"num"</span>,LongType,<span class="literal">true</span>)::Nil        <span class="comment">//人数</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//输入数据类型</span></span><br><span class="line">    override def dataType: DataType = DoubleType</span><br><span class="line"></span><br><span class="line">    <span class="comment">//输入数据类型是否和输出数据类型相等</span></span><br><span class="line">    override def deterministic: <span class="built_in">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//聚合内部buffer的初始化</span></span><br><span class="line">    override def initialize(buffer: MutableAggregationBuffer): Unit = &#123;</span><br><span class="line">      buffer(<span class="number">0</span>) = <span class="number">0.0</span></span><br><span class="line">      buffer(<span class="number">1</span>) = <span class="number">0</span>L</span><br><span class="line">	  <span class="comment">//buffer.update(0,0.0)</span></span><br><span class="line">	  <span class="comment">//buffer.update(1,0L) </span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区内更新聚合buffer</span></span><br><span class="line">    override def update(buffer: MutableAggregationBuffer, <span class="attr">input</span>: Row): Unit = &#123;</span><br><span class="line">      buffer.update(<span class="number">0</span>,buffer.getDouble(<span class="number">0</span>)+input.getDouble(<span class="number">0</span>))</span><br><span class="line">      buffer.update(<span class="number">1</span>,buffer.getLong(<span class="number">1</span>)+<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//分区间合并</span></span><br><span class="line">    override def merge(buffer1: MutableAggregationBuffer, <span class="attr">buffer2</span>: Row): Unit = &#123;</span><br><span class="line">      buffer1.update(<span class="number">0</span>,buffer1.getDouble(<span class="number">0</span>)+buffer2.getDouble(<span class="number">0</span>))</span><br><span class="line">      buffer1.update(<span class="number">1</span>,buffer1.getLong(<span class="number">1</span>)+buffer2.getLong(<span class="number">1</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//最终计算</span></span><br><span class="line">    override def evaluate(buffer: Row): Any = &#123;</span><br><span class="line">      buffer.getDouble(<span class="number">0</span>)/buffer.getLong(<span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>2.注册并使用UDAF</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">  val spark = SparkSession</span><br><span class="line">    .builder()</span><br><span class="line">    .master(<span class="string">"local[2]"</span>)</span><br><span class="line">    .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义数据源</span></span><br><span class="line">  val list = <span class="keyword">new</span> util.ArrayList[Row]()</span><br><span class="line">  list.add(Row(<span class="string">"pk"</span>,<span class="number">28</span>,<span class="string">"男"</span>))</span><br><span class="line">  list.add(Row(<span class="string">"xingxing"</span>,<span class="number">60</span>,<span class="string">"男"</span>))</span><br><span class="line">  list.add(Row(<span class="string">"J哥"</span>,<span class="number">18</span>,<span class="string">"男"</span>))</span><br><span class="line">  list.add(Row(<span class="string">"ailsa"</span>,<span class="number">18</span>,<span class="string">"女"</span>))</span><br><span class="line">  list.add(Row(<span class="string">"laura"</span>,<span class="number">26</span>,<span class="string">"女"</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义Schema</span></span><br><span class="line">  val schema = StructType(</span><br><span class="line">    StructField(<span class="string">"name"</span>, StringType, <span class="literal">true</span>)::</span><br><span class="line">    StructField(<span class="string">"age"</span>, IntegerType, <span class="literal">true</span>)::</span><br><span class="line">    StructField(<span class="string">"sex"</span>, StringType, <span class="literal">true</span>)::Nil</span><br><span class="line">  )</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//创建df</span></span><br><span class="line">  val df = spark.createDataFrame(list, schema)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//创建视图</span></span><br><span class="line">  df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//注册UDAF</span></span><br><span class="line">  spark.udf.register(<span class="string">"age_avg_udaf"</span>,AgeAvgUDAF)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//使用UDAF</span></span><br><span class="line">  spark.sql(<span class="string">"select sex,age_avg_udaf(age) as ave_age from people group by sex"</span>).show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.运行结果</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+---+---------+</span><br><span class="line">|sex|  ave_age|</span><br><span class="line">+---+---------+</span><br><span class="line">| 男|    <span class="number">35.33</span>|</span><br><span class="line">| 女|     <span class="number">22.0</span>|</span><br><span class="line">+---+---------+</span><br></pre></td></tr></table></figure>
<h4 id="自定义udtf函数">自定义UDTF函数</h4>
<p>1.示例</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">object ExplodeUDTF &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val spark = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .appName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 自定义schema  </span></span><br><span class="line">    val schema = StructType(</span><br><span class="line">      StructField(<span class="string">"teacher"</span>, StringType, <span class="literal">true</span>) ::</span><br><span class="line">      StructField(<span class="string">"sources"</span>, StringType, <span class="literal">true</span>) :: Nil</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 自定义数据源</span></span><br><span class="line">    val list = <span class="keyword">new</span> util.ArrayList[Row]()</span><br><span class="line">    list.add(Row(<span class="string">"pk"</span>, <span class="string">"hive,spark,flink"</span>))</span><br><span class="line">    list.add(Row(<span class="string">"J哥"</span>, <span class="string">"cdh,kafka,hbase"</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建临时视图</span></span><br><span class="line">    val df = spark.createDataFrame(list, schema)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">// 使用flatMap拆分</span></span><br><span class="line">    df.flatMap(<span class="function"><span class="params">x</span> =&gt;</span> &#123;</span><br><span class="line">      val line = <span class="keyword">new</span> ListBuffer[(<span class="built_in">String</span>, <span class="built_in">String</span>)]()</span><br><span class="line">      val sources = x.getString(<span class="number">1</span>).split(<span class="string">","</span>)</span><br><span class="line">      <span class="keyword">for</span> (source &lt;- sources)&#123;</span><br><span class="line">        line.append((x.getString(<span class="number">0</span>),source))</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">//返回</span></span><br><span class="line">      line</span><br><span class="line">    &#125;).toDF(<span class="string">"teacher"</span>,<span class="string">"source"</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>2.运行结果</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">|teacher|source|</span><br><span class="line">+-------+------+</span><br><span class="line">|     PK|  hive|</span><br><span class="line">|     PK| spark|</span><br><span class="line">|     PK| flink|</span><br><span class="line">|    J哥|   cdh|</span><br><span class="line">|    J哥| kafka|</span><br><span class="line">|    J哥| hbase|</span><br><span class="line">+-------+------+</span><br></pre></td></tr></table></figure>
<h4 id="解读spark-sql执行计划优化">解读Spark SQL执行计划优化</h4>
<p>1.建空表</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table sqltest (key string,value string)</span><br></pre></td></tr></table></figure>
<p>2.执行SQL</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain extended select a.key*(<span class="number">3</span>*<span class="number">5</span>),b.value <span class="keyword">from</span> sqltest a join sqltest b on a.key=b.key and a.key &gt;<span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<p>3.解读执行计划</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 解析逻辑计划，做些简单的解析</span></span><br><span class="line">== Parsed Logical Plan ==</span><br><span class="line"><span class="string">'Project [unresolvedalias(('</span>a.key * (<span class="number">3</span> * <span class="number">5</span>)), None), <span class="string">'b.value]</span></span><br><span class="line"><span class="string">+- '</span>Join Inner, ((<span class="string">'a.key = '</span>b.key) &amp;&amp; (<span class="string">'a.key &gt; 3))</span></span><br><span class="line"><span class="string">   :- '</span>SubqueryAlias <span class="string">`a`</span></span><br><span class="line">   :  +- <span class="string">'UnresolvedRelation `sqltest`</span></span><br><span class="line"><span class="string">   +- '</span>SubqueryAlias <span class="string">`b`</span></span><br><span class="line">      +- <span class="string">'UnresolvedRelation `sqltest`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 分析逻辑计划，解析出了数据类型，拿到数据库和表，拿到了序列化方式                           </span></span><br><span class="line"><span class="string">== Analyzed Logical Plan ==</span></span><br><span class="line"><span class="string">(CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE)): double, value: string</span></span><br><span class="line"><span class="string">Project [(cast(key#2 as double) * cast((3 * 5) as double)) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line"><span class="string">+- Join Inner, ((key#2 = key#4) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line"><span class="string">   :- SubqueryAlias `a`</span></span><br><span class="line"><span class="string">   :  +- SubqueryAlias `default`.`sqltest`</span></span><br><span class="line"><span class="string">   :     +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3]</span></span><br><span class="line"><span class="string">   +- SubqueryAlias `b`</span></span><br><span class="line"><span class="string">      +- SubqueryAlias `default`.`sqltest`</span></span><br><span class="line"><span class="string">         +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 优化逻辑计划，数值类型的运算直接拿到结果，解析过滤条件</span></span><br><span class="line"><span class="string">== Optimized Logical Plan ==</span></span><br><span class="line"><span class="string">Project [(cast(key#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line"><span class="string">+- Join Inner, (key#2 = key#4)</span></span><br><span class="line"><span class="string">   :- Project [key#2]</span></span><br><span class="line"><span class="string">   :  +- Filter (isnotnull(key#2) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line"><span class="string">   :     +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3]</span></span><br><span class="line"><span class="string">   +- Filter ((cast(key#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4))</span></span><br><span class="line"><span class="string">      +- HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">// 物理计划，join方式为SortMergeJoin，数据使用hashpartitioning保存，扫描表的方式是HiveTableRelation</span></span><br><span class="line"><span class="string">== Physical Plan ==</span></span><br><span class="line"><span class="string">*(5) Project [(cast(key#2 as double) * 15.0) AS (CAST(key AS DOUBLE) * CAST((3 * 5) AS DOUBLE))#6, value#5]</span></span><br><span class="line"><span class="string">+- *(5) SortMergeJoin [key#2], [key#4], Inner</span></span><br><span class="line"><span class="string">   :- *(2) Sort [key#2 ASC NULLS FIRST], false, 0</span></span><br><span class="line"><span class="string">   :  +- Exchange hashpartitioning(key#2, 200)</span></span><br><span class="line"><span class="string">   :     +- *(1) Filter (isnotnull(key#2) &amp;&amp; (cast(key#2 as int) &gt; 3))</span></span><br><span class="line"><span class="string">   :        +- Scan hive default.sqltest [key#2], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#2, value#3]</span></span><br><span class="line"><span class="string">   +- *(4) Sort [key#4 ASC NULLS FIRST], false, 0</span></span><br><span class="line"><span class="string">      +- Exchange hashpartitioning(key#4, 200)</span></span><br><span class="line"><span class="string">         +- *(3) Filter ((cast(key#4 as int) &gt; 3) &amp;&amp; isnotnull(key#4))</span></span><br><span class="line"><span class="string">            +- Scan hive default.sqltest [key#4, value#5], HiveTableRelation `default`.`sqltest`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [key#4, value#5]</span></span><br></pre></td></tr></table></figure>
<p>可以简单的看做四步，分别是解析逻辑计划、分析逻辑计划、优化逻辑计划、物理执行计划</p>
</div><div class="post-copyright"><blockquote><p>原文作者: 暗也橙子</p><p>原文链接: <a href="https://zzuuriel.github.io/大数据技术/数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划/">https://zzuuriel.github.io/大数据技术/数据列自动推导&amp;数据错误执行模式&amp;UDAF&amp;UDTF&amp;解读Spark SQL执行计划/</a></p><p>版权声明: 转载请注明出处(必须保留作者署名及链接)</p></blockquote></div><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/大数据技术/Spark SQL从jdbc的角度解读外部数据源/" class="pre">Spark SQL-从jdbc的角度解读外部数据源</a><a href="/大数据技术/Spark SQL之RDD转换DataFrame&amp;DF和DS的转换&amp;跨数据源操作&amp;用SQL方式操作数据源&amp;元数据catalog/" class="next">Spark SQL之RDD转换DataFrame&amp;DF和DS的转换&amp;跨数据源操作&amp;用SQL方式操作数据源&amp;元数据catalog</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据列自动推导amp数据错误执行模式ampudafampudtfamp解读spark-sql执行计划"><span class="toc-text">&#x6570;&#x636E;&#x5217;&#x81EA;&#x52A8;&#x63A8;&#x5BFC;&amp;&#x6570;&#x636E;&#x9519;&#x8BEF;&#x6267;&#x884C;&#x6A21;&#x5F0F;&amp;UDAF&amp;UDTF&amp;&#x89E3;&#x8BFB;Spark SQL&#x6267;&#x884C;&#x8BA1;&#x5212;</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#数据列自动推导"><span class="toc-text">&#x6570;&#x636E;&#x5217;&#x81EA;&#x52A8;&#x63A8;&#x5BFC;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#数据错误执行模式"><span class="toc-text">&#x6570;&#x636E;&#x9519;&#x8BEF;&#x6267;&#x884C;&#x6A21;&#x5F0F;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#自定义udaf函数"><span class="toc-text">&#x81EA;&#x5B9A;&#x4E49;UDAF&#x51FD;&#x6570;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#自定义udtf函数"><span class="toc-text">&#x81EA;&#x5B9A;&#x4E49;UDTF&#x51FD;&#x6570;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#解读spark-sql执行计划优化"><span class="toc-text">&#x89E3;&#x8BFB;Spark SQL&#x6267;&#x884C;&#x8BA1;&#x5212;&#x4F18;&#x5316;</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/大数据技术/修改Flume源码使taildir source支持递归（可配置）/">修改Flume源码使taildir source支持递归（可配置）</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/解决Spark on YARN时jar包乱飞的问题/">解决Spark on YARN时jar包乱飞的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink之Watermark理解/">Flink之Watermark理解</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/"> Spark-2.4.5源码编译</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink自定义触发器/">Flink自定义触发器</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.深入理解Spark算子aggregate/">深入理解Spark算子aggregate</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.Kudu常用Api(java)/">Kudu常用Api(java) </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.CentOS7安装单机版Kudu/">CentOS7安装单机版Kudu </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/HBase架构和读写流程/">HBase架构和读写流程</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/HBase的Rowkey设计/">HBase的Rowkey设计</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux基础/">Linux基础</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala编程语言/">Scala编程语言</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据技术/">大数据技术</a><span class="category-list-count">35</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Kudu/" style="font-size: 15px;">Kudu</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Flume/" style="font-size: 15px;">Flume</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">40</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">2</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"> <div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">暗也橙子.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/zzuUriel/zzuUriel.github.io"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>