<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><link rel="stylesheet" type="text/css" href="//fonts.loli.net/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style-dark.css?v=2.0.5"><link rel="stylesheet" type="text/css" href="/css/highlight-dark.css?v=2.0.5"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><title>Spark Streaming基础 | 暗也橙子Blog</title></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Spark Streaming基础</h1><a id="logo" href="/.">暗也橙子Blog</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="搜索"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">Spark Streaming基础</h1><div class="post-meta"><a href="/大数据技术/Spark Streaming基础/#comments" class="comment-count"></a><p><span class="date">Aug 01, 2019</span><span><a href="/categories/大数据技术/" class="category">大数据技术</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><h3 id="spark-streaming简介ampspark-streaming的内部结构ampstreamingcontext对象amp离散流dstreamampspark-streaming开发示例ampstate维护">Spark Streaming简介&amp;Spark Streaming的内部结构&amp;StreamingContext对象&amp;离散流（DStream）&amp;Spark Streaming开发示例&amp;state维护</h3>
<hr>
<h4 id="spark-streaming简介">Spark Streaming简介</h4>
<p>Spark Streaming是核心Spark API的扩展，可实现可扩展、高吞吐量、可容错的实时数据流处理。数据可以从诸如Kafka，Flume，Kinesis或TCP套接字等众多来源获取，并且可以使用由高级函数（如map，reduce，join和window）开发的复杂算法进行流数据处理。最后，处理后的数据可以被推送到文件系统，数据库和实时仪表板。而且还可以在数据流上应用Spark提供的机器学习和图处理算法。<br>
<img src="/img/spark/what-is-spark-streaming.png" alt="what-is-spark-streaming.png"></p>
<h4 id="spark-streaming的内部结构">Spark Streaming的内部结构</h4>
<p>在内部，它的工作原理如下。Spark Streaming接收实时输入数据流，并将数据切分成批，然后由Spark引擎对其进行处理，最后生成“批”形式的结果流。<br>
<img src="/img/spark/streaming-batch-process.png" alt="streaming-batch-process.png"><br>
Spark Streaming将连续的数据流抽象为discretizedstream(DStream)。在内部，DStream由一个RDD序列表示。</p>
<h4 id="streamingcontext对象">StreamingContext对象</h4>
<p>初始化StreamingContext：<br>
方式一，从SparkConf对象中创建：</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建一个Context对象：StreamingContext</span></span><br><span class="line">val conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"MyNetworkWordCount"</span>).setMaster(<span class="string">"local[2]"</span>)</span><br><span class="line"><span class="comment">//指定批处理的时间间隔</span></span><br><span class="line">val ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<p>方式二，从现有的SparkContext实例中创建</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val ssc = <span class="keyword">new</span> StreamingContext(sc, Seconds(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>说明：</p>
<ul>
<li>appName参数是应用程序在集群UI上显示的名称。</li>
<li>master是Spark，Mesos或YARN集群的URL，或者一个特殊的“local [*]”字符串来让程序以本地模式运行。</li>
<li>当在集群上运行程序时，不需要在程序中硬编码master参数，而是使用spark-submit提交应用程序并将master的URL以脚本参数的形式传入。但是，对于本地测试和单元测试，您可以通过“local[*]”来运行Spark Streaming程序（请确保本地系统中的cpu核心数够用）。</li>
<li>StreamingContext会内在的创建一个SparkContext的实例（所有Spark功能的起始点），你可以通过ssc.sparkContext访问到这个实例。</li>
<li>批处理的时间窗口长度必须根据应用程序的延迟要求和可用的集群资源进行设置。</li>
</ul>
<p>注意：</p>
<ul>
<li>一旦一个StreamingContext开始运作，就不能设置或添加新的流计算。</li>
<li>一旦一个上下文被停止，它将无法重新启动。</li>
<li>同一时刻，一个JVM中只能有一个StreamingContext处于活动状态。</li>
<li>StreamingContext上的stop()方法也会停止SparkContext。 要仅停止StreamingContext（保持SparkContext活跃），请将stop() 方法的可选参数stopSparkContext设置为false。</li>
<li>只要前一个StreamingContext在下一个StreamingContext被创建之前停止（不停止SparkContext），SparkContext就可以被重用来创建多个StreamingContext。</li>
</ul>
<h4 id="离散流dstream">离散流（DStream）</h4>
<p>DiscretizedStream(DStream) 是Spark Streaming对流式数据的基本抽象。它表示连续的数据流，这些连续的数据流可以是从数据源接收的输入数据流，也可以是通过对输入数据流执行转换操作而生成的经处理的数据流。在内部，DStream由一系列连续的RDD表示，如下图：<br>
<img src="/img/spark/streaming-dstream-1.png" alt="streaming-dstream-1.png"><br>
我们将一行行文本组成的流转换为单词流，具体做法为：将flatMap操作应用于名为lines的 DStream中的每个RDD上，以生成words DStream的RDD。如下图所示：<br>
<img src="/img/spark/streaming-dstream-2.png" alt="streaming-dstream-2.png"><br>
但是DStream和RDD也有区别，下面画图说明：<br>
RDD的结构：<br>
<img src="/img/spark/streaming-dstream-3.png" alt="streaming-dstream-3.png"><br>
DStream的结构：<br>
<img src="/img/spark/streaming-dstream-4.png" alt="streaming-dstream-4.png"></p>
<h4 id="spark-streaming开发示例">Spark Streaming开发示例</h4>
<p>1.要编写自己的Spark流程序，必须将以下依赖项添加到Maven项目中。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;<span class="regexp">/groupId&gt;</span></span><br><span class="line"><span class="regexp">    &lt;artifactId&gt;spark-streaming_2.12&lt;/</span>artifactId&gt;</span><br><span class="line">    &lt;version&gt;<span class="number">2.4</span><span class="number">.5</span>&lt;<span class="regexp">/version&gt;</span></span><br><span class="line"><span class="regexp">    &lt;scope&gt;provided&lt;/</span>scope&gt;</span><br><span class="line">&lt;<span class="regexp">/dependency&gt;</span></span><br></pre></td></tr></table></figure>
<p>2.SocketFile实现简单的词频统计</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到StreamingContext对象</span></span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//开启StreamingContext</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private def dispose(ssc: StreamingContext) = &#123;</span><br><span class="line">    <span class="comment">//输入记录</span></span><br><span class="line">    val lines = ssc.socketTextStream(<span class="string">"hadoop000"</span>, <span class="number">9998</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//逻辑处理</span></span><br><span class="line">    val words = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    val pair = words.map(<span class="function"><span class="params">x</span> =&gt;</span> (x, <span class="number">1</span>))</span><br><span class="line">    val result = pair.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//输出记录</span></span><br><span class="line">    result.print()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.使用nc发送消息</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ nc -lk <span class="number">9100</span></span><br><span class="line"></span><br><span class="line"><span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>4.客户端接收消息</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1357008430000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>DStreams 是表示从源端接收的输入数据的数据流。在这个简单的示例中，行是一个输入DStream，因为它表示从netcat服务器接收到的数据流。每个输入DStream(本节后面讨论的文件流除外)都与接收方(Scala doc、Java doc)对象相关联，接收方接收来自源的数据并将其存储在Spark内存中进行处理。</p>
<p>注意：Spark流应用程序需要分配足够的Core来处理接收到的数据，以及运行接收方。设置core的数量要大于Receivers的数量。</p>
<h4 id="checkpoint维护state">Checkpoint维护state</h4>
<h5 id="updatestatebykey维护state">updateStateByKey维护State</h5>
<p>1.一些基本概念<br>
什么是updateStateByKey?</p>
<ul>
<li>updateStateByKey(func)可以返回一个新“state”的DStream，其中通过对键的前一个状态和键的新值应用给定的函数来更新每个键的状态。这可以用来维护每个键的任意状态数据。</li>
</ul>
<p>什么是Checkpoint?</p>
<ul>
<li>Checkpoint可以通过在一个容错的、可靠的文件系统中设置一个目录来启用，Checkpoint信息将被保存到这个目录中。这是通过使用streamingContext.checkpoint(checkpointDirectory)实现的。</li>
</ul>
<p>2.下面案例也是词频统计，只不过带了state信息</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 处理逻辑</span></span><br><span class="line">private def dispose(ssc: StreamingContext) = &#123;</span><br><span class="line">    val lines = ssc.socketTextStream(<span class="string">"hadoop000"</span>, <span class="number">9998</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 设置checkpoint目录，保存offset</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"./chk"</span>)</span><br><span class="line">    <span class="comment">// updateStateByKey：维护记录的state</span></span><br><span class="line">    lines.flatMap(_.split(<span class="string">","</span>)).map((_, <span class="number">1</span>)).updateStateByKey(updateFunction)</span><br><span class="line">    .print()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现对新值和旧值的累加</span></span><br><span class="line">def updateFunction(newValues: Seq[Int], <span class="attr">oldValues</span>: Option[Int]): Option[Int] = &#123;</span><br><span class="line">    val curr = newValues.sum</span><br><span class="line">    val old = oldValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    val count = curr + old</span><br><span class="line">    Some(count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3.使用nc发送消息</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ nc -lk <span class="number">9998</span></span><br><span class="line"></span><br><span class="line">a a a b b c</span><br><span class="line">a a a b b c</span><br></pre></td></tr></table></figure>
<p>客户端接收消息</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1587439170000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,<span class="number">2</span>)</span><br><span class="line">(a,<span class="number">3</span>)</span><br><span class="line">(c,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: <span class="number">1587439175000</span> ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(b,<span class="number">4</span>)</span><br><span class="line">(a,<span class="number">6</span>)</span><br><span class="line">(c,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>上面的程序如果一直运行，结果可以一直累加，但是程序一旦停止运行，重新启动时，结果就不会接着上一次进行计算，主要原因上每次程序运行都会初始化一个程序入口，而两次运行的程序入口不是同一个入口，所以会导致第一次计算的结果丢失。</p>
<p>使用checkpoint方法可以把上一次Driver里面的运算结果状态保存在checkpoint的目录里面，我们在第二次启动程序时，就可以从checkpoint里面取出上一次的运行结果状态，把这次的Driver状态恢复成和上一次Driver一样的状态。</p>
<h5 id="updatestatebykey维护state优化版本">updateStateByKey维护State（优化版本）</h5>
<p>以下代码参考官网：<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-programming-guide.html#checkpointing</a><br>
如果想让应用程序从驱动程序故障中恢复，我们应该重写代码，让它具备下面的功能</p>
<ul>
<li>当程序第一次启动时，它将创建一个新的StreamingContext，设置所有的流，然后调用start()。</li>
<li>当程序在失败后重新启动时，它将从Checkpoint目录中的Checkpoint数据重新创建一个StreamingContext。</li>
</ul>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">val checkpoint = <span class="string">"./chk_v2"</span></span><br><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 拿到StreamingContext</span></span><br><span class="line">    val ssc  = StreamingContext.getOrCreate(checkpoint, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建StreamingContext带业务逻辑</span></span><br><span class="line">def functionToCreateContext(): StreamingContext = &#123;</span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(conf,Seconds(<span class="number">5</span>))   <span class="comment">// new context</span></span><br><span class="line"></span><br><span class="line">    dispose(ssc)</span><br><span class="line"></span><br><span class="line">    ssc.checkpoint(checkpoint)   <span class="comment">// set checkpoint directory</span></span><br><span class="line">    ssc</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 业务逻辑处理</span></span><br><span class="line">private def dispose(ssc: StreamingContext) = &#123;</span><br><span class="line">    val lines = ssc.socketTextStream(<span class="string">"hadoop000"</span>, <span class="number">9998</span>) <span class="comment">// create DStreams</span></span><br><span class="line"></span><br><span class="line">    lines</span><br><span class="line">    .flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">    .map((_, <span class="number">1</span>))</span><br><span class="line">    .updateStateByKey(updateFunction)</span><br><span class="line">    .print()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 更新state</span></span><br><span class="line">def updateFunction(newValues: Seq[Int], <span class="attr">oldValues</span>: Option[Int]): Option[Int] = &#123;</span><br><span class="line">    val curr = newValues.sum</span><br><span class="line">    val old = oldValues.getOrElse(<span class="number">0</span>)</span><br><span class="line">    val count = curr + old</span><br><span class="line">    Some(count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="mapwithstate维护state的方法">mapWithState维护State的方法</h5>
<p>官网中描述用updateStateByKey维护State是一种过时的方法，但并没有举例说明最新的方法，这种新方法就是mapWithState，我们可以在源码中找到它的使用方法。</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定checkpoint路径</span></span><br><span class="line">val checkpoint = <span class="string">"./chk_v3"</span></span><br><span class="line"></span><br><span class="line">def main(args: <span class="built_in">Array</span>[<span class="built_in">String</span>]): Unit = &#123;</span><br><span class="line">    <span class="comment">// 拿到 StreamingContext</span></span><br><span class="line">    val ssc = StreamingContext.getOrCreate(checkpoint, functionToCreateContext)</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建 StreamingContext</span></span><br><span class="line">def functionToCreateContext(): StreamingContext = &#123;</span><br><span class="line">    val conf = <span class="keyword">new</span> SparkConf().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">    val ssc = <span class="keyword">new</span> StreamingContext(conf, Seconds(<span class="number">5</span>))</span><br><span class="line">    ssc.checkpoint(checkpoint)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 对记录做累加操作</span></span><br><span class="line">    val mappingFunc = <span class="function">(<span class="params">word: <span class="built_in">String</span>, one: Option[Int], state: State[Int]</span>) =&gt;</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(state.isTimingOut())&#123;</span><br><span class="line">            println(<span class="string">"超时3秒没拿到数据"</span>)</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            val sum = one.getOrElse(<span class="number">0</span>) + state.getOption.getOrElse(<span class="number">0</span>)</span><br><span class="line">            val output = (word, sum)</span><br><span class="line">            state.update(sum)</span><br><span class="line">            output</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 业务逻辑处理</span></span><br><span class="line">    val lines = ssc.socketTextStream(<span class="string">"hadoop000"</span>, <span class="number">9998</span>)</span><br><span class="line">    lines</span><br><span class="line">    .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    .map((_,<span class="number">1</span>))</span><br><span class="line">    .mapWithState(StateSpec.function(mappingFunc)</span><br><span class="line">                  .timeout(Seconds(<span class="number">3</span>))</span><br><span class="line">                 ).print()</span><br><span class="line">				 </span><br><span class="line">    ssc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><div class="post-copyright"><blockquote><p>原文作者: 暗也橙子</p><p>原文链接: <a href="https://zzuuriel.github.io/大数据技术/Spark Streaming基础/">https://zzuuriel.github.io/大数据技术/Spark Streaming基础/</a></p><p>版权声明: 转载请注明出处(必须保留作者署名及链接)</p></blockquote></div><div class="tags"><a href="/tags/Spark/">Spark</a></div><div class="post-share"><div class="social-share"><span>分享到:</span></div></div><div class="post-nav"><a href="/大数据技术/Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka/" class="pre">Spark Streaming transform的使用&amp;闭包&amp;Spark Streaming对接Kafka</a><a href="/大数据技术/Spark SQL自定义外部text数据源/" class="next">Spark SQL-自定义外部text数据源</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-streaming简介ampspark-streaming的内部结构ampstreamingcontext对象amp离散流dstreamampspark-streaming开发示例ampstate维护"><span class="toc-text">Spark Streaming&#x7B80;&#x4ECB;&amp;Spark Streaming&#x7684;&#x5185;&#x90E8;&#x7ED3;&#x6784;&amp;StreamingContext&#x5BF9;&#x8C61;&amp;&#x79BB;&#x6563;&#x6D41;&#xFF08;DStream&#xFF09;&amp;Spark Streaming&#x5F00;&#x53D1;&#x793A;&#x4F8B;&amp;state&#x7EF4;&#x62A4;</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-streaming简介"><span class="toc-text">Spark Streaming&#x7B80;&#x4ECB;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-streaming的内部结构"><span class="toc-text">Spark Streaming&#x7684;&#x5185;&#x90E8;&#x7ED3;&#x6784;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#streamingcontext对象"><span class="toc-text">StreamingContext&#x5BF9;&#x8C61;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#离散流dstream"><span class="toc-text">&#x79BB;&#x6563;&#x6D41;&#xFF08;DStream&#xFF09;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#spark-streaming开发示例"><span class="toc-text">Spark Streaming&#x5F00;&#x53D1;&#x793A;&#x4F8B;</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#checkpoint维护state"><span class="toc-text">Checkpoint&#x7EF4;&#x62A4;state</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#updatestatebykey维护state"><span class="toc-text">updateStateByKey&#x7EF4;&#x62A4;State</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#updatestatebykey维护state优化版本"><span class="toc-text">updateStateByKey&#x7EF4;&#x62A4;State&#xFF08;&#x4F18;&#x5316;&#x7248;&#x672C;&#xFF09;</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#mapwithstate维护state的方法"><span class="toc-text">mapWithState&#x7EF4;&#x62A4;State&#x7684;&#x65B9;&#x6CD5;</span></a></li></ol></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/大数据技术/修改Flume源码使taildir source支持递归（可配置）/">修改Flume源码使taildir source支持递归（可配置）</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/解决Spark on YARN时jar包乱飞的问题/">解决Spark on YARN时jar包乱飞的问题</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink之Watermark理解/">Flink之Watermark理解</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/2.spark-2.4.5-bin-2.6.0-cdh5.15.1.tgz/"> Spark-2.4.5源码编译</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/Flink自定义触发器/">Flink自定义触发器</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.深入理解Spark算子aggregate/">深入理解Spark算子aggregate</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.Kudu常用Api(java)/">Kudu常用Api(java) </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/60.CentOS7安装单机版Kudu/">CentOS7安装单机版Kudu </a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/CDH安装Kafka/">CDH安装Kafka</a></li><li class="post-list-item"><a class="post-list-link" href="/大数据技术/HBase架构和读写流程/">HBase架构和读写流程</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux基础/">Linux基础</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala编程语言/">Scala编程语言</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/大数据技术/">大数据技术</a><span class="category-list-count">40</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据结构与算法/">数据结构与算法</a><span class="category-list-count">3</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/tags/Kudu/" style="font-size: 15px;">Kudu</a> <a href="/tags/CDH/" style="font-size: 15px;">CDH</a> <a href="/tags/Kafka/" style="font-size: 15px;">Kafka</a> <a href="/tags/Flink/" style="font-size: 15px;">Flink</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Hive/" style="font-size: 15px;">Hive</a> <a href="/tags/Linux/" style="font-size: 15px;">Linux</a> <a href="/tags/Shell/" style="font-size: 15px;">Shell</a> <a href="/tags/Flume/" style="font-size: 15px;">Flume</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">48</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">6</span></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"> <div class="footer-info"><p><a href="/baidusitemap.xml">网站地图</a> |  <a href="/atom.xml">订阅本站</a> |  <a href="/about/">联系博主</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次，本站总访客数:<i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>人</p><p><span> Copyright &copy;<a href="/." rel="nofollow">暗也橙子.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/zzuUriel/zzuUriel.github.io"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.5"></script><div id="fullscreen-img" class="hide"><span class="close"></span></div><script type="text/javascript" src="/js/imgview.js?v=2.0.5" async></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.5" async></script><link rel="stylesheet" type="text/css" href="/share/css/share.css"><script type="text/javascript" src="/share/js/social-share.js" charset="utf-8"></script><script type="text/javascript" src="/share/js/qrcode.js" charset="utf-8"></script></body></html>